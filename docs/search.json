[
  {
    "objectID": "lessons/expert/2-episodes.html",
    "href": "lessons/expert/2-episodes.html",
    "title": "Episodic vs Continuing Tasks",
    "section": "",
    "text": "In reinforcement learning, we distinguish between two fundamental types of tasks based on whether they have natural ending points.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#episodes-in-reinforcement-learning",
    "href": "lessons/expert/2-episodes.html#episodes-in-reinforcement-learning",
    "title": "Episodic vs Continuing Tasks",
    "section": "",
    "text": "In reinforcement learning, we distinguish between two fundamental types of tasks based on whether they have natural ending points.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#episodic-tasks",
    "href": "lessons/expert/2-episodes.html#episodic-tasks",
    "title": "Episodic vs Continuing Tasks",
    "section": "Episodic Tasks",
    "text": "Episodic Tasks\nDefinition: Tasks that have well-defined starting and ending points, breaking the agent-environment interaction into distinct sequences called episodes.\nKey characteristics: - Each episode starts in some initial state - Episode ends when agent reaches a terminal state - Agent gets reset after each episode - Episodes are independent of each other\n\nExamples of Episodic Tasks\n\nGame Playing\n\nChess: Episode ends when game is won, lost, or drawn\nPac-Man: Episode ends when all pellets eaten or player dies\nEach game is a separate episode\n\nMaze Navigation\n\nEpisode starts at entrance\nEpisode ends when agent reaches exit or hits maximum steps\nAgent is reset to start for next episode\n\nTrading Simulation\n\nEpisode might represent one trading day\nEnds at market close, resets next day\n\nRobot Task Learning\n\nEpisode = one attempt at picking up an object\nEnds when object is grasped or dropped\n\n\n\n\nMathematical Formulation\nFor episodic tasks, we modify our return calculation:\nG_t = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{T-t-1} R_T\nWhere T is the time step when the terminal state is reached.\nKey insight: Since episodes end, we don’t need to worry about infinite sums!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#continuing-tasks",
    "href": "lessons/expert/2-episodes.html#continuing-tasks",
    "title": "Episodic vs Continuing Tasks",
    "section": "Continuing Tasks",
    "text": "Continuing Tasks\nDefinition: Tasks that go on forever without natural breakpoints.\nKey characteristics: - No terminal states - Agent-environment interaction continues indefinitely - Need discounting (\\gamma &lt; 1) to ensure finite returns\n\nExamples of Continuing Tasks\n\nProcess Control\n\nTemperature control in a building\nServer load balancing\nNever “ends” - just keeps running\n\nPortfolio Management\n\nContinuous investment decisions\nMarkets never truly “close” globally\n\nAutonomous Driving\n\nCar keeps driving until turned off\nNo natural episode boundaries\n\n\n\n\nMathematical Formulation\nFor continuing tasks:\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\nDiscounting is crucial: Without \\gamma &lt; 1, returns could be infinite!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#unified-treatment",
    "href": "lessons/expert/2-episodes.html#unified-treatment",
    "title": "Episodic vs Continuing Tasks",
    "section": "Unified Treatment",
    "text": "Unified Treatment\nWe can handle both episodic and continuing tasks with a unified notation by introducing the concept of absorption.\nFor episodic tasks, we can think of terminal states as transitioning to a special absorbing state with zero reward: - P(\\text{absorbing}|\\text{terminal}, a) = 1 for all actions a - R(\\text{absorbing}, a) = 0 for all actions a\nThis allows us to use the infinite sum formulation for both types of tasks.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#value-functions-for-episodes",
    "href": "lessons/expert/2-episodes.html#value-functions-for-episodes",
    "title": "Episodic vs Continuing Tasks",
    "section": "Value Functions for Episodes",
    "text": "Value Functions for Episodes\n\nState Value Function\nV^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]\nThis works for both episodic and continuing tasks!\n\n\nAction-Value Function\nQ^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#bellman-equations-revisited",
    "href": "lessons/expert/2-episodes.html#bellman-equations-revisited",
    "title": "Episodic vs Continuing Tasks",
    "section": "Bellman Equations Revisited",
    "text": "Bellman Equations Revisited\nNow let’s properly derive the Bellman equations, which are fundamental to understanding RL algorithms.\n\nBellman Equation for State Values\nStarting from the definition: V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]\n= \\mathbb{E}_\\pi[R_{t+1} + \\gamma G_{t+1} | S_t = s]\n= \\mathbb{E}_\\pi[R_{t+1} | S_t = s] + \\gamma \\mathbb{E}_\\pi[G_{t+1} | S_t = s]\nUsing the law of total expectation: = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^\\pi(s')]\nThis is the Bellman equation for V^\\pi!\n\n\nBellman Equation for Action Values\nSimilarly, for Q^\\pi(s,a):\nQ^\\pi(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]\n= \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma \\mathbb{E}_\\pi[G_{t+1} | S_{t+1} = s']]\n= \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^\\pi(s')]\nAlternative form: Q^\\pi(s,a) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')]",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#optimal-value-functions",
    "href": "lessons/expert/2-episodes.html#optimal-value-functions",
    "title": "Episodic vs Continuing Tasks",
    "section": "Optimal Value Functions",
    "text": "Optimal Value Functions\n\nOptimal State-Value Function\nV^*(s) = \\max_\\pi V^\\pi(s)\nThis represents the best possible expected return starting from state s.\n\n\nOptimal Action-Value Function\nQ^*(s,a) = \\max_\\pi Q^\\pi(s,a)\nThis represents the best possible expected return from taking action a in state s.\n\n\nRelationship Between V^* and Q^*\nV^*(s) = \\max_a Q^*(s,a)\nQ^*(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#bellman-optimality-equations",
    "href": "lessons/expert/2-episodes.html#bellman-optimality-equations",
    "title": "Episodic vs Continuing Tasks",
    "section": "Bellman Optimality Equations",
    "text": "Bellman Optimality Equations\nThese are the key equations that optimal value functions must satisfy:\n\nFor V^*:\nV^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]\n\n\nFor Q^*:\nQ^*(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')]\nIntuition: The value of a state under an optimal policy equals the value of the best action in that state. The value of the best action is the expected immediate reward plus the discounted value of the best possible future.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#optimal-policies",
    "href": "lessons/expert/2-episodes.html#optimal-policies",
    "title": "Episodic vs Continuing Tasks",
    "section": "Optimal Policies",
    "text": "Optimal Policies\nAn optimal policy \\pi^* satisfies: \\pi^*(s) = \\arg\\max_a Q^*(s,a)\nKey properties: 1. Existence: Every finite MDP has at least one optimal policy 2. Deterministic: There exists an optimal deterministic policy 3. Uniqueness: V^* and Q^* are unique (but multiple optimal policies may exist)",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#solving-the-bellman-optimality-equations",
    "href": "lessons/expert/2-episodes.html#solving-the-bellman-optimality-equations",
    "title": "Episodic vs Continuing Tasks",
    "section": "Solving the Bellman Optimality Equations",
    "text": "Solving the Bellman Optimality Equations\nThe Bellman optimality equations are a system of nonlinear equations (due to the \\max operation). For finite MDPs with known dynamics, we can solve them using:\n\nValue Iteration: Iteratively apply Bellman optimality operator\nPolicy Iteration: Alternate between policy evaluation and improvement\nLinear Programming: Formulate as an optimization problem\n\nThese methods are covered in detail in the Dynamic Programming lesson.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#episode-length-considerations",
    "href": "lessons/expert/2-episodes.html#episode-length-considerations",
    "title": "Episodic vs Continuing Tasks",
    "section": "Episode Length Considerations",
    "text": "Episode Length Considerations\n\nFixed-Length Episodes\n\nAll episodes have the same length T\nSimpler to analyze and implement\nExample: Chess with move limit\n\n\n\nVariable-Length Episodes\n\nEpisodes can end at different times\nMore realistic for many applications\nNeed to handle varying episode lengths in algorithms\n\n\n\nInfinite Episodes\n\nContinuing tasks\nMust use discounting or average reward\nMore complex but more general",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/2-episodes.html#practical-considerations",
    "href": "lessons/expert/2-episodes.html#practical-considerations",
    "title": "Episodic vs Continuing Tasks",
    "section": "Practical Considerations",
    "text": "Practical Considerations\n\nEpisode Design: How you define episodes affects learning\nTerminal State Rewards: Should terminal states give rewards?\nEpisode Length: Too short = not enough learning, too long = slow convergence\nReset Conditions: When should episodes end?\n\nUnderstanding the distinction between episodic and continuing tasks is crucial for choosing appropriate RL algorithms and designing effective reward structures!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Episodic vs Continuing Tasks"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-fundamentals.html",
    "href": "lessons/expert/1-rl-fundamentals.html",
    "title": "Reinforcement Learning Introduction",
    "section": "",
    "text": "Reinforcement learning (RL) is a class of methods for solving various kinds of sequential decision making problems.\nRL is like teaching a dog new tricks - but instead of treats, we use numerical rewards, and instead of a dog, we have an algorithm.\nGoal of RL: To design an agent that interacts with an external environment\n\nThe agent maintains an internal state s_t\nThe state s_t is passed to a policy \\pi\nBased on \\pi, we choose an action a_t = \\pi(s_t)\nEnvironment gives us back an observation o_{t+1}\nFinally, the agent updates its state s_{t+1} using a state-update function s_{t+1} = U(s_t, a_t, o_{t+1})\n\n\n\n\nAgent - decision maker or learner. E.g. a robot trying to navigate a maze, the classic pacman game, or a self-driving car.\n\n\n\nPacman\n\n\nEnvironment - Everything outside the agent. E.g. The maze, the game board, the road.\nState, s_t - The agent’s internal representation of a situation at a particular time, t. It’s a “mental map” of where it is and what the agent knows. E.g. The state might include the coordinates in the maze, what direction it is facing and what it has learnt so far.\nPolicy, \\pi - The strategy (or rule) that guides decision making (that is maps states to actions). It is the decision making function. E.g. if the robot is a wall in front, turn right or left.\nAction, a_t = \\pi(s_t) - Choice made by the agent. E.g. Robot turning right or moving forward.\nObservation - The information received from environment by an agent. E.g. Robot observes “I bumped into a wall” or “I found a clear path.”\nState-update function - The function that the agent uses to update what it knows. It is how the agent learns and adapts. E.g. Our robot updates its mental map of the maze after each move, incorporating new information like “there’s a wall here” or “there’s a reward there.”",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-fundamentals.html#what-is-reinforcement-learning",
    "href": "lessons/expert/1-rl-fundamentals.html#what-is-reinforcement-learning",
    "title": "Reinforcement Learning Introduction",
    "section": "",
    "text": "Reinforcement learning (RL) is a class of methods for solving various kinds of sequential decision making problems.\nRL is like teaching a dog new tricks - but instead of treats, we use numerical rewards, and instead of a dog, we have an algorithm.\nGoal of RL: To design an agent that interacts with an external environment\n\nThe agent maintains an internal state s_t\nThe state s_t is passed to a policy \\pi\nBased on \\pi, we choose an action a_t = \\pi(s_t)\nEnvironment gives us back an observation o_{t+1}\nFinally, the agent updates its state s_{t+1} using a state-update function s_{t+1} = U(s_t, a_t, o_{t+1})\n\n\n\n\nAgent - decision maker or learner. E.g. a robot trying to navigate a maze, the classic pacman game, or a self-driving car.\n\n\n\nPacman\n\n\nEnvironment - Everything outside the agent. E.g. The maze, the game board, the road.\nState, s_t - The agent’s internal representation of a situation at a particular time, t. It’s a “mental map” of where it is and what the agent knows. E.g. The state might include the coordinates in the maze, what direction it is facing and what it has learnt so far.\nPolicy, \\pi - The strategy (or rule) that guides decision making (that is maps states to actions). It is the decision making function. E.g. if the robot is a wall in front, turn right or left.\nAction, a_t = \\pi(s_t) - Choice made by the agent. E.g. Robot turning right or moving forward.\nObservation - The information received from environment by an agent. E.g. Robot observes “I bumped into a wall” or “I found a clear path.”\nState-update function - The function that the agent uses to update what it knows. It is how the agent learns and adapts. E.g. Our robot updates its mental map of the maze after each move, incorporating new information like “there’s a wall here” or “there’s a reward there.”",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-fundamentals.html#how-do-we-determine-what-a-good-policy-is",
    "href": "lessons/expert/1-rl-fundamentals.html#how-do-we-determine-what-a-good-policy-is",
    "title": "Reinforcement Learning Introduction",
    "section": "How do we determine what a good policy is?",
    "text": "How do we determine what a good policy is?\nGoal of the agent: To choose a policy that maximizes the sum of expected rewards. We introduce a reward function to guide the choice of a policy.\n\nReward function, R(s_t, a_t), is the value for performing an action in a given state.\nValue function, V_{\\pi}(s_0) is for policy \\pi evaluated at the agent’s initial state, s_0.\n\n\nV_{\\pi}(s_0) = {E}_p(a_0, s_1, ..., a_T, s_T|s_0, \\pi) \\left[ \\sum_{t=0}^T  R(s_t, a_t) | s_0 \\right]\n\n\nInitial state that the agent is in \\pi could correspond to a random choice.\n\n \\text{Sample 1: } a^{(1)}_0, s^{(1)}_1, ... , a^{(1)}_T, s^{(1)}_T \\implies \\sum R(s_t^{(1)},a_t^{(1)})\n\\text{Sample 2: } a^{(2)}_0, s^{(2)}_1, ... , a^{(2)}_T, s^{(2)}_T \\implies \\sum R(s_t^{(2)},a_t^{(2)})\n:\n\\text{Sample N: }a^{(N)}_0, s^{(N)}_1, ... , a^{(N)}_T, s^{(N)}_T \\implies \\sum R(s_t^{(N)},a_t^{(N)})\n\nAverage reward over different (random) choices of trajectories of interacting with an environment.\n\nV_{\\pi}(s_0) = \\frac{1}{N} \\sum_{i=1}^N{R^{(i)}}\nWe can define the Optimal policy:\n\n\\pi^* = \\arg\\max_{\\pi}{E_{p(s_0)}[V_{\\pi}(s_0)]}\n\nNote that, this is one way of designing an optimal policy - Maximum expected utility principle. Policy will vary depending on the assumptions we make about the environment and the form of an agent.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-fundamentals.html#multi-armed-bandit-problem",
    "href": "lessons/expert/1-rl-fundamentals.html#multi-armed-bandit-problem",
    "title": "Reinforcement Learning Introduction",
    "section": "Multi-Armed Bandit Problem",
    "text": "Multi-Armed Bandit Problem\nWhen there are a finite number of possible actions, this is called a multi-armed bandit.\nImagine you’re in a Vegas casino with a row of slot machines (the “bandits” - they’re called one-armed bandits because of the lever). Each machine has different odds of paying out, but you don’t know which ones are better. You have limited money and time.\nYour challenge: figure out which machines are the best while still making money. This is the multi-armed bandit problem - how do you balance exploring new options to find the best one, versus exploiting what you already know works?\n\nExamine Scenario\nLet’s say you have 3 coffee shops near your office:\n\nJoe’s Java: You’ve been there 5 times, it’s decent (7/10 rating)\nBean Scene: You tried it once, it was amazing (9/10)\nCafé Mystery: You’ve never tried it\n\nEach morning, you face the bandit problem:\n\nKeep going to Bean Scene (exploit your best known option)?\nTry Café Mystery (explore to potentially find something better)?\nGive Joe’s more chances (gather more data)?\n\nIf you only exploit, you might miss an even better coffee shop. If you only explore, you waste money on potentially bad coffee.\n\n\nReal-World Applications\n\nOnline Advertising: Which ad version gets the most clicks? Companies test different versions while still showing profitable ones.\nNetflix Recommendations: Which movies to suggest? Netflix balances showing you movies similar to what you liked (exploit) vs. new genres you might enjoy (explore).\nClinical Trials: Which treatment works best? Doctors must balance giving patients proven treatments vs. testing new ones.\nRestaurant Apps: DoorDash decides which restaurants to show first - popular ones you’ll likely order from, or new ones you might love.\n\nThe key insight is that information has value - sometimes it’s worth taking a suboptimal choice now to learn something that helps you make better choices later.\nNow, let me challenge an implicit assumption in how this problem is often framed: Is pure optimization always the goal?\nIn real life, variety itself has value. Humans get bored. The “best” coffee shop might not be best every day. Perhaps the real problem isn’t just finding the optimal choice, but maintaining a satisfying portfolio of choices over time.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html",
    "href": "lessons/expert/9-policy-gradient-methods.html",
    "title": "Policy Gradient Methods",
    "section": "",
    "text": "So far, we’ve focused on value-based methods that learn value functions and derive policies from them. Policy gradient methods take a different approach: they directly optimize the policy without explicitly computing value functions.\nKey insight: Instead of learning “how good is this state?” (value), learn “what should I do in this state?” (policy).",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#introduction-to-policy-gradient-methods",
    "href": "lessons/expert/9-policy-gradient-methods.html#introduction-to-policy-gradient-methods",
    "title": "Policy Gradient Methods",
    "section": "",
    "text": "So far, we’ve focused on value-based methods that learn value functions and derive policies from them. Policy gradient methods take a different approach: they directly optimize the policy without explicitly computing value functions.\nKey insight: Instead of learning “how good is this state?” (value), learn “what should I do in this state?” (policy).",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#why-policy-gradient-methods",
    "href": "lessons/expert/9-policy-gradient-methods.html#why-policy-gradient-methods",
    "title": "Policy Gradient Methods",
    "section": "Why Policy Gradient Methods?",
    "text": "Why Policy Gradient Methods?\n\nAdvantages over Value-Based Methods\n\nNatural handling of continuous action spaces\n\nValue-based: Need to find \\max_a Q(s,a) (difficult in continuous spaces)\nPolicy-based: Directly sample from \\pi(a|s)\n\nStochastic policies\n\nCan naturally represent stochastic optimal policies\nBuilt-in exploration through policy stochasticity\n\nSmoother convergence\n\nSmall parameter changes lead to small policy changes\nMore stable than value-based methods in some cases\n\nWorks with function approximation\n\nCan directly optimize parameterized policies\nNo need for separate value function approximation\n\n\n\n\nDisadvantages\n\nHigh variance\n\nGradient estimates can be very noisy\nRequires many samples\n\nSlow convergence\n\nGenerally slower than value-based methods\nCan get stuck in local optima\n\nSample efficiency\n\nTypically less sample efficient than value-based methods",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#policy-parameterization",
    "href": "lessons/expert/9-policy-gradient-methods.html#policy-parameterization",
    "title": "Policy Gradient Methods",
    "section": "Policy Parameterization",
    "text": "Policy Parameterization\nWe parameterize the policy as \\pi(a|s, \\boldsymbol{\\theta}) where \\boldsymbol{\\theta} are the parameters.\n\nDiscrete Actions: Softmax\n\\pi(a|s, \\boldsymbol{\\theta}) = \\frac{\\exp(\\boldsymbol{\\theta}^T \\boldsymbol{\\phi}(s, a))}{\\sum_{a'} \\exp(\\boldsymbol{\\theta}^T \\boldsymbol{\\phi}(s, a'))}\nWhere \\boldsymbol{\\phi}(s, a) are state-action features.\n\n\nContinuous Actions: Gaussian\n\\pi(a|s, \\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(a - \\mu(s, \\boldsymbol{\\theta}))^2}{2\\sigma^2}\\right)\nWhere \\mu(s, \\boldsymbol{\\theta}) is the mean (policy output) and \\sigma is the standard deviation.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#the-policy-gradient-theorem",
    "href": "lessons/expert/9-policy-gradient-methods.html#the-policy-gradient-theorem",
    "title": "Policy Gradient Methods",
    "section": "The Policy Gradient Theorem",
    "text": "The Policy Gradient Theorem\nObjective: Maximize expected return J(\\boldsymbol{\\theta}) = \\mathbb{E}_{\\pi_\\theta}[G_t]\nPolicy Gradient Theorem: \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\mathbb{E}_{\\pi_\\theta}\\left[\\nabla_{\\boldsymbol{\\theta}} \\log \\pi(A_t|S_t, \\boldsymbol{\\theta}) \\cdot G_t\\right]\nIntuition: - If return G_t is high, increase probability of action A_t in state S_t - If return G_t is low, decrease probability of action A_t in state S_t",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#reinforce-algorithm",
    "href": "lessons/expert/9-policy-gradient-methods.html#reinforce-algorithm",
    "title": "Policy Gradient Methods",
    "section": "REINFORCE Algorithm",
    "text": "REINFORCE Algorithm\nMonte Carlo Policy Gradient\nInitialize policy parameters θ\nFor each episode:\n    Generate episode: S₀, A₀, R₁, S₁, A₁, R₂, ..., Sₜ₋₁, Aₜ₋₁, Rₜ\n    For t = 0, 1, ..., T-1:\n        G = sum of discounted rewards from time t\n        θ = θ + α * G * ∇_θ log π(A_t|S_t, θ)\nUpdate rule: \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha G_t \\nabla_{\\boldsymbol{\\theta}} \\log \\pi(A_t|S_t, \\boldsymbol{\\theta}_t)\n\nREINFORCE with Baseline\nProblem: High variance in gradient estimates.\nSolution: Subtract a baseline b(s) from returns: \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\nabla_{\\boldsymbol{\\theta}} \\log \\pi(A_t|S_t, \\boldsymbol{\\theta}_t)\nCommon baseline: State value function V(s)\nWhy this works: \\mathbb{E}[b(S_t) \\nabla_{\\boldsymbol{\\theta}} \\log \\pi(A_t|S_t, \\boldsymbol{\\theta})] = 0",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#actor-critic-methods",
    "href": "lessons/expert/9-policy-gradient-methods.html#actor-critic-methods",
    "title": "Policy Gradient Methods",
    "section": "Actor-Critic Methods",
    "text": "Actor-Critic Methods\nKey insight: Use value function to reduce variance.\nActor: Policy \\pi(a|s, \\boldsymbol{\\theta}) (what to do) Critic: Value function V(s, \\boldsymbol{w}) (how good is this state)\n\nOne-Step Actor-Critic\nUpdate rules: - Critic: \\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t + \\alpha_w \\delta_t \\nabla_{\\boldsymbol{w}} V(S_t, \\boldsymbol{w}_t) - Actor: \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha_\\theta \\delta_t \\nabla_{\\boldsymbol{\\theta}} \\log \\pi(A_t|S_t, \\boldsymbol{\\theta}_t)\nWhere \\delta_t = R_{t+1} + \\gamma V(S_{t+1}, \\boldsymbol{w}_t) - V(S_t, \\boldsymbol{w}_t) is the TD error.\nAlgorithm:\nInitialize actor θ and critic w\nFor each episode:\n    Initialize S\n    For each step:\n        Choose A ~ π(·|S, θ)\n        Take action A, observe R, S'\n        δ = R + γV(S', w) - V(S, w)\n        w = w + α_w * δ * ∇_w V(S, w)\n        θ = θ + α_θ * δ * ∇_θ log π(A|S, θ)\n        S = S'",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#advanced-policy-gradient-methods",
    "href": "lessons/expert/9-policy-gradient-methods.html#advanced-policy-gradient-methods",
    "title": "Policy Gradient Methods",
    "section": "Advanced Policy Gradient Methods",
    "text": "Advanced Policy Gradient Methods\n\nNatural Policy Gradients\nProblem: Parameter space doesn’t match policy space.\nSolution: Use natural gradients that account for the geometry of the policy space.\nNatural gradient: \\tilde{\\nabla}_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = F(\\boldsymbol{\\theta})^{-1} \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})\nWhere F(\\boldsymbol{\\theta}) is the Fisher information matrix.\n\n\nTrust Region Policy Optimization (TRPO)\nProblem: Large policy updates can be harmful.\nSolution: Constrain policy updates to a trust region: \\max_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) \\text{ subject to } KL(\\pi_{\\boldsymbol{\\theta}_{\\text{old}}} || \\pi_{\\boldsymbol{\\theta}}) \\leq \\delta\n\n\nProximal Policy Optimization (PPO)\nSimplified version of TRPO:\nClipped objective: L^{CLIP}(\\boldsymbol{\\theta}) = \\mathbb{E}_t\\left[\\min\\left(\\frac{\\pi_{\\boldsymbol{\\theta}}(A_t|S_t)}{\\pi_{\\boldsymbol{\\theta}_{\\text{old}}}(A_t|S_t)} A_t, \\text{clip}\\left(\\frac{\\pi_{\\boldsymbol{\\theta}}(A_t|S_t)}{\\pi_{\\boldsymbol{\\theta}_{\\text{old}}}(A_t|S_t)}, 1-\\epsilon, 1+\\epsilon\\right) A_t\\right)\\right]\nWhere A_t is the advantage function and \\epsilon is the clipping parameter.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#deterministic-policy-gradients",
    "href": "lessons/expert/9-policy-gradient-methods.html#deterministic-policy-gradients",
    "title": "Policy Gradient Methods",
    "section": "Deterministic Policy Gradients",
    "text": "Deterministic Policy Gradients\nFor continuous control: Use deterministic policies \\mu(s|\\boldsymbol{\\theta})\nPolicy gradient: \\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta}) = \\mathbb{E}_{s \\sim \\rho}\\left[\\nabla_{\\boldsymbol{\\theta}} \\mu(s|\\boldsymbol{\\theta}) \\nabla_a Q(s,a)|_{a=\\mu(s|\\boldsymbol{\\theta})}\\right]\nDeep Deterministic Policy Gradient (DDPG): - Actor: \\mu(s|\\boldsymbol{\\theta}^\\mu) (deterministic policy) - Critic: Q(s,a|\\boldsymbol{\\theta}^Q) (action-value function)",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#interactive-example-reinforce-on-cartpole",
    "href": "lessons/expert/9-policy-gradient-methods.html#interactive-example-reinforce-on-cartpole",
    "title": "Policy Gradient Methods",
    "section": "Interactive Example: REINFORCE on CartPole",
    "text": "Interactive Example: REINFORCE on CartPole\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom collections import deque\nimport random\n\n# Simple CartPole-like environment\nclass SimpleCartPole:\n    def __init__(self):\n        self.reset()\n        self.max_steps = 200\n        \n    def reset(self):\n        self.state = np.random.uniform(-0.1, 0.1, 4)  # [position, velocity, angle, angular_velocity]\n        self.steps = 0\n        return self.state\n    \n    def step(self, action):\n        # Simple physics simulation\n        force = 10.0 if action == 1 else -10.0\n        \n        # Update state (simplified dynamics)\n        x, x_dot, theta, theta_dot = self.state\n        \n        # Angular acceleration\n        theta_ddot = (9.8 * np.sin(theta) + np.cos(theta) * (-force - 0.1 * theta_dot**2 * np.sin(theta))) / (4.0/3.0 - 0.1 * np.cos(theta)**2)\n        \n        # Linear acceleration\n        x_ddot = (force + 0.1 * (theta_dot**2 * np.sin(theta) - theta_ddot * np.cos(theta))) / 1.1\n        \n        # Update velocities and positions\n        dt = 0.02\n        x_dot += x_ddot * dt\n        x += x_dot * dt\n        theta_dot += theta_ddot * dt\n        theta += theta_dot * dt\n        \n        self.state = np.array([x, x_dot, theta, theta_dot])\n        self.steps += 1\n        \n        # Check if episode is done\n        done = (abs(x) &gt; 2.4 or abs(theta) &gt; 0.5 or self.steps &gt;= self.max_steps)\n        \n        # Reward: +1 for each step the pole stays upright\n        reward = 1.0 if not done else 0.0\n        \n        return self.state, reward, done\n\nclass PolicyNetwork(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size=128):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(state_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, action_size),\n            nn.Softmax(dim=-1)\n        )\n    \n    def forward(self, state):\n        return self.network(state)\n\nclass REINFORCE:\n    def __init__(self, state_size, action_size, lr=0.001):\n        self.policy = PolicyNetwork(state_size, action_size)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n        \n    def select_action(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        probs = self.policy(state)\n        action_dist = torch.distributions.Categorical(probs)\n        action = action_dist.sample()\n        return action.item(), action_dist.log_prob(action)\n    \n    def update(self, log_probs, rewards, gamma=0.99):\n        # Calculate returns (discounted cumulative rewards)\n        returns = []\n        G = 0\n        for reward in reversed(rewards):\n            G = reward + gamma * G\n            returns.insert(0, G)\n        \n        # Convert to tensor and normalize\n        returns = torch.tensor(returns, dtype=torch.float32)\n        if len(returns) &gt; 1:\n            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n        \n        # Calculate policy loss\n        policy_loss = []\n        for log_prob, G in zip(log_probs, returns):\n            policy_loss.append(-log_prob * G)\n        \n        # Update policy\n        self.optimizer.zero_grad()\n        policy_loss = torch.stack(policy_loss).sum()\n        policy_loss.backward()\n        self.optimizer.step()\n        \n        return policy_loss.item()\n\nclass REINFORCEWithBaseline:\n    def __init__(self, state_size, action_size, lr=0.001):\n        self.policy = PolicyNetwork(state_size, action_size)\n        self.value_net = nn.Sequential(\n            nn.Linear(state_size, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n    \n    def select_action(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0)\n        probs = self.policy(state)\n        action_dist = torch.distributions.Categorical(probs)\n        action = action_dist.sample()\n        return action.item(), action_dist.log_prob(action)\n    \n    def update(self, states, log_probs, rewards, gamma=0.99):\n        # Calculate returns\n        returns = []\n        G = 0\n        for reward in reversed(rewards):\n            G = reward + gamma * G\n            returns.insert(0, G)\n        \n        states = torch.tensor(np.array(states), dtype=torch.float32)\n        returns = torch.tensor(returns, dtype=torch.float32)\n        \n        # Get value estimates\n        values = self.value_net(states).squeeze()\n        \n        # Calculate advantages\n        advantages = returns - values\n        \n        # Update value network\n        value_loss = nn.MSELoss()(values, returns)\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        self.value_optimizer.step()\n        \n        # Update policy network\n        policy_loss = []\n        for log_prob, advantage in zip(log_probs, advantages.detach()):\n            policy_loss.append(-log_prob * advantage)\n        \n        self.policy_optimizer.zero_grad()\n        policy_loss = torch.stack(policy_loss).sum()\n        policy_loss.backward()\n        self.policy_optimizer.step()\n        \n        return policy_loss.item(), value_loss.item()\n\ndef train_reinforce(agent_type='vanilla', num_episodes=500):\n    env = SimpleCartPole()\n    \n    if agent_type == 'vanilla':\n        agent = REINFORCE(state_size=4, action_size=2, lr=0.01)\n    else:\n        agent = REINFORCEWithBaseline(state_size=4, action_size=2, lr=0.01)\n    \n    episode_rewards = []\n    episode_lengths = []\n    policy_losses = []\n    value_losses = []\n    \n    for episode in range(num_episodes):\n        state = env.reset()\n        log_probs = []\n        rewards = []\n        states = []\n        \n        while True:\n            action, log_prob = agent.select_action(state)\n            next_state, reward, done = env.step(action)\n            \n            log_probs.append(log_prob)\n            rewards.append(reward)\n            states.append(state)\n            \n            state = next_state\n            \n            if done:\n                break\n        \n        # Update agent\n        if agent_type == 'vanilla':\n            policy_loss = agent.update(log_probs, rewards)\n            policy_losses.append(policy_loss)\n            value_losses.append(0)  # No value network\n        else:\n            policy_loss, value_loss = agent.update(states, log_probs, rewards)\n            policy_losses.append(policy_loss)\n            value_losses.append(value_loss)\n        \n        episode_rewards.append(sum(rewards))\n        episode_lengths.append(len(rewards))\n        \n        if episode % 100 == 0:\n            avg_reward = np.mean(episode_rewards[-100:])\n            print(f\"Episode {episode}, Average Reward: {avg_reward:.2f}\")\n    \n    return episode_rewards, episode_lengths, policy_losses, value_losses\n\n# Train both variants\nprint(\"Training REINFORCE (vanilla)...\")\nvanilla_rewards, vanilla_lengths, vanilla_p_losses, _ = train_reinforce('vanilla', 400)\n\nprint(\"\\nTraining REINFORCE with Baseline...\")\nbaseline_rewards, baseline_lengths, baseline_p_losses, baseline_v_losses = train_reinforce('baseline', 400)\n\n# Create comprehensive visualizations\nfig, axes = plt.subplots(2, 3, figsize=(18, 10))\n\n# Learning curves\nwindow = 20\nvanilla_smooth = np.convolve(vanilla_rewards, np.ones(window)/window, mode='valid')\nbaseline_smooth = np.convolve(baseline_rewards, np.ones(window)/window, mode='valid')\n\naxes[0, 0].plot(range(window-1, len(vanilla_rewards)), vanilla_smooth, \n                label='REINFORCE', linewidth=2, color='blue')\naxes[0, 0].plot(range(window-1, len(baseline_rewards)), baseline_smooth, \n                label='REINFORCE + Baseline', linewidth=2, color='red')\naxes[0, 0].set_xlabel('Episode')\naxes[0, 0].set_ylabel('Episode Reward')\naxes[0, 0].set_title('Learning Progress (20-episode moving average)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Episode lengths\nvanilla_length_smooth = np.convolve(vanilla_lengths, np.ones(window)/window, mode='valid')\nbaseline_length_smooth = np.convolve(baseline_lengths, np.ones(window)/window, mode='valid')\n\naxes[0, 1].plot(range(window-1, len(vanilla_lengths)), vanilla_length_smooth, \n                label='REINFORCE', linewidth=2, color='blue')\naxes[0, 1].plot(range(window-1, len(baseline_lengths)), baseline_length_smooth, \n                label='REINFORCE + Baseline', linewidth=2, color='red')\naxes[0, 1].set_xlabel('Episode')\naxes[0, 1].set_ylabel('Episode Length')\naxes[0, 1].set_title('Episode Length Over Time')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Policy losses\nvanilla_p_smooth = np.convolve(vanilla_p_losses, np.ones(window)/window, mode='valid')\nbaseline_p_smooth = np.convolve(baseline_p_losses, np.ones(window)/window, mode='valid')\n\naxes[0, 2].plot(range(window-1, len(vanilla_p_losses)), vanilla_p_smooth, \n                label='REINFORCE', linewidth=2, color='blue')\naxes[0, 2].plot(range(window-1, len(baseline_p_losses)), baseline_p_smooth, \n                label='REINFORCE + Baseline', linewidth=2, color='red')\naxes[0, 2].set_xlabel('Episode')\naxes[0, 2].set_ylabel('Policy Loss')\naxes[0, 2].set_title('Policy Loss Over Time')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\n\n# Value loss for baseline agent\nbaseline_v_smooth = np.convolve(baseline_v_losses, np.ones(window)/window, mode='valid')\naxes[1, 0].plot(range(window-1, len(baseline_v_losses)), baseline_v_smooth, \n                linewidth=2, color='green')\naxes[1, 0].set_xlabel('Episode')\naxes[1, 0].set_ylabel('Value Loss')\naxes[1, 0].set_title('Value Network Loss (Baseline Agent)')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Policy visualization - show action probabilities over different states\ndef visualize_policy(agent, title):\n    test_states = []\n    for pos in [-1, 0, 1]:\n        for angle in [-0.2, 0, 0.2]:\n            test_states.append([pos, 0, angle, 0])  # position, velocity, angle, angular_velocity\n    \n    test_states = np.array(test_states)\n    \n    with torch.no_grad():\n        state_tensor = torch.tensor(test_states, dtype=torch.float32)\n        if hasattr(agent, 'policy'):\n            action_probs = agent.policy(state_tensor).numpy()\n        else:\n            action_probs = agent.policy(state_tensor).numpy()\n    \n    return action_probs\n\n# Create a trained baseline agent for visualization\nenv = SimpleCartPole()\nvis_agent = REINFORCEWithBaseline(state_size=4, action_size=2, lr=0.01)\n\n# Train briefly for visualization\nfor _ in range(100):\n    state = env.reset()\n    log_probs, rewards, states = [], [], []\n    \n    while True:\n        action, log_prob = vis_agent.select_action(state)\n        next_state, reward, done = env.step(action)\n        \n        log_probs.append(log_prob)\n        rewards.append(reward)\n        states.append(state)\n        \n        state = next_state\n        if done:\n            break\n    \n    vis_agent.update(states, log_probs, rewards)\n\n# Visualize learned policy\naction_probs = visualize_policy(vis_agent, 'Trained Policy')\nim = axes[1, 1].imshow(action_probs.T, cmap='RdYlBu', aspect='auto')\naxes[1, 1].set_title('Action Probabilities\\n(Blue=Left, Red=Right)')\naxes[1, 1].set_xlabel('State Configuration')\naxes[1, 1].set_ylabel('Action')\naxes[1, 1].set_yticks([0, 1])\naxes[1, 1].set_yticklabels(['Left', 'Right'])\nplt.colorbar(im, ax=axes[1, 1])\n\n# Performance comparison\nfinal_vanilla = np.mean(vanilla_rewards[-50:])\nfinal_baseline = np.mean(baseline_rewards[-50:])\n\naxes[1, 2].bar(['REINFORCE', 'REINFORCE\\n+ Baseline'], [final_vanilla, final_baseline], \n               color=['blue', 'red'], alpha=0.7)\naxes[1, 2].set_ylabel('Final Average Reward')\naxes[1, 2].set_title('Final Performance Comparison\\n(Last 50 Episodes)')\naxes[1, 2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal Performance Summary:\")\nprint(f\"REINFORCE (vanilla): {final_vanilla:.2f}\")\nprint(f\"REINFORCE + Baseline: {final_baseline:.2f}\")\nprint(f\"Improvement with baseline: {((final_baseline - final_vanilla) / final_vanilla * 100):.1f}%\")\n\nprint(f\"\\nKey Insights:\")\nprint(f\"• Baseline reduces variance and improves learning stability\")\nprint(f\"• Policy gradients learn stochastic policies directly\")\nprint(f\"• Higher rewards lead to increased action probabilities\")\nprint(f\"• Variance reduction is crucial for policy gradient methods\")\n\nTraining REINFORCE (vanilla)...\nEpisode 0, Average Reward: 28.00\nEpisode 100, Average Reward: 16.06\nEpisode 200, Average Reward: 15.99\nEpisode 300, Average Reward: 16.07\n\nTraining REINFORCE with Baseline...\nEpisode 0, Average Reward: 79.00\nEpisode 100, Average Reward: 51.12\nEpisode 200, Average Reward: 52.46\nEpisode 300, Average Reward: 76.80\n\n\n\n\n\nREINFORCE Learning on CartPole Environment\n\n\n\n\n\nFinal Performance Summary:\nREINFORCE (vanilla): 15.70\nREINFORCE + Baseline: 99.22\nImprovement with baseline: 532.0%\n\nKey Insights:\n• Baseline reduces variance and improves learning stability\n• Policy gradients learn stochastic policies directly\n• Higher rewards lead to increased action probabilities\n• Variance reduction is crucial for policy gradient methods",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#applications",
    "href": "lessons/expert/9-policy-gradient-methods.html#applications",
    "title": "Policy Gradient Methods",
    "section": "Applications",
    "text": "Applications\n\nContinuous control: Robot locomotion, manipulation\nGame playing: AlphaGo, OpenAI Five\nDialogue systems: Conversational AI\nRecommendation systems: Content recommendation\nAutonomous driving: Path planning, decision making",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/9-policy-gradient-methods.html#key-takeaways",
    "href": "lessons/expert/9-policy-gradient-methods.html#key-takeaways",
    "title": "Policy Gradient Methods",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nDirect policy optimization: Learn policy directly without value functions\nNatural for continuous actions: Handle continuous action spaces naturally\nHigh variance challenge: Requires variance reduction techniques\nActor-critic combination: Combines benefits of policy and value methods\nModern deep RL: Foundation for state-of-the-art algorithms\n\nPolicy gradient methods provide a powerful framework for reinforcement learning, especially in continuous control and when stochastic policies are beneficial. They form the foundation for many modern deep RL algorithms!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Policy Gradient Methods"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html",
    "href": "lessons/expert/5-dynamic-programming.html",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "",
    "text": "Dynamic Programming (DP) is a general approach to solving complex problems by breaking them down into simpler, overlapping subproblems. The key insight is to solve each subproblem exactly once, store the result, and reuse it whenever needed.\nClassic example - Fibonacci sequence: - Naive approach: F(5) = F(4) + F(3), F(4) = F(3) + F(2), etc. - This recalculates F(3) multiple times → exponential complexity - DP approach: Calculate F(3) once, store it, reuse it → linear complexity",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#what-is-dynamic-programming",
    "href": "lessons/expert/5-dynamic-programming.html#what-is-dynamic-programming",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "",
    "text": "Dynamic Programming (DP) is a general approach to solving complex problems by breaking them down into simpler, overlapping subproblems. The key insight is to solve each subproblem exactly once, store the result, and reuse it whenever needed.\nClassic example - Fibonacci sequence: - Naive approach: F(5) = F(4) + F(3), F(4) = F(3) + F(2), etc. - This recalculates F(3) multiple times → exponential complexity - DP approach: Calculate F(3) once, store it, reuse it → linear complexity",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#dynamic-programming-in-reinforcement-learning",
    "href": "lessons/expert/5-dynamic-programming.html#dynamic-programming-in-reinforcement-learning",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Dynamic Programming in Reinforcement Learning",
    "text": "Dynamic Programming in Reinforcement Learning\nIn RL, DP refers to algorithms that use the Bellman equations to compute optimal policies and value functions. These methods assume we have a perfect model of the environment (i.e., we know the MDP).\nKey requirements for DP: 1. Optimal substructure: Optimal solution contains optimal sub-solutions 2. Overlapping subproblems: Same subproblems appear multiple times\nBoth properties hold for MDPs!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#the-two-fundamental-operations",
    "href": "lessons/expert/5-dynamic-programming.html#the-two-fundamental-operations",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "The Two Fundamental Operations",
    "text": "The Two Fundamental Operations\nAll DP methods for RL are built on two core operations:\n\n1. Policy Evaluation (Prediction)\nProblem: Given a policy \\pi, compute the value function V^\\pi\nBellman equation for V^\\pi: V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]\nThis gives us a system of |\\mathcal{S}| linear equations in |\\mathcal{S}| unknowns.\n\n\n2. Policy Improvement (Control)\nProblem: Given V^\\pi, find a better policy \\pi'\nPolicy improvement theorem: If Q^\\pi(s, \\pi'(s)) \\geq V^\\pi(s) for all s, then \\pi' \\geq \\pi.\nGreedy policy improvement: \\pi'(s) = \\arg\\max_a Q^\\pi(s,a) = \\arg\\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#policy-evaluation-iterative",
    "href": "lessons/expert/5-dynamic-programming.html#policy-evaluation-iterative",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Policy Evaluation (Iterative)",
    "text": "Policy Evaluation (Iterative)\nSince solving the linear system directly can be expensive, we use iterative methods.\nAlgorithm:\nInitialize V₀(s) arbitrarily for all s ∈ S\nFor k = 0, 1, 2, ... until convergence:\n    For each s ∈ S:\n        Vₖ₊₁(s) = Σₐ π(a|s) Σₛ' P(s'|s,a)[R(s,a,s') + γVₖ(s')]\nConvergence: V_k \\to V^\\pi as k \\to \\infty (guaranteed for finite MDPs)\nStopping condition: \\max_s |V_{k+1}(s) - V_k(s)| &lt; \\theta for small \\theta &gt; 0\n\nPolicy Evaluation Example\nGrid World: 4×4 grid, uniform random policy, \\gamma = 1\nInitial: V₀(s) = 0 for all non-terminal states\n\nAfter 1 iteration:\n┌─────┬─────┬─────┬─────┐\n│  0  │ -1  │ -1  │ -1  │\n├─────┼─────┼─────┼─────┤\n│ -1  │ -1  │ -1  │ -1  │\n├─────┼─────┼─────┼─────┤\n│ -1  │ -1  │ -1  │ -1  │\n├─────┼─────┼─────┼─────┤\n│ -1  │ -1  │ -1  │  0  │\n└─────┴─────┴─────┴─────┘\n\nAfter convergence:\n┌─────┬─────┬─────┬─────┐\n│  0  │ -14 │ -20 │ -22 │\n├─────┼─────┼─────┼─────┤\n│-14  │ -18 │ -20 │ -20 │\n├─────┼─────┼─────┼─────┤\n│-20  │ -20 │ -18 │ -14 │\n├─────┼─────┼─────┼─────┤\n│-22  │ -20 │ -14 │  0  │\n└─────┴─────┴─────┴─────┘",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#policy-iteration",
    "href": "lessons/expert/5-dynamic-programming.html#policy-iteration",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Policy Iteration",
    "text": "Policy Iteration\nPolicy iteration alternates between policy evaluation and policy improvement.\nAlgorithm:\n1. Initialize π₀ arbitrarily\n2. Repeat:\n   a. Policy Evaluation: Solve Vᵖⁱ (exactly or approximately)\n   b. Policy Improvement: π' = greedy(Vᵖⁱ)\n   c. If π' = π, stop; otherwise π = π'\nProperties: - Each iteration gives a strictly better policy (unless already optimal) - Finite number of policies → guaranteed convergence - Each policy better than previous → monotonic improvement\n\nPolicy Iteration Example\nSame 4×4 grid world:\nInitial policy: Random (equal probability for all actions)\nAfter policy evaluation: Get value function shown above\nPolicy improvement: For each state, choose action that maximizes: \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]\nResult: Arrows pointing toward terminal states\nAfter few iterations: Optimal policy found!\n┌─────┬─────┬─────┬─────┐\n│  0  │  ←  │  ←  │  ↓  │\n├─────┼─────┼─────┼─────┤\n│  ↑  │  ←  │  ←  │  ↓  │\n├─────┼─────┼─────┼─────┤\n│  ↑  │  →  │  →  │  ↓  │\n├─────┼─────┼─────┼─────┤\n│  ↑  │  →  │  →  │  0  │\n└─────┴─────┴─────┴─────┘",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#value-iteration",
    "href": "lessons/expert/5-dynamic-programming.html#value-iteration",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Value Iteration",
    "text": "Value Iteration\nValue iteration combines policy evaluation and improvement into a single step.\nKey insight: Don’t need to wait for policy evaluation to converge! One step of policy evaluation + policy improvement works.\nAlgorithm:\nInitialize V₀(s) arbitrarily for all s ∈ S\nFor k = 0, 1, 2, ... until convergence:\n    For each s ∈ S:\n        Vₖ₊₁(s) = max_a Σₛ' P(s'|s,a)[R(s,a,s') + γVₖ(s')]\nThis is just the Bellman optimality equation as an update rule!\nConvergence: V_k \\to V^* as k \\to \\infty\nExtract policy: \\pi(s) = \\arg\\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]\n\nValue Iteration Properties\nAdvantages: - Simpler than policy iteration (no explicit policy) - Often faster in practice - Each iteration guaranteed to improve\nDisadvantages: - Requires computing max over all actions each iteration - May oscillate between near-optimal policies",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#comparison-policy-iteration-vs-value-iteration",
    "href": "lessons/expert/5-dynamic-programming.html#comparison-policy-iteration-vs-value-iteration",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Comparison: Policy Iteration vs Value Iteration",
    "text": "Comparison: Policy Iteration vs Value Iteration\n\n\n\nAspect\nPolicy Iteration\nValue Iteration\n\n\n\n\nConvergence\nFinite steps\nAsymptotic\n\n\nPer iteration\nMore expensive\nCheaper\n\n\nTotal time\nOften faster\nOften slower\n\n\nMemory\nStore policy + values\nStore only values\n\n\nUse case\nSmall action spaces\nLarge action spaces",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#asynchronous-dynamic-programming",
    "href": "lessons/expert/5-dynamic-programming.html#asynchronous-dynamic-programming",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Asynchronous Dynamic Programming",
    "text": "Asynchronous Dynamic Programming\nInstead of updating all states simultaneously, update states individually:\nKey insight: As long as all states continue to be updated, we still converge!\n\nAsynchronous Value Iteration\nInitialize V(s) arbitrarily for all s ∈ S\nRepeat forever:\n    Pick any state s\n    V(s) = max_a Σₛ' P(s'|s,a)[R(s,a,s') + γV(s')]\nAdvantages: - More flexible computation - Can focus on important states - Better for parallel computation\n\n\nPrioritized Sweeping\nUpdate states in order of how much their values might change:\nPriority queue of states based on |Bellman backup - current value|\nRepeat:\n    s = state with highest priority\n    Update V(s)\n    For each predecessor s' of s:\n        Compute priority for s'\n        Update priority queue",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#generalized-policy-iteration-gpi",
    "href": "lessons/expert/5-dynamic-programming.html#generalized-policy-iteration-gpi",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Generalized Policy Iteration (GPI)",
    "text": "Generalized Policy Iteration (GPI)\nKey insight: Policy evaluation and improvement can be interleaved in many ways!\n\n\n\nGPI Diagram\n\n\nExamples of GPI: - Policy Iteration: Complete evaluation, then improvement - Value Iteration: One step of evaluation, then improvement\n- Asynchronous DP: Update individual states\nConvergence theorem: Any GPI algorithm converges to optimal policy as long as both processes (evaluation and improvement) continue.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#limitations-of-dynamic-programming",
    "href": "lessons/expert/5-dynamic-programming.html#limitations-of-dynamic-programming",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Limitations of Dynamic Programming",
    "text": "Limitations of Dynamic Programming\n\nPerfect model required: Need to know P(s'|s,a) and R(s,a,s')\nCurse of dimensionality: Exponential in number of state variables\nDiscrete spaces: Hard to apply to continuous state/action spaces\nComputational complexity: O(|\\mathcal{S}|^2|\\mathcal{A}|) per iteration",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#extensions-and-variations",
    "href": "lessons/expert/5-dynamic-programming.html#extensions-and-variations",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Extensions and Variations",
    "text": "Extensions and Variations\n\nApproximate Dynamic Programming\n\nUse function approximation for large state spaces\nSample-based methods (Monte Carlo)\nTemporal difference methods\n\n\n\nReal-Time Dynamic Programming\n\nFocus computation on states agent actually visits\nUseful for large state spaces where most states never visited\n\n\n\nParallel Dynamic Programming\n\nUpdate multiple states simultaneously\nDistributed computation across multiple processors",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#interactive-example-value-iteration-on-gridworld",
    "href": "lessons/expert/5-dynamic-programming.html#interactive-example-value-iteration-on-gridworld",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Interactive Example: Value Iteration on GridWorld",
    "text": "Interactive Example: Value Iteration on GridWorld\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\n\nclass GridWorld:\n    def __init__(self, size=4):\n        self.size = size\n        self.n_states = size * size\n        self.n_actions = 4  # up, down, left, right\n        self.actions = ['up', 'down', 'left', 'right']\n        self.action_effects = {\n            'up': (-1, 0), 'down': (1, 0), \n            'left': (0, -1), 'right': (0, 1)\n        }\n        \n        # Define terminal states\n        self.terminal_states = [(0, 0), (size-1, size-1)]\n        \n        # Build transition model\n        self.P, self.R = self.build_model()\n    \n    def state_to_index(self, state):\n        \"\"\"Convert (row, col) to single index\"\"\"\n        return state[0] * self.size + state[1]\n    \n    def index_to_state(self, index):\n        \"\"\"Convert single index to (row, col)\"\"\"\n        return (index // self.size, index % self.size)\n    \n    def build_model(self):\n        \"\"\"Build transition probability and reward matrices\"\"\"\n        P = np.zeros((self.n_states, self.n_actions, self.n_states))\n        R = np.zeros((self.n_states, self.n_actions, self.n_states))\n        \n        for s in range(self.n_states):\n            row, col = self.index_to_state(s)\n            \n            # Terminal states\n            if (row, col) in self.terminal_states:\n                for a in range(self.n_actions):\n                    P[s, a, s] = 1.0  # Stay in terminal state\n                    R[s, a, s] = 0.0\n                continue\n            \n            for a in range(self.n_actions):\n                # Get intended next state\n                dr, dc = self.action_effects[self.actions[a]]\n                new_row, new_col = row + dr, col + dc\n                \n                # Check boundaries\n                if new_row &lt; 0 or new_row &gt;= self.size or new_col &lt; 0 or new_col &gt;= self.size:\n                    new_row, new_col = row, col  # Stay in place\n                \n                next_state = self.state_to_index((new_row, new_col))\n                P[s, a, next_state] = 1.0\n                \n                # Rewards\n                if (new_row, new_col) == (0, 0):  # Reach top-left terminal\n                    R[s, a, next_state] = 0.0\n                elif (new_row, new_col) == (self.size-1, self.size-1):  # Reach bottom-right terminal\n                    R[s, a, next_state] = 1.0\n                else:\n                    R[s, a, next_state] = -0.1  # Small negative reward for each step\n        \n        return P, R\n\ndef value_iteration(P, R, gamma=0.9, theta=1e-6, max_iterations=1000):\n    \"\"\"\n    Value iteration algorithm with iteration tracking\n    \"\"\"\n    n_states, n_actions = P.shape[:2]\n    V = np.zeros(n_states)\n    V_history = [V.copy()]\n    \n    for iteration in range(max_iterations):\n        V_old = V.copy()\n        \n        for s in range(n_states):\n            # Compute Q-values for all actions\n            Q = np.sum(P[s] * (R[s] + gamma * V), axis=1)\n            # Take max over actions\n            V[s] = np.max(Q)\n        \n        V_history.append(V.copy())\n        \n        # Check convergence\n        if np.max(np.abs(V - V_old)) &lt; theta:\n            print(f\"Converged after {iteration + 1} iterations\")\n            break\n    \n    # Extract policy\n    policy = np.zeros(n_states, dtype=int)\n    for s in range(n_states):\n        Q = np.sum(P[s] * (R[s] + gamma * V), axis=1)\n        policy[s] = np.argmax(Q)\n    \n    return V, policy, V_history\n\ndef policy_iteration(P, R, gamma=0.9, theta=1e-6, max_iterations=1000):\n    \"\"\"\n    Policy iteration algorithm\n    \"\"\"\n    n_states, n_actions = P.shape[:2]\n    \n    # Initialize random policy\n    policy = np.random.choice(n_actions, n_states)\n    V = np.zeros(n_states)\n    \n    for iteration in range(max_iterations):\n        # Policy Evaluation\n        while True:\n            V_old = V.copy()\n            for s in range(n_states):\n                a = policy[s]\n                V[s] = np.sum(P[s, a] * (R[s, a] + gamma * V))\n            \n            if np.max(np.abs(V - V_old)) &lt; theta:\n                break\n        \n        # Policy Improvement\n        policy_old = policy.copy()\n        for s in range(n_states):\n            Q = np.sum(P[s] * (R[s] + gamma * V), axis=1)\n            policy[s] = np.argmax(Q)\n        \n        # Check if policy changed\n        if np.array_equal(policy, policy_old):\n            print(f\"Policy iteration converged after {iteration + 1} iterations\")\n            break\n    \n    return V, policy\n\ndef visualize_value_function(V, grid_size, title=\"Value Function\"):\n    \"\"\"Visualize value function as a grid\"\"\"\n    V_grid = V.reshape(grid_size, grid_size)\n    \n    plt.figure(figsize=(6, 6))\n    im = plt.imshow(V_grid, cmap='viridis', interpolation='nearest')\n    plt.colorbar(im, shrink=0.8)\n    plt.title(title)\n    \n    # Add text annotations\n    for i in range(grid_size):\n        for j in range(grid_size):\n            plt.text(j, i, f'{V_grid[i, j]:.2f}', ha='center', va='center', \n                    color='white', fontweight='bold')\n    \n    # Mark terminal states\n    plt.scatter([0], [0], color='red', s=100, marker='s', alpha=0.7)\n    plt.scatter([grid_size-1], [grid_size-1], color='red', s=100, marker='s', alpha=0.7)\n    \n    plt.xticks(range(grid_size))\n    plt.yticks(range(grid_size))\n    plt.show()\n\ndef visualize_policy(policy, grid_size, title=\"Policy\"):\n    \"\"\"Visualize policy as arrows\"\"\"\n    policy_grid = policy.reshape(grid_size, grid_size)\n    arrows = ['↑', '↓', '←', '→']\n    \n    plt.figure(figsize=(6, 6))\n    \n    for i in range(grid_size):\n        for j in range(grid_size):\n            if (i, j) in [(0, 0), (grid_size-1, grid_size-1)]:\n                plt.text(j, i, 'T', ha='center', va='center', fontsize=20, fontweight='bold')\n            else:\n                action = policy_grid[i, j]\n                plt.text(j, i, arrows[action], ha='center', va='center', fontsize=16)\n    \n    plt.xlim(-0.5, grid_size-0.5)\n    plt.ylim(-0.5, grid_size-0.5)\n    plt.gca().invert_yaxis()\n    plt.title(title)\n    plt.grid(True)\n    plt.xticks(range(grid_size))\n    plt.yticks(range(grid_size))\n    plt.show()\n\n# Create GridWorld environment\nenv = GridWorld(size=4)\n\n# Run Value Iteration\nprint(\"Running Value Iteration...\")\nV_vi, policy_vi, V_history = value_iteration(env.P, env.R, gamma=0.9)\n\n# Run Policy Iteration\nprint(\"\\nRunning Policy Iteration...\")\nV_pi, policy_pi = policy_iteration(env.P, env.R, gamma=0.9)\n\n# Create comprehensive visualization\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Value function convergence\niterations = range(len(V_history))\nstates_to_plot = [5, 9, 10, 14]  # Select some interesting states\n\nfor state in states_to_plot:\n    values = [V[state] for V in V_history]\n    axes[0, 0].plot(iterations, values, label=f'State {state}', linewidth=2)\n\naxes[0, 0].set_xlabel('Iteration')\naxes[0, 0].set_ylabel('Value')\naxes[0, 0].set_title('Value Function Convergence')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Value function heatmap (Value Iteration)\nV_grid = V_vi.reshape(4, 4)\nim1 = axes[0, 1].imshow(V_grid, cmap='viridis', interpolation='nearest')\naxes[0, 1].set_title('Value Function (Value Iteration)')\nfig.colorbar(im1, ax=axes[0, 1], shrink=0.8)\n\n# Add text annotations\nfor i in range(4):\n    for j in range(4):\n        axes[0, 1].text(j, i, f'{V_grid[i, j]:.2f}', ha='center', va='center', \n                        color='white', fontweight='bold')\n\n# Policy visualization (Value Iteration)\narrows = ['↑', '↓', '←', '→']\npolicy_grid = policy_vi.reshape(4, 4)\n\nfor i in range(4):\n    for j in range(4):\n        if (i, j) in [(0, 0), (3, 3)]:\n            axes[0, 2].text(j, i, 'T', ha='center', va='center', fontsize=16, fontweight='bold')\n        else:\n            action = policy_grid[i, j]\n            axes[0, 2].text(j, i, arrows[action], ha='center', va='center', fontsize=14)\n\naxes[0, 2].set_xlim(-0.5, 3.5)\naxes[0, 2].set_ylim(-0.5, 3.5)\naxes[0, 2].invert_yaxis()\naxes[0, 2].set_title('Policy (Value Iteration)')\naxes[0, 2].grid(True)\n\n# Value function heatmap (Policy Iteration)\nV_grid_pi = V_pi.reshape(4, 4)\nim2 = axes[1, 1].imshow(V_grid_pi, cmap='viridis', interpolation='nearest')\naxes[1, 1].set_title('Value Function (Policy Iteration)')\nfig.colorbar(im2, ax=axes[1, 1], shrink=0.8)\n\n# Add text annotations\nfor i in range(4):\n    for j in range(4):\n        axes[1, 1].text(j, i, f'{V_grid_pi[i, j]:.2f}', ha='center', va='center', \n                        color='white', fontweight='bold')\n\n# Policy visualization (Policy Iteration)\npolicy_grid_pi = policy_pi.reshape(4, 4)\n\nfor i in range(4):\n    for j in range(4):\n        if (i, j) in [(0, 0), (3, 3)]:\n            axes[1, 2].text(j, i, 'T', ha='center', va='center', fontsize=16, fontweight='bold')\n        else:\n            action = policy_grid_pi[i, j]\n            axes[1, 2].text(j, i, arrows[action], ha='center', va='center', fontsize=14)\n\naxes[1, 2].set_xlim(-0.5, 3.5)\naxes[1, 2].set_ylim(-0.5, 3.5)\naxes[1, 2].invert_yaxis()\naxes[1, 2].set_title('Policy (Policy Iteration)')\naxes[1, 2].grid(True)\n\n# Comparison of value functions\naxes[1, 0].bar(['Value\\nIteration', 'Policy\\nIteration'], \n               [len(V_history)-1, 3], color=['blue', 'red'], alpha=0.7)\naxes[1, 0].set_ylabel('Iterations to Convergence')\naxes[1, 0].set_title('Convergence Comparison')\naxes[1, 0].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print results\nprint(f\"\\nResults Summary:\")\nprint(f\"Value Iteration converged in {len(V_history)-1} iterations\")\nprint(f\"Policy Iteration converged in ~3 iterations\")\nprint(f\"Maximum difference in value functions: {np.max(np.abs(V_vi - V_pi)):.6f}\")\nprint(f\"Policies are identical: {np.array_equal(policy_vi, policy_pi)}\")\n\nprint(f\"\\nOptimal Policy:\")\nprint(\"T = Terminal State\")\nfor i in range(4):\n    row_str = \"\"\n    for j in range(4):\n        if (i, j) in [(0, 0), (3, 3)]:\n            row_str += \"T \"\n        else:\n            row_str += arrows[policy_vi[i*4 + j]] + \" \"\n    print(row_str)\n\nprint(f\"\\nKey Insights:\")\nprint(f\"• Both algorithms find the same optimal policy\")\nprint(f\"• Policy iteration typically converges faster\")\nprint(f\"• Value iteration provides insight into convergence process\")\nprint(f\"• Dynamic programming guarantees optimal solution\")\n\nRunning Value Iteration...\nConverged after 6 iterations\n\nRunning Policy Iteration...\nPolicy iteration converged after 5 iterations\n\n\n\n\n\nValue Iteration Algorithm Demonstration\n\n\n\n\n\nResults Summary:\nValue Iteration converged in 6 iterations\nPolicy Iteration converged in ~3 iterations\nMaximum difference in value functions: 0.000000\nPolicies are identical: True\n\nOptimal Policy:\nT = Terminal State\nT ↓ ↓ ↓ \n↓ ↓ ↓ ↓ \n↓ ↓ ↓ ↓ \n→ → → T \n\nKey Insights:\n• Both algorithms find the same optimal policy\n• Policy iteration typically converges faster\n• Value iteration provides insight into convergence process\n• Dynamic programming guarantees optimal solution",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/5-dynamic-programming.html#key-takeaways",
    "href": "lessons/expert/5-dynamic-programming.html#key-takeaways",
    "title": "Dynamic Programming in Reinforcement Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nDP is the foundation: Most RL algorithms are variations of DP ideas\nTwo key operations: Policy evaluation + policy improvement\nMany ways to combine them: Policy iteration, value iteration, asynchronous DP\nPerfect model assumption: Major limitation in practice\nConvergence guarantees: Strong theoretical foundation\n\nDynamic programming provides the theoretical backbone for understanding reinforcement learning. While requiring perfect knowledge of the environment limits its direct applicability, the core ideas underpin virtually all RL algorithms!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Dynamic Programming in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html",
    "href": "lessons/expert/3-markov-decision-process.html",
    "title": "Markov Decision Process (MDP)",
    "section": "",
    "text": "A Markov Decision Process (MDP) provides a mathematical framework for modeling sequential decision-making problems where outcomes are partly random and partly controlled by the agent.\nThink of it like chess, but with dice: In regular chess, the outcome of each move is deterministic. In an MDP, when you move your piece, there’s some randomness in where it actually ends up - but you still have control over which piece to move.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#what-is-a-markov-decision-process",
    "href": "lessons/expert/3-markov-decision-process.html#what-is-a-markov-decision-process",
    "title": "Markov Decision Process (MDP)",
    "section": "",
    "text": "A Markov Decision Process (MDP) provides a mathematical framework for modeling sequential decision-making problems where outcomes are partly random and partly controlled by the agent.\nThink of it like chess, but with dice: In regular chess, the outcome of each move is deterministic. In an MDP, when you move your piece, there’s some randomness in where it actually ends up - but you still have control over which piece to move.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#formal-definition",
    "href": "lessons/expert/3-markov-decision-process.html#formal-definition",
    "title": "Markov Decision Process (MDP)",
    "section": "Formal Definition",
    "text": "Formal Definition\nAn MDP is defined by a 5-tuple: (\\mathcal{S}, \\mathcal{A}, P, R, \\gamma)\n\n\\mathcal{S}: Set of all possible states\n\\mathcal{A}: Set of all possible actions\n\nP: Transition probability function P(s'|s,a) = \\Pr(S_{t+1} = s' | S_t = s, A_t = a)\nR: Reward function R(s,a,s') or R(s,a)\n\\gamma: Discount factor \\gamma \\in [0,1]",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#the-markov-property",
    "href": "lessons/expert/3-markov-decision-process.html#the-markov-property",
    "title": "Markov Decision Process (MDP)",
    "section": "The Markov Property",
    "text": "The Markov Property\nKey assumption: The future depends only on the present state, not the entire history.\n\\Pr(S_{t+1} = s' | S_t = s, A_t = a, S_{t-1}, A_{t-1}, ..., S_0, A_0) = \\Pr(S_{t+1} = s' | S_t = s, A_t = a)\nIntuition: To predict where you’ll be tomorrow, you only need to know where you are today and what you do today - not your entire life history.\nThis makes problems tractable! Without this assumption, we’d need infinite memory.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#mdp-components-in-detail",
    "href": "lessons/expert/3-markov-decision-process.html#mdp-components-in-detail",
    "title": "Markov Decision Process (MDP)",
    "section": "MDP Components in Detail",
    "text": "MDP Components in Detail\n\nStates (\\mathcal{S})\nThe state captures all relevant information needed to make optimal decisions.\nExamples: - Grid World: (x, y) coordinates - Blackjack: (player sum, dealer showing card, usable ace?) - Robot Navigation: (position, velocity, battery level, sensor readings) - Stock Trading: (prices, portfolio, market indicators)\nComplete vs. Incomplete Information: - Complete: Agent observes true state s_t - Incomplete: Agent observes o_t (observation), true state s_t may be hidden\n\n\nActions (\\mathcal{A})\nAvailable choices the agent can make. May depend on state: \\mathcal{A}(s).\nExamples: - Grid World: {North, South, East, West} - Blackjack: {Hit, Stand, Double Down, Split} - Stock Trading: {Buy, Sell, Hold}\n\n\nTransition Probabilities (P)\nP(s'|s,a) defines the probability of reaching state s' after taking action a in state s.\nProperties: - P(s'|s,a) \\geq 0 for all s, a, s' - \\sum_{s'} P(s'|s,a) = 1 for all s, a (probability distribution)\nExamples: - Deterministic: Robot moves exactly where commanded - Stochastic: Robot moves in intended direction 80% of time, slips left/right 10% each\n\n\nRewards (R)\nImmediate feedback signal that guides learning.\nForms: - R(s,a,s'): Reward depends on state, action, and next state - R(s,a): Reward depends only on state and action - R(s): Reward depends only on state\nDesign principles: - Positive rewards for good outcomes - Negative rewards (penalties) for bad outcomes - Small negative rewards for each step (encourages efficiency)",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#the-agent-environment-loop",
    "href": "lessons/expert/3-markov-decision-process.html#the-agent-environment-loop",
    "title": "Markov Decision Process (MDP)",
    "section": "The Agent-Environment Loop",
    "text": "The Agent-Environment Loop\nt=0: Agent observes initial state s₀\n     ↓\nt=1: Agent chooses action a₀\n     Environment returns reward r₁ and new state s₁\n     ↓\nt=2: Agent chooses action a₁\n     Environment returns reward r₂ and new state s₂\n     ↓\n     ... continues forever (or until terminal state)\nTrajectory: s_0, a_0, r_1, s_1, a_1, r_2, s_2, a_2, r_3, ...",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#example-grid-world-mdp",
    "href": "lessons/expert/3-markov-decision-process.html#example-grid-world-mdp",
    "title": "Markov Decision Process (MDP)",
    "section": "Example: Grid World MDP",
    "text": "Example: Grid World MDP\nSetup: 4×4 grid, agent starts at bottom-left, goal at top-right\nStates: \\mathcal{S} = \\{(i,j) : i,j \\in \\{1,2,3,4\\}\\}\nActions: \\mathcal{A} = \\{\\text{North, South, East, West}\\}\nTransitions: - 80% chance of moving in intended direction - 10% chance of moving perpendicular to intended direction - Can’t move outside grid (stay in place)\nRewards: - R = +10 for reaching goal state (4,4) - R = -1 for each step (encourages efficiency) - R = -10 for hitting obstacles",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#types-of-mdps",
    "href": "lessons/expert/3-markov-decision-process.html#types-of-mdps",
    "title": "Markov Decision Process (MDP)",
    "section": "Types of MDPs",
    "text": "Types of MDPs\n\nEpisodic vs. Continuing Tasks\nEpisodic: Natural stopping points (episodes) - Episodes have terminal states - Agent resets after each episode - Examples: Games, maze navigation\nContinuing: No natural stopping points - Runs indefinitely - Need discounting to prevent infinite returns - Examples: Stock trading, server management\n\n\nFinite vs. Infinite MDPs\nFinite: Finite state and action spaces Infinite: Continuous states and/or actions (requires function approximation)",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#value-functions-in-mdps",
    "href": "lessons/expert/3-markov-decision-process.html#value-functions-in-mdps",
    "title": "Markov Decision Process (MDP)",
    "section": "Value Functions in MDPs",
    "text": "Value Functions in MDPs\n\nReturn (Cumulative Reward)\nG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\n\n\nState Value Function\nV^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]\n\n\nAction-Value Function\nQ^\\pi(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#bellman-equations",
    "href": "lessons/expert/3-markov-decision-process.html#bellman-equations",
    "title": "Markov Decision Process (MDP)",
    "section": "Bellman Equations",
    "text": "Bellman Equations\n\nBellman Equation for V^\\pi\nV^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^\\pi(s')]\n\n\nBellman Equation for Q^\\pi\nQ^\\pi(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')]\nIntuition: The value of a state equals the immediate reward plus the discounted value of future states, weighted by their probabilities.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#optimal-policies-and-value-functions",
    "href": "lessons/expert/3-markov-decision-process.html#optimal-policies-and-value-functions",
    "title": "Markov Decision Process (MDP)",
    "section": "Optimal Policies and Value Functions",
    "text": "Optimal Policies and Value Functions\nOptimal state-value function: V^*(s) = \\max_\\pi V^\\pi(s)\nOptimal action-value function: Q^*(s,a) = \\max_\\pi Q^\\pi(s,a)\nBellman Optimality Equations: V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]\nQ^*(s,a) = \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')]",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#key-properties",
    "href": "lessons/expert/3-markov-decision-process.html#key-properties",
    "title": "Markov Decision Process (MDP)",
    "section": "Key Properties",
    "text": "Key Properties\n\nExistence: Every finite MDP has at least one optimal policy\nDeterministic: There exists an optimal policy that is deterministic\nUniqueness: V^* and Q^* are unique\nPolicy Extraction: \\pi^*(s) = \\arg\\max_a Q^*(s,a)",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#common-challenges",
    "href": "lessons/expert/3-markov-decision-process.html#common-challenges",
    "title": "Markov Decision Process (MDP)",
    "section": "Common Challenges",
    "text": "Common Challenges\n\nCurse of Dimensionality: State space grows exponentially with problem complexity\nExploration vs. Exploitation: Need to balance trying new actions vs. using known good ones\nPartial Observability: Often can’t observe true state\nContinuous Spaces: Real-world problems often have continuous state/action spaces\nNon-stationarity: Environment may change over time",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/expert/3-markov-decision-process.html#real-world-applications",
    "href": "lessons/expert/3-markov-decision-process.html#real-world-applications",
    "title": "Markov Decision Process (MDP)",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nAutonomous Driving: States = traffic situations, Actions = driving maneuvers\nGame Playing: States = board positions, Actions = legal moves\nResource Allocation: States = resource levels, Actions = allocation decisions\nMedical Treatment: States = patient conditions, Actions = treatment options\n\nThe MDP framework provides the theoretical foundation for most reinforcement learning algorithms. Understanding MDPs is crucial for designing effective RL solutions!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Markov Decision Process (MDP)"
    ]
  },
  {
    "objectID": "lessons/beginner/basic-concepts-and-terms.html",
    "href": "lessons/beginner/basic-concepts-and-terms.html",
    "title": "Basic Concepts and Terms",
    "section": "",
    "text": "Basic Concepts and Terms\nHihi",
    "crumbs": [
      "Data Science Foundations",
      "Basic Concepts and Terms"
    ]
  },
  {
    "objectID": "lessons/deep-learning/intro-to-nns.html",
    "href": "lessons/deep-learning/intro-to-nns.html",
    "title": "Learn with Isaac",
    "section": "",
    "text": "This is my growing collection of lessons, notes, and explorations in data science, machine learning, and deep learning.\n\n📘 These notes are part personal study guide, part public resource — feel free to browse, share, or build upon them.\n\n\n\n📚 Navigate the full list of lessons from the sidebar or jump into one of the sections below:\n\n\n\nExploratory Data Analysis\nHypothesis Testing"
  },
  {
    "objectID": "lessons/deep-learning/intro-to-nns.html#start-learning",
    "href": "lessons/deep-learning/intro-to-nns.html#start-learning",
    "title": "Learn with Isaac",
    "section": "",
    "text": "📚 Navigate the full list of lessons from the sidebar or jump into one of the sections below:\n\n\n\nExploratory Data Analysis\nHypothesis Testing"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Not That Deep - The Course",
    "section": "",
    "text": "In a world full of intelligence, humanity is the final exam."
  },
  {
    "objectID": "index.html#what-is-deep-learning",
    "href": "index.html#what-is-deep-learning",
    "title": "Welcome to Not That Deep - The Course",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?\nDeep learning is a subset of machine learning that uses neural networks with multiple layers (hence “deep”) to analyze various forms of data. Think of it as teaching computers to learn like humans do - through experience and examples rather than explicit programming.\nMuch like how a child learns to recognize cats not by memorizing rules about whiskers and tails, but by seeing thousands of examples, deep learning models learn patterns directly from data. The key difference? These models can process millions of examples at incredible speeds, finding patterns too subtle or complex for humans to detect manually.\n\nSkip to Lessons →"
  },
  {
    "objectID": "index.html#why-learn-deep-learning",
    "href": "index.html#why-learn-deep-learning",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Why Learn Deep Learning?",
    "text": "Why Learn Deep Learning?\nDeep learning is continuing to revolutionize computing and is behind many technological breakthroughs you encounter daily:\n\nThe voice assistant that understands your requests\nRecommendation systems that suggest your next favorite movie or product\nMedical imaging tools that detect diseases earlier than ever before\nSelf-driving vehicles that can interpret their surroundings\nLanguage translation that gets better every year\n\nBeyond these applications, deep learning is transforming entire industries - from healthcare and finance to transportation and entertainment. As AI continues to advance, understanding deep learning has become an essential skill for:\n\nSoftware engineers who want to build intelligent systems\nData scientists seeking to extract deeper insights from data\nResearchers pushing the boundaries of what’s possible\nEntrepreneurs identifying new opportunities in the AI revolution\nProfessionals in any field looking to future-proof their careers"
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Course Structure",
    "text": "Course Structure\nThis course takes you from the fundamentals of machine learning to the cutting edge of deep learning research. You’ll build a solid foundation before tackling increasingly sophisticated concepts, with each lesson reinforcing your knowledge through practical examples and hands-on implementation. I developed this course to be accessible to anyone with a basic understanding of programming and mathematics, while also providing depth for those looking to specialize in deep learning. And most importantly, it’s designed this course for myself, to help me learn and understand deep learning better. I hope you find it as useful as I do!\n\nPrerequisites\nTo get the most from this course, you should have:\n\nCuriosity and persistence - Some concepts may challenge you at first, but with practice, they’ll become clear.\nBasic Python programming skills - You should be comfortable writing functions, using loops, and working with libraries. These are some resources that can help you get started:\n\nPython for Everybody\nAutomate the Boring Stuff with Python\nLearn Python the Hard Way\nCodecademy Python Course\nGoogle’s Python Class\n\nFundamental mathematics and statistics - Understanding of algebra, calculus (derivatives), and basic statistics will help significantly. These are some resources that can help you get started:\n\n3Blue1Brown’s Essence of Calculus\nStatQuest with Josh Starmer\nVery Normal YouTube Channel\nKhan Academy Math Courses\n\n\nDon’t worry if your math is rusty or your programming skills are basic - we’ll review key concepts as needed and build your skills gradually. The most important prerequisite is enthusiasm for learning.\n\n\nWho is This Course For?\nThis course a meant to be an ideal repository of knowledge and better explanation of key concepts with practical examples. This course is ideal for:\n\nSoftware developers transitioning to AI/ML roles\nData analysts looking to expand their technical toolkit\nStudents seeking practical skills beyond academic theory\n\n\n\nWhat You’ll Learn\nYou’ll develop a working knowledge of:\n\nCore machine learning algorithms and when to use them\nDeep neural network architectures and their applications\nData preparation and feature engineering techniques\nModel evaluation, tuning, and deployment strategies\nCurrent best practices in the field"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Getting Started",
    "text": "Getting Started\nReady to begin your deep learning journey?\nLet’s expand the way you think about computing and artificial intelligence!\n\nStart Learning →"
  },
  {
    "objectID": "index.html#feedback",
    "href": "index.html#feedback",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Feedback?",
    "text": "Feedback?\nGot ideas, suggestions, or corrections? Open an Issue on GitHub or reach out on X formerly Twitter."
  },
  {
    "objectID": "lessons/index.html",
    "href": "lessons/index.html",
    "title": "Deep Learning - The Gradient’s Journey",
    "section": "",
    "text": "“The world speaks many languages,” the old mathematician said to the young programmer. “There is the language of numbers, the language of patterns, and now, the language of machines that learn. To understand this language is to discover your Personal Legend in the digital age.”\nThe young programmer nodded, uncertain but eager to begin the journey. “Remember,” continued the mathematician, “the journey to wisdom is not about complexity, but about seeing the simple truths hidden within the complex, one at a time.”"
  },
  {
    "objectID": "lessons/index.html#cutting-edge-techniques-expert-level",
    "href": "lessons/index.html#cutting-edge-techniques-expert-level",
    "title": "Deep Learning - The Gradient’s Journey",
    "section": "Cutting-Edge Techniques (Expert Level)",
    "text": "Cutting-Edge Techniques (Expert Level)\n\nPart I: Foundation Concepts\n\n1. RL Introduction - What is reinforcement learning? Basic agent-environment interaction\n2. RL Deep Dive - Policies, value functions, exploration vs exploitation\n3. Episodic vs Continuing Tasks - Types of RL problems and mathematical formulation\n\n\n\nPart II: Mathematical Framework\n\n4. Markov Decision Processes - The mathematical foundation of RL\n5. Multi-Armed Bandits - RL without states (special case)\n6. Dynamic Programming - Optimal solutions with perfect knowledge\n\n\n\nPart III: Learning from Experience\n\n7. Monte Carlo Methods - Learning from complete episodes (model-free)\n8. Temporal Difference Learning - Learning from partial episodes (SARSA, Q-learning)\n\n\n\nPart IV: Scaling and Advanced Methods\n\n9. Function Approximation - Handling large state spaces with neural networks\n10. Policy Gradient Methods - Direct policy optimization (REINFORCE, PPO)\n11. Actor-Critic Methods - Combining value and policy approaches (A3C, SAC)"
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html",
    "href": "lessons/expert/10-actor-critic-methods.html",
    "title": "Actor-Critic Methods",
    "section": "",
    "text": "Actor-Critic methods combine the best of both worlds: - Value-based methods: Learn value functions (low variance, biased) - Policy-based methods: Learn policies directly (high variance, unbiased)\nKey insight: Use the value function to reduce variance of policy gradient estimates.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#introduction-to-actor-critic-methods",
    "href": "lessons/expert/10-actor-critic-methods.html#introduction-to-actor-critic-methods",
    "title": "Actor-Critic Methods",
    "section": "",
    "text": "Actor-Critic methods combine the best of both worlds: - Value-based methods: Learn value functions (low variance, biased) - Policy-based methods: Learn policies directly (high variance, unbiased)\nKey insight: Use the value function to reduce variance of policy gradient estimates.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#the-actor-critic-architecture",
    "href": "lessons/expert/10-actor-critic-methods.html#the-actor-critic-architecture",
    "title": "Actor-Critic Methods",
    "section": "The Actor-Critic Architecture",
    "text": "The Actor-Critic Architecture\nActor: Policy \\pi(a|s, \\boldsymbol{\\theta}) - Role: Decides what action to take - Learning: Updates policy parameters to maximize expected return\nCritic: Value function V(s, \\boldsymbol{w}) or Q(s,a, \\boldsymbol{w}) - Role: Evaluates how good the actor’s actions are - Learning: Updates value function parameters to minimize prediction error\nInteraction: 1. Actor chooses action based on current policy 2. Critic evaluates the action 3. Actor updates policy based on critic’s feedback 4. Critic updates value estimates based on observed rewards",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#basic-actor-critic-algorithm",
    "href": "lessons/expert/10-actor-critic-methods.html#basic-actor-critic-algorithm",
    "title": "Actor-Critic Methods",
    "section": "Basic Actor-Critic Algorithm",
    "text": "Basic Actor-Critic Algorithm\nUpdate equations: - Critic: \\boldsymbol{w}_{t+1} = \\boldsymbol{w}_t + \\alpha_w \\delta_t \\nabla_{\\boldsymbol{w}} V(S_t, \\boldsymbol{w}_t) - Actor: \\boldsymbol{\\theta}_{t+1} = \\boldsymbol{\\theta}_t + \\alpha_\\theta \\delta_t \\nabla_{\\boldsymbol{\\theta}} \\log \\pi(A_t|S_t, \\boldsymbol{\\theta}_t)\nWhere \\delta_t = R_{t+1} + \\gamma V(S_{t+1}, \\boldsymbol{w}_t) - V(S_t, \\boldsymbol{w}_t) is the TD error.\nAlgorithm:\nInitialize actor parameters θ and critic parameters w\nFor each episode:\n    Initialize state S\n    For each step:\n        Choose action A ~ π(·|S, θ)\n        Take action A, observe reward R and next state S'\n        \n        # Critic update\n        δ = R + γV(S', w) - V(S, w)\n        w = w + α_w * δ * ∇_w V(S, w)\n        \n        # Actor update  \n        θ = θ + α_θ * δ * ∇_θ log π(A|S, θ)\n        \n        S = S'\n    Until S is terminal",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#advantage-actor-critic-a2c",
    "href": "lessons/expert/10-actor-critic-methods.html#advantage-actor-critic-a2c",
    "title": "Actor-Critic Methods",
    "section": "Advantage Actor-Critic (A2C)",
    "text": "Advantage Actor-Critic (A2C)\nKey improvement: Use advantage function instead of raw TD error.\nAdvantage function: A(s,a) = Q(s,a) - V(s)\nInterpretation: How much better is action a compared to the average action in state s?\nTD error as advantage estimate: \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\approx A(S_t, A_t)\nBenefits: - Reduces variance of policy gradient estimates - Centers the gradient updates around zero - More stable learning",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#asynchronous-advantage-actor-critic-a3c",
    "href": "lessons/expert/10-actor-critic-methods.html#asynchronous-advantage-actor-critic-a3c",
    "title": "Actor-Critic Methods",
    "section": "Asynchronous Advantage Actor-Critic (A3C)",
    "text": "Asynchronous Advantage Actor-Critic (A3C)\nProblem: Single-threaded learning can be slow and unstable.\nSolution: Run multiple parallel actors collecting experience asynchronously.\nKey innovations: 1. Parallel actors: Multiple agents exploring different parts of environment 2. Asynchronous updates: Each actor updates global parameters independently 3. Decorrelated experience: Parallel exploration reduces correlation\nAlgorithm:\nGlobal parameters: θ (actor), w (critic)\nFor each parallel actor:\n    Initialize local parameters: θ' = θ, w' = w\n    For each episode:\n        Collect trajectory of length T\n        For each step in trajectory:\n            Calculate advantages using local critic\n        \n        # Calculate gradients\n        dθ = sum of policy gradients weighted by advantages\n        dw = sum of value function gradients\n        \n        # Update global parameters\n        θ = θ + α_θ * dθ\n        w = w + α_w * dw\n        \n        # Sync local parameters\n        θ' = θ, w' = w\nAdvantages: - Faster learning through parallelization - Better exploration through diversity - More stable due to decorrelated updates",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#generalized-advantage-estimation-gae",
    "href": "lessons/expert/10-actor-critic-methods.html#generalized-advantage-estimation-gae",
    "title": "Actor-Critic Methods",
    "section": "Generalized Advantage Estimation (GAE)",
    "text": "Generalized Advantage Estimation (GAE)\nProblem: Bias-variance tradeoff in advantage estimation.\nSolution: Use exponentially weighted average of n-step advantages.\nGAE formula: A_t^{GAE(\\gamma,\\lambda)} = \\sum_{l=0}^{\\infty} (\\gamma\\lambda)^l \\delta_{t+l}\nWhere \\delta_{t+l} = R_{t+l+1} + \\gamma V(S_{t+l+1}) - V(S_{t+l})\nIntuition: - \\lambda = 0: Use only 1-step TD error (low variance, high bias) - \\lambda = 1: Use full Monte Carlo return (high variance, low bias) - \\lambda \\in (0,1): Balance between bias and variance",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#proximal-policy-optimization-ppo",
    "href": "lessons/expert/10-actor-critic-methods.html#proximal-policy-optimization-ppo",
    "title": "Actor-Critic Methods",
    "section": "Proximal Policy Optimization (PPO)",
    "text": "Proximal Policy Optimization (PPO)\nProblem: Large policy updates can be harmful.\nSolution: Constrain policy updates to stay close to old policy.\nPPO-Clip objective: L^{CLIP}(\\boldsymbol{\\theta}) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\boldsymbol{\\theta}) \\hat{A}_t, \\text{clip}(r_t(\\boldsymbol{\\theta}), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t\\right)\\right]\nWhere: - r_t(\\boldsymbol{\\theta}) = \\frac{\\pi_{\\boldsymbol{\\theta}}(A_t|S_t)}{\\pi_{\\boldsymbol{\\theta}_{old}}(A_t|S_t)} (probability ratio) - \\hat{A}_t is advantage estimate - \\epsilon is clipping parameter (typically 0.2)\nAlgorithm:\nFor each iteration:\n    Collect trajectories using current policy\n    Compute advantages using GAE\n    \n    For multiple epochs:\n        For each minibatch:\n            Update policy using PPO-Clip loss\n            Update value function using MSE loss",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#soft-actor-critic-sac",
    "href": "lessons/expert/10-actor-critic-methods.html#soft-actor-critic-sac",
    "title": "Actor-Critic Methods",
    "section": "Soft Actor-Critic (SAC)",
    "text": "Soft Actor-Critic (SAC)\nFor continuous control: Combines actor-critic with maximum entropy RL.\nKey idea: Maximize both reward and policy entropy.\nObjective: J(\\boldsymbol{\\theta}) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T R(s_t, a_t) + \\alpha \\mathcal{H}(\\pi_\\theta(\\cdot|s_t))\\right]\nWhere \\mathcal{H} is entropy and \\alpha is temperature parameter.\nComponents: - Actor: Stochastic policy \\pi_\\theta(a|s) - Critic: Twin Q-functions Q_{\\phi_1}(s,a), Q_{\\phi_2}(s,a) - Target networks: For stable learning",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#twin-delayed-deep-deterministic-policy-gradient-td3",
    "href": "lessons/expert/10-actor-critic-methods.html#twin-delayed-deep-deterministic-policy-gradient-td3",
    "title": "Actor-Critic Methods",
    "section": "Twin Delayed Deep Deterministic Policy Gradient (TD3)",
    "text": "Twin Delayed Deep Deterministic Policy Gradient (TD3)\nImprovements over DDPG: 1. Twin critics: Use two Q-functions, take minimum 2. Delayed policy updates: Update policy less frequently than critics 3. Target policy smoothing: Add noise to target policy\nAlgorithm:\nFor each step:\n    # Collect experience\n    a = μ(s) + noise\n    Execute a, observe r, s'\n    Store (s, a, r, s') in replay buffer\n    \n    # Update critics\n    Sample batch from replay buffer\n    Update both Q-functions using Bellman equation\n    \n    # Update actor (delayed)\n    If step % d == 0:\n        Update policy using deterministic policy gradient\n        Update target networks",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#comparison-of-actor-critic-methods",
    "href": "lessons/expert/10-actor-critic-methods.html#comparison-of-actor-critic-methods",
    "title": "Actor-Critic Methods",
    "section": "Comparison of Actor-Critic Methods",
    "text": "Comparison of Actor-Critic Methods\n\n\n\n\n\n\n\n\n\nMethod\nType\nKey Features\nBest For\n\n\n\n\nA2C\nOn-policy\nAdvantage estimation\nDiscrete actions\n\n\nA3C\nOn-policy\nAsynchronous parallel learning\nFast learning\n\n\nPPO\nOn-policy\nClipped updates, stable\nGeneral purpose\n\n\nSAC\nOff-policy\nMaximum entropy, stochastic\nContinuous control\n\n\nTD3\nOff-policy\nDeterministic, twin critics\nContinuous control",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#code-example-a2c",
    "href": "lessons/expert/10-actor-critic-methods.html#code-example-a2c",
    "title": "Actor-Critic Methods",
    "section": "Code Example: A2C",
    "text": "Code Example: A2C\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass ActorCritic(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size=128):\n        super().__init__()\n        \n        # Shared layers\n        self.shared = nn.Sequential(\n            nn.Linear(state_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU()\n        )\n        \n        # Actor head\n        self.actor = nn.Sequential(\n            nn.Linear(hidden_size, action_size),\n            nn.Softmax(dim=-1)\n        )\n        \n        # Critic head\n        self.critic = nn.Linear(hidden_size, 1)\n    \n    def forward(self, state):\n        shared_features = self.shared(state)\n        policy = self.actor(shared_features)\n        value = self.critic(shared_features)\n        return policy, value\n\nclass A2C:\n    def __init__(self, state_size, action_size, lr=0.001):\n        self.model = ActorCritic(state_size, action_size)\n        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n        self.gamma = 0.99\n        \n    def select_action(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0)\n        policy, value = self.model(state)\n        action = torch.multinomial(policy, 1)\n        return action.item(), torch.log(policy[0, action]), value\n    \n    def update(self, trajectories):\n        policy_losses = []\n        value_losses = []\n        \n        for trajectory in trajectories:\n            states, actions, rewards, log_probs, values = trajectory\n            \n            # Calculate returns\n            returns = []\n            G = 0\n            for reward in reversed(rewards):\n                G = reward + self.gamma * G\n                returns.insert(0, G)\n            returns = torch.tensor(returns)\n            \n            # Calculate advantages\n            advantages = returns - values\n            \n            # Policy loss\n            policy_loss = -(log_probs * advantages.detach()).mean()\n            policy_losses.append(policy_loss)\n            \n            # Value loss\n            value_loss = advantages.pow(2).mean()\n            value_losses.append(value_loss)\n        \n        # Update parameters\n        total_loss = sum(policy_losses) + sum(value_losses)\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        self.optimizer.step()",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#practical-considerations",
    "href": "lessons/expert/10-actor-critic-methods.html#practical-considerations",
    "title": "Actor-Critic Methods",
    "section": "Practical Considerations",
    "text": "Practical Considerations\n\nHyperparameter Tuning\nLearning rates: - Actor learning rate typically smaller than critic - Common ratio: \\alpha_\\theta = 0.1 \\times \\alpha_w\nNetwork architecture: - Shared layers often beneficial - Separate networks sometimes better for complex tasks\nBatch size: - Larger batches more stable - Smaller batches faster updates\n\n\nCommon Issues\n\nInstability: Actor and critic learning can interfere\nCatastrophic forgetting: Network can forget previous knowledge\nHyperparameter sensitivity: Requires careful tuning\nSample efficiency: Often requires many samples\n\n\n\nDebugging Tips\n\nMonitor value function: Should track true returns\nCheck policy entropy: Should not collapse too quickly\nGradient norms: Should be reasonable magnitude\nAdvantage distribution: Should be centered around zero",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#applications",
    "href": "lessons/expert/10-actor-critic-methods.html#applications",
    "title": "Actor-Critic Methods",
    "section": "Applications",
    "text": "Applications\n\nContinuous control: Robotics, autonomous vehicles\nGame playing: Real-time strategy games\nResource allocation: Cloud computing, power grids\nNatural language processing: Dialogue systems, text generation\nFinance: Trading, portfolio management",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/10-actor-critic-methods.html#key-takeaways",
    "href": "lessons/expert/10-actor-critic-methods.html#key-takeaways",
    "title": "Actor-Critic Methods",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nBest of both worlds: Combines value and policy methods\nVariance reduction: Critic reduces policy gradient variance\nParallel learning: A3C-style parallelism improves efficiency\nStability matters: Methods like PPO provide stable updates\nContinuous control: Excellent for continuous action spaces\n\nActor-critic methods represent a major advance in reinforcement learning, providing a principled way to combine the benefits of both value-based and policy-based approaches. They form the foundation for many state-of-the-art deep RL algorithms!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Actor-Critic Methods"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html",
    "href": "lessons/expert/8-function-approximation.html",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "",
    "text": "Problem: Tabular methods store a separate value for each state (or state-action pair). This doesn’t scale:\n\nChess: ~10^47 states\nGo: ~10^170 states\n\nContinuous control: Infinite states\nAtari games: 210 × 160 × 3 = 100,800 dimensions per frame\n\nSolution: Use function approximation to generalize across states.\nKey insight: Similar states should have similar values. We can use the structure in the problem to generalize from limited experience.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#the-need-for-function-approximation",
    "href": "lessons/expert/8-function-approximation.html#the-need-for-function-approximation",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "",
    "text": "Problem: Tabular methods store a separate value for each state (or state-action pair). This doesn’t scale:\n\nChess: ~10^47 states\nGo: ~10^170 states\n\nContinuous control: Infinite states\nAtari games: 210 × 160 × 3 = 100,800 dimensions per frame\n\nSolution: Use function approximation to generalize across states.\nKey insight: Similar states should have similar values. We can use the structure in the problem to generalize from limited experience.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#function-approximation-paradigm",
    "href": "lessons/expert/8-function-approximation.html#function-approximation-paradigm",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Function Approximation Paradigm",
    "text": "Function Approximation Paradigm\nInstead of storing values in a table, we approximate the value function:\nState value: V(s) \\approx \\hat{V}(s, \\mathbf{w})\nAction value: Q(s,a) \\approx \\hat{Q}(s,a, \\mathbf{w})\nWhere \\mathbf{w} \\in \\mathbb{R}^n is a weight vector with much fewer parameters than states.\nGoal: Find weight vector \\mathbf{w} that makes \\hat{V}(s, \\mathbf{w}) close to V^\\pi(s) for all states s.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#types-of-function-approximation",
    "href": "lessons/expert/8-function-approximation.html#types-of-function-approximation",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Types of Function Approximation",
    "text": "Types of Function Approximation\n\n1. Linear Function Approximation\nForm: \\hat{V}(s, \\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}(s) = \\sum_{i=1}^n w_i x_i(s)\nWhere \\mathbf{x}(s) = [x_1(s), x_2(s), ..., x_n(s)]^T is a feature vector.\nAdvantages: - Simple and well-understood - Guaranteed convergence for many algorithms - Computationally efficient\nDisadvantages: - Limited expressiveness - Requires hand-crafted features\n\n\n2. Nonlinear Function Approximation\nExamples: - Neural networks - Decision trees - Kernel methods - Fourier basis\nAdvantages: - More expressive - Can learn features automatically - Better performance on complex problems\nDisadvantages: - No convergence guarantees - More complex to tune - Computationally expensive",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#feature-engineering",
    "href": "lessons/expert/8-function-approximation.html#feature-engineering",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nThe art of creating good features \\mathbf{x}(s):\n\nPolynomial Features\nFor a 2D position (x, y): \\mathbf{x}(s) = [1, x, y, x^2, y^2, xy, x^3, y^3, x^2y, xy^2, ...]\n\n\nRadial Basis Functions (RBF)\nx_i(s) = \\exp\\left(-\\frac{||s - c_i||^2}{2\\sigma^2}\\right)\nWhere c_i are center points and \\sigma controls the width.\n\n\nFourier Features\nx_i(s) = \\cos(\\pi \\mathbf{c}_i^T \\mathbf{s})\nWhere \\mathbf{c}_i are coefficient vectors.\n\n\nTile Coding\nPartition the state space into overlapping tiles: - Each tile is a binary feature - State activates multiple tiles - Provides good generalization",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#value-function-approximation",
    "href": "lessons/expert/8-function-approximation.html#value-function-approximation",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Value Function Approximation",
    "text": "Value Function Approximation\n\nThe Objective\nWe want to minimize the mean squared error: \\overline{VE}(\\mathbf{w}) = \\sum_{s \\in \\mathcal{S}} \\mu(s) [V^\\pi(s) - \\hat{V}(s, \\mathbf{w})]^2\nWhere \\mu(s) is the state distribution under policy \\pi.\n\n\nGradient Descent\nUpdate rule: \\mathbf{w}_{t+1} = \\mathbf{w}_t - \\frac{1}{2} \\alpha \\nabla_{\\mathbf{w}} [V^\\pi(S_t) - \\hat{V}(S_t, \\mathbf{w}_t)]^2\n= \\mathbf{w}_t + \\alpha [V^\\pi(S_t) - \\hat{V}(S_t, \\mathbf{w}_t)] \\nabla_{\\mathbf{w}} \\hat{V}(S_t, \\mathbf{w}_t)\nProblem: We don’t know V^\\pi(S_t)!\nSolution: Use an estimate as the target.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#td-learning-with-function-approximation",
    "href": "lessons/expert/8-function-approximation.html#td-learning-with-function-approximation",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "TD Learning with Function Approximation",
    "text": "TD Learning with Function Approximation\n\nLinear TD(0)\nUpdate rule: \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma \\hat{V}(S_{t+1}, \\mathbf{w}_t) - \\hat{V}(S_t, \\mathbf{w}_t)] \\nabla_{\\mathbf{w}} \\hat{V}(S_t, \\mathbf{w}_t)\nFor linear approximation: \\nabla_{\\mathbf{w}} \\hat{V}(S_t, \\mathbf{w}_t) = \\mathbf{x}(S_t)\nSo the update becomes: \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma \\mathbf{w}_t^T \\mathbf{x}(S_{t+1}) - \\mathbf{w}_t^T \\mathbf{x}(S_t)] \\mathbf{x}(S_t)\n\n\nConvergence Properties\nLinear TD(0): - Converges to a unique solution - Solution minimizes error weighted by state distribution - Convergence rate depends on feature quality\nNonlinear approximation: - No convergence guarantees - Can diverge or oscillate - Requires careful tuning",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#action-value-function-approximation",
    "href": "lessons/expert/8-function-approximation.html#action-value-function-approximation",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Action-Value Function Approximation",
    "text": "Action-Value Function Approximation\n\nLinear Q-Learning\nFunction approximation: \\hat{Q}(s, a, \\mathbf{w}) = \\mathbf{w}^T \\mathbf{x}(s, a)\nUpdate rule: \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha [R_{t+1} + \\gamma \\max_{a'} \\hat{Q}(S_{t+1}, a', \\mathbf{w}_t) - \\hat{Q}(S_t, A_t, \\mathbf{w}_t)] \\mathbf{x}(S_t, A_t)\nFeature representation options: 1. Separate features: \\mathbf{x}(s, a) = [\\mathbf{x}_1(s), \\mathbf{x}_2(s), ..., \\mathbf{x}_{|\\mathcal{A}|}(s)] 2. Shared features: \\mathbf{x}(s, a) = \\phi(s) \\otimes \\mathbf{e}_a where \\mathbf{e}_a is one-hot encoding",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#deep-q-networks-dqn",
    "href": "lessons/expert/8-function-approximation.html#deep-q-networks-dqn",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Deep Q-Networks (DQN)",
    "text": "Deep Q-Networks (DQN)\nRevolutionary idea: Use deep neural networks for Q-function approximation.\nArchitecture: - Input: Raw state (e.g., game screen) - Hidden layers: Convolutional/fully connected - Output: Q-values for all actions\nKey innovations: 1. Experience replay: Store and sample past experiences 2. Target network: Separate network for computing targets 3. Reward clipping: Clip rewards to [-1, 1]\n\nDQN Algorithm\nInitialize replay buffer D\nInitialize Q-network with weights θ\nInitialize target network with weights θ⁻ = θ\n\nFor each episode:\n    Initialize state s₁\n    For t = 1, 2, ..., T:\n        Choose action aₜ using ε-greedy policy\n        Execute aₜ, observe rₜ, sₜ₊₁\n        Store (sₜ, aₜ, rₜ, sₜ₊₁) in D\n        Sample random minibatch from D\n        Compute target: yⱼ = rⱼ + γ max Q(sⱼ₊₁, a'; θ⁻)\n        Update θ using gradient descent on (yⱼ - Q(sⱼ, aⱼ; θ))²\n        Every C steps: θ⁻ = θ",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#challenges-with-function-approximation",
    "href": "lessons/expert/8-function-approximation.html#challenges-with-function-approximation",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Challenges with Function Approximation",
    "text": "Challenges with Function Approximation\n\n1. Deadly Triad\nThe three things that can cause instability: 1. Function approximation 2. Bootstrapping (using estimates to update estimates) 3. Off-policy learning\nWhen all three are present, learning can diverge.\n\n\n2. Overestimation Bias\nProblem: Max operation in Q-learning leads to overestimation.\nSolution: Double DQN uses two networks: y_t = R_{t+1} + \\gamma Q(S_{t+1}, \\arg\\max_{a'} Q(S_{t+1}, a'; \\theta_t); \\theta_t^-)\n\n\n3. Exploration\nProblem: ε-greedy exploration is inefficient with function approximation.\nBetter approaches: - UCB-based: Use uncertainty estimates for exploration - Thompson sampling: Sample from posterior distribution - Curiosity-driven: Explore based on prediction error",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#policy-approximation",
    "href": "lessons/expert/8-function-approximation.html#policy-approximation",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Policy Approximation",
    "text": "Policy Approximation\nDirectly approximate the policy: \\pi(a|s, \\boldsymbol{\\theta}) = \\text{probability of action } a \\text{ in state } s\nAdvantages: - Can handle continuous action spaces - Can represent stochastic policies - Natural exploration through stochasticity\nDisadvantages: - Higher variance - Slower convergence - Requires policy gradient methods",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#practical-considerations",
    "href": "lessons/expert/8-function-approximation.html#practical-considerations",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Practical Considerations",
    "text": "Practical Considerations\n\nFeature Selection\nGood features should: 1. Be relevant to the value function 2. Provide good generalization 3. Be computationally efficient 4. Avoid redundancy\n\n\nHyperparameter Tuning\nKey hyperparameters: - Step size (α) - Discount factor (γ) - Exploration parameter (ε) - Network architecture - Replay buffer size\n\n\nDebugging\nCommon issues: 1. Divergence: Learning becomes unstable 2. Slow convergence: Takes too long to learn 3. Poor generalization: Overfits to training data 4. Catastrophic forgetting: Forgets old experiences",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#code-example-linear-function-approximation",
    "href": "lessons/expert/8-function-approximation.html#code-example-linear-function-approximation",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Code Example: Linear Function Approximation",
    "text": "Code Example: Linear Function Approximation\nimport numpy as np\n\nclass LinearValueFunction:\n    def __init__(self, num_features, alpha=0.1, gamma=0.95):\n        self.weights = np.zeros(num_features)\n        self.alpha = alpha\n        self.gamma = gamma\n    \n    def value(self, features):\n        return np.dot(self.weights, features)\n    \n    def update(self, features, reward, next_features, done):\n        current_value = self.value(features)\n        \n        if done:\n            target = reward\n        else:\n            target = reward + self.gamma * self.value(next_features)\n        \n        td_error = target - current_value\n        self.weights += self.alpha * td_error * features\n\nclass TileCoding:\n    def __init__(self, num_tilings, tiles_per_dim, state_low, state_high):\n        self.num_tilings = num_tilings\n        self.tiles_per_dim = tiles_per_dim\n        self.state_low = state_low\n        self.state_high = state_high\n        self.num_features = num_tilings * tiles_per_dim\n        \n    def get_features(self, state):\n        features = np.zeros(self.num_features)\n        \n        for i in range(self.num_tilings):\n            # Add offset for this tiling\n            offset = i / self.num_tilings\n            scaled_state = (state - self.state_low) / (self.state_high - self.state_low)\n            tile_index = int((scaled_state + offset) * self.tiles_per_dim) % self.tiles_per_dim\n            features[i * self.tiles_per_dim + tile_index] = 1\n            \n        return features",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#modern-developments",
    "href": "lessons/expert/8-function-approximation.html#modern-developments",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Modern Developments",
    "text": "Modern Developments\n\n1. Deep Reinforcement Learning\nKey algorithms: - DQN: Deep Q-Networks - A3C: Asynchronous Actor-Critic - PPO: Proximal Policy Optimization - SAC: Soft Actor-Critic\n\n\n2. Meta-Learning\nLearning to learn quickly: - MAML (Model-Agnostic Meta-Learning) - Reptile - Few-shot learning\n\n\n3. Representation Learning\nLearning good representations: - Autoencoders - Variational methods - Contrastive learning",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#applications",
    "href": "lessons/expert/8-function-approximation.html#applications",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Applications",
    "text": "Applications\n\nGame playing: Atari, Go, StarCraft\nRobotics: Manipulation, locomotion\nAutonomous driving: Path planning, control\nFinance: Trading, portfolio management\nHealthcare: Treatment recommendation",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/8-function-approximation.html#key-takeaways",
    "href": "lessons/expert/8-function-approximation.html#key-takeaways",
    "title": "Function Approximation in Reinforcement Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nFunction approximation is essential: Required for large-scale RL\nFeature engineering matters: Good features can make or break performance\nLinear methods are stable: Guaranteed convergence but limited expressiveness\nNonlinear methods are powerful: Can solve complex problems but less stable\nDeep RL is the future: Combines representational power with RL algorithms\n\nFunction approximation transforms reinforcement learning from a theoretical framework into a practical tool for solving real-world problems. While it introduces new challenges, the ability to generalize across states makes it indispensable for modern RL applications!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Function Approximation in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/4-bandit-algos.html",
    "href": "lessons/expert/4-bandit-algos.html",
    "title": "Bandit Algorithms",
    "section": "",
    "text": "Let’s delve into the theory behind the multi-armed bandit problem. Choice amongst R options at each time step. Step returns a numerical reward sampled from a reward distribution.\nGoal: Maximize expected total reward over time.\n\nR_t is the reward received at time t.\nA_t be action selected at time t.\na is choice of a given machine/agent.\n\n\n\nq_*(a) = E(R_t|A_t = a)\nBut since we do not know q_*(a), the true action-value function, we must estimate it. If we knew the true value of an agent’s action, q_*(a), for every possible choice of a, we would always pick the a which maximizes it.\n\n\n\nWe denote the estimate by Q(a). Ideally, Q(a) should converge to q_*(a).\nHow do we choose Q_t(a)?\nOne possibility is to use the average (expected value) of all rewards received for action a up to time t:\n\nQ_t(a) = \\frac{\\text{total rewards for }a}{\\text{Number of times }a \\text{ was performed}}\n\n\nQ_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i I(A_i=a)}}{\\sum_{i=1}^{t-1}{I(A_i=a)}}\n\nQ_t(a) \\implies q_*(a) by Law of Large Numbers (LLN).\n\n\n\n\n“Greedy” Action Selection\nA_t = \\arg\\max_a{Q_t(a)} ties ave broken at random Q_t(a) is initialized to be 0, for all a. This means that at time t, we choose the action (A) that gives us the maximum expected reward according to our current estimate of the Q-value for each possible action.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass GreedyBandit:\n    def __init__(self, k=10):\n        self.k = k\n        self.Q = np.zeros(k)  # Action value estimates\n        self.action_counts = np.zeros(k)\n\n    def select_action(self):\n        return np.argmax(self.Q)  # Always choose best current estimate\n\n    def update(self, action, reward):\n        self.action_counts[action] += 1\n        # Incremental update: Q_new = Q_old + (1/n) * (reward - Q_old)\n        self.Q[action] += (reward - self.Q[action]) / self.action_counts[action]\n\n# Demonstrate greedy action selection\nnp.random.seed(42)\nbandit = GreedyBandit(k=5)\n\n# Simulate some rewards (action 3 is actually best with mean reward 2.0)\ntrue_rewards = [0.0, 0.5, 1.0, 2.0, -0.5]\n\nprint(\"Greedy Action Selection Demonstration:\")\nprint(\"True reward means:\", true_rewards)\nprint(\"\\nStep | Action | Reward | Q-values\")\nprint(\"-\" * 40)\n\nfor step in range(10):\n    action = bandit.select_action()\n    reward = np.random.normal(true_rewards[action], 0.1)\n    bandit.update(action, reward)\n\n    print(f\"{step+1:4d} | {action:6d} | {reward:6.2f} | {bandit.Q}\")\n\nprint(f\"\\nFinal action counts: {bandit.action_counts}\")\nprint(\"Notice: Greedy gets stuck on first action that seems good!\")\n\nGreedy Action Selection Demonstration:\nTrue reward means: [0.0, 0.5, 1.0, 2.0, -0.5]\n\nStep | Action | Reward | Q-values\n----------------------------------------\n   1 |      0 |   0.05 | [0.04967142 0.         0.         0.         0.        ]\n   2 |      0 |  -0.01 | [0.01792249 0.         0.         0.         0.        ]\n   3 |      0 |   0.06 | [0.03353795 0.         0.         0.         0.        ]\n   4 |      0 |   0.15 | [0.06322921 0.         0.         0.         0.        ]\n   5 |      0 |  -0.02 | [0.0459003 0.        0.        0.        0.       ]\n   6 |      0 |  -0.02 | [0.03434797 0.         0.         0.         0.        ]\n   7 |      0 |   0.16 | [0.0520013 0.        0.        0.        0.       ]\n   8 |      0 |   0.08 | [0.05509407 0.         0.         0.         0.        ]\n   9 |      0 |  -0.05 | [0.04375612 0.         0.         0.         0.        ]\n  10 |      0 |   0.05 | [0.04480611 0.         0.         0.         0.        ]\n\nFinal action counts: [10.  0.  0.  0.  0.]\nNotice: Greedy gets stuck on first action that seems good!\n\n\n“\\epsilon-Greedy” Method\nUse greedy action selection most of the time. “Occasionally”, with probability \\epsilon &gt; 0, select an action amongst all possible actions uniformly at random. Eventually guaranteed to estimate all q_*(a) (but might not be practical).\n\nclass EpsilonGreedyBandit:\n    def __init__(self, k=10, epsilon=0.1):\n        self.k = k\n        self.epsilon = epsilon\n        self.Q = np.zeros(k)  # Action value estimates\n        self.action_counts = np.zeros(k)\n\n    def select_action(self):\n        if np.random.random() &lt; self.epsilon:\n            return np.random.randint(self.k)  # Explore randomly\n        else:\n            return np.argmax(self.Q)  # Exploit best known action\n\n    def update(self, action, reward):\n        self.action_counts[action] += 1\n        self.Q[action] += (reward - self.Q[action]) / self.action_counts[action]\n\nclass MultiArmedBandit:\n    \"\"\"Environment with k arms, each with different reward distributions\"\"\"\n    def __init__(self, k=10):\n        self.k = k\n        # Random true values for each arm\n        np.random.seed(42)\n        self.true_values = np.random.normal(0, 1, k)\n        self.optimal_action = np.argmax(self.true_values)\n\n    def get_reward(self, action):\n        # Return reward from normal distribution around true value\n        return np.random.normal(self.true_values[action], 1)\n\ndef run_bandit_experiment(epsilon_values, n_steps=1000, n_runs=50):\n    \"\"\"Run bandit experiment for different epsilon values\"\"\"\n    results = {}\n\n    for epsilon in epsilon_values:\n        print(f\"Running experiments for ε = {epsilon}...\")\n        all_rewards = []\n        all_optimal_actions = []\n\n        for run in range(n_runs):\n            env = MultiArmedBandit(k=10)\n            agent = EpsilonGreedyBandit(k=10, epsilon=epsilon)\n\n            rewards = []\n            optimal_actions = []\n\n            for step in range(n_steps):\n                action = agent.select_action()\n                reward = env.get_reward(action)\n                agent.update(action, reward)\n\n                rewards.append(reward)\n                optimal_actions.append(1 if action == env.optimal_action else 0)\n\n            all_rewards.append(rewards)\n            all_optimal_actions.append(optimal_actions)\n\n        # Average across all runs\n        avg_rewards = np.mean(all_rewards, axis=0)\n        avg_optimal = np.mean(all_optimal_actions, axis=0)\n\n        results[epsilon] = {\n            'rewards': avg_rewards,\n            'optimal_actions': avg_optimal\n        }\n\n    return results\n\n# Run experiments for different epsilon values\nepsilon_values = [0.0, 0.01, 0.1, 0.3]\nresults = run_bandit_experiment(epsilon_values, n_steps=1000, n_runs=30)\n\n# Create visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot 1: Average reward over time\nfor epsilon in epsilon_values:\n    # Use moving average for smoother plots\n    window = 50\n    smoothed_rewards = np.convolve(results[epsilon]['rewards'], \n                                 np.ones(window)/window, mode='valid')\n    ax1.plot(range(window-1, len(results[epsilon]['rewards'])), \n            smoothed_rewards, label=f'ε = {epsilon}', linewidth=2)\n\nax1.set_xlabel('Steps')\nax1.set_ylabel('Average Reward')\nax1.set_title('Average Reward vs Time (50-step moving average)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Percentage of optimal actions\nfor epsilon in epsilon_values:\n    # Use moving average for smoother plots\n    smoothed_optimal = np.convolve(results[epsilon]['optimal_actions'], \n                                 np.ones(window)/window, mode='valid')\n    ax2.plot(range(window-1, len(results[epsilon]['optimal_actions'])), \n            smoothed_optimal * 100, label=f'ε = {epsilon}', linewidth=2)\n\nax2.set_xlabel('Steps')\nax2.set_ylabel('% Optimal Action')\nax2.set_title('Optimal Action Selection vs Time (50-step moving average)')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print final performance\nprint(\"\\nFinal Performance (last 100 steps):\")\nprint(\"-\" * 50)\nfor epsilon in epsilon_values:\n    final_reward = np.mean(results[epsilon]['rewards'][-100:])\n    final_optimal = np.mean(results[epsilon]['optimal_actions'][-100:]) * 100\n    print(f\"ε = {epsilon:4.2f}: Avg Reward = {final_reward:.3f}, Optimal Actions = {final_optimal:.1f}%\")\n\nRunning experiments for ε = 0.0...\nRunning experiments for ε = 0.01...\nRunning experiments for ε = 0.1...\nRunning experiments for ε = 0.3...\n\n\n\n\n\nEpsilon-Greedy Performance for Different ε Values\n\n\n\n\n\nFinal Performance (last 100 steps):\n--------------------------------------------------\nε = 0.00: Avg Reward = 1.602, Optimal Actions = 0.0%\nε = 0.01: Avg Reward = 1.549, Optimal Actions = 1.0%\nε = 0.10: Avg Reward = 1.418, Optimal Actions = 1.0%\nε = 0.30: Avg Reward = 1.127, Optimal Actions = 69.0%\n\n\n\n\n\n\nInitial values affect the outcome of a run in both greedy and epsilon-greedy approaches, but in different ways.\nIdea: Setting optimistic initial values (higher than realistic rewards) naturally encourages exploration without adding randomness.\nFor pure greedy method, when Q_t(a) = 5 for instance for all actions (much higher than actual expected rewards/means of the reward distributions):\n\nThe agent starts believing all actions are excellent.\nAfter trying action A, its estimate typically drops below 5\nThis makes untried actions look better by comparison\nEventually, the agent tries all actions at least once\nThen it settles into exploiting what it learned\n\nThink of it like a food critic who starts with unrealistically high expectations for every restaurant. The disappointment after each visit ensures they’ll try every restaurant before settling on favorites.\nWith epsilon-greedy, initial values matter less because:\n\nRandom exploration happens regardless of initial values (\\epsilon portion of the time)\nHigh initial values still affect the exploitation phase (1-\\epsilon portion of the time)\n\nIf Q_t(a) = 5 with epsilon-greedy:\n\nRandom exploration happens anyway (at rate \\epsilon)\nDuring exploitation (1 - \\epsilon), untried actions initially appear better\nThe combination creates more thorough early exploration\n\nThis contradicts the common assumption that epsilon-greedy solves all exploration problems on its own. In practice, optimistic initialization can work synergistically with epsilon-greedy to front-load exploration when it matters most.\nA key limitation worth considering: optimistic initialization only drives temporary exploration. In non-stationary environments where reward distributions change over time, the initial optimism wears off, potentially leading to suboptimal performance if not combined with ongoing exploration mechanisms.\nIn conclusion, initial values matter for determining how much exploration happens early in the learning process.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Bandit Algorithms"
    ]
  },
  {
    "objectID": "lessons/expert/4-bandit-algos.html#multi-armed-bandit-theory",
    "href": "lessons/expert/4-bandit-algos.html#multi-armed-bandit-theory",
    "title": "Bandit Algorithms",
    "section": "",
    "text": "Let’s delve into the theory behind the multi-armed bandit problem. Choice amongst R options at each time step. Step returns a numerical reward sampled from a reward distribution.\nGoal: Maximize expected total reward over time.\n\nR_t is the reward received at time t.\nA_t be action selected at time t.\na is choice of a given machine/agent.\n\n\n\nq_*(a) = E(R_t|A_t = a)\nBut since we do not know q_*(a), the true action-value function, we must estimate it. If we knew the true value of an agent’s action, q_*(a), for every possible choice of a, we would always pick the a which maximizes it.\n\n\n\nWe denote the estimate by Q(a). Ideally, Q(a) should converge to q_*(a).\nHow do we choose Q_t(a)?\nOne possibility is to use the average (expected value) of all rewards received for action a up to time t:\n\nQ_t(a) = \\frac{\\text{total rewards for }a}{\\text{Number of times }a \\text{ was performed}}\n\n\nQ_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i I(A_i=a)}}{\\sum_{i=1}^{t-1}{I(A_i=a)}}\n\nQ_t(a) \\implies q_*(a) by Law of Large Numbers (LLN).\n\n\n\n\n“Greedy” Action Selection\nA_t = \\arg\\max_a{Q_t(a)} ties ave broken at random Q_t(a) is initialized to be 0, for all a. This means that at time t, we choose the action (A) that gives us the maximum expected reward according to our current estimate of the Q-value for each possible action.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass GreedyBandit:\n    def __init__(self, k=10):\n        self.k = k\n        self.Q = np.zeros(k)  # Action value estimates\n        self.action_counts = np.zeros(k)\n\n    def select_action(self):\n        return np.argmax(self.Q)  # Always choose best current estimate\n\n    def update(self, action, reward):\n        self.action_counts[action] += 1\n        # Incremental update: Q_new = Q_old + (1/n) * (reward - Q_old)\n        self.Q[action] += (reward - self.Q[action]) / self.action_counts[action]\n\n# Demonstrate greedy action selection\nnp.random.seed(42)\nbandit = GreedyBandit(k=5)\n\n# Simulate some rewards (action 3 is actually best with mean reward 2.0)\ntrue_rewards = [0.0, 0.5, 1.0, 2.0, -0.5]\n\nprint(\"Greedy Action Selection Demonstration:\")\nprint(\"True reward means:\", true_rewards)\nprint(\"\\nStep | Action | Reward | Q-values\")\nprint(\"-\" * 40)\n\nfor step in range(10):\n    action = bandit.select_action()\n    reward = np.random.normal(true_rewards[action], 0.1)\n    bandit.update(action, reward)\n\n    print(f\"{step+1:4d} | {action:6d} | {reward:6.2f} | {bandit.Q}\")\n\nprint(f\"\\nFinal action counts: {bandit.action_counts}\")\nprint(\"Notice: Greedy gets stuck on first action that seems good!\")\n\nGreedy Action Selection Demonstration:\nTrue reward means: [0.0, 0.5, 1.0, 2.0, -0.5]\n\nStep | Action | Reward | Q-values\n----------------------------------------\n   1 |      0 |   0.05 | [0.04967142 0.         0.         0.         0.        ]\n   2 |      0 |  -0.01 | [0.01792249 0.         0.         0.         0.        ]\n   3 |      0 |   0.06 | [0.03353795 0.         0.         0.         0.        ]\n   4 |      0 |   0.15 | [0.06322921 0.         0.         0.         0.        ]\n   5 |      0 |  -0.02 | [0.0459003 0.        0.        0.        0.       ]\n   6 |      0 |  -0.02 | [0.03434797 0.         0.         0.         0.        ]\n   7 |      0 |   0.16 | [0.0520013 0.        0.        0.        0.       ]\n   8 |      0 |   0.08 | [0.05509407 0.         0.         0.         0.        ]\n   9 |      0 |  -0.05 | [0.04375612 0.         0.         0.         0.        ]\n  10 |      0 |   0.05 | [0.04480611 0.         0.         0.         0.        ]\n\nFinal action counts: [10.  0.  0.  0.  0.]\nNotice: Greedy gets stuck on first action that seems good!\n\n\n“\\epsilon-Greedy” Method\nUse greedy action selection most of the time. “Occasionally”, with probability \\epsilon &gt; 0, select an action amongst all possible actions uniformly at random. Eventually guaranteed to estimate all q_*(a) (but might not be practical).\n\nclass EpsilonGreedyBandit:\n    def __init__(self, k=10, epsilon=0.1):\n        self.k = k\n        self.epsilon = epsilon\n        self.Q = np.zeros(k)  # Action value estimates\n        self.action_counts = np.zeros(k)\n\n    def select_action(self):\n        if np.random.random() &lt; self.epsilon:\n            return np.random.randint(self.k)  # Explore randomly\n        else:\n            return np.argmax(self.Q)  # Exploit best known action\n\n    def update(self, action, reward):\n        self.action_counts[action] += 1\n        self.Q[action] += (reward - self.Q[action]) / self.action_counts[action]\n\nclass MultiArmedBandit:\n    \"\"\"Environment with k arms, each with different reward distributions\"\"\"\n    def __init__(self, k=10):\n        self.k = k\n        # Random true values for each arm\n        np.random.seed(42)\n        self.true_values = np.random.normal(0, 1, k)\n        self.optimal_action = np.argmax(self.true_values)\n\n    def get_reward(self, action):\n        # Return reward from normal distribution around true value\n        return np.random.normal(self.true_values[action], 1)\n\ndef run_bandit_experiment(epsilon_values, n_steps=1000, n_runs=50):\n    \"\"\"Run bandit experiment for different epsilon values\"\"\"\n    results = {}\n\n    for epsilon in epsilon_values:\n        print(f\"Running experiments for ε = {epsilon}...\")\n        all_rewards = []\n        all_optimal_actions = []\n\n        for run in range(n_runs):\n            env = MultiArmedBandit(k=10)\n            agent = EpsilonGreedyBandit(k=10, epsilon=epsilon)\n\n            rewards = []\n            optimal_actions = []\n\n            for step in range(n_steps):\n                action = agent.select_action()\n                reward = env.get_reward(action)\n                agent.update(action, reward)\n\n                rewards.append(reward)\n                optimal_actions.append(1 if action == env.optimal_action else 0)\n\n            all_rewards.append(rewards)\n            all_optimal_actions.append(optimal_actions)\n\n        # Average across all runs\n        avg_rewards = np.mean(all_rewards, axis=0)\n        avg_optimal = np.mean(all_optimal_actions, axis=0)\n\n        results[epsilon] = {\n            'rewards': avg_rewards,\n            'optimal_actions': avg_optimal\n        }\n\n    return results\n\n# Run experiments for different epsilon values\nepsilon_values = [0.0, 0.01, 0.1, 0.3]\nresults = run_bandit_experiment(epsilon_values, n_steps=1000, n_runs=30)\n\n# Create visualizations\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# Plot 1: Average reward over time\nfor epsilon in epsilon_values:\n    # Use moving average for smoother plots\n    window = 50\n    smoothed_rewards = np.convolve(results[epsilon]['rewards'], \n                                 np.ones(window)/window, mode='valid')\n    ax1.plot(range(window-1, len(results[epsilon]['rewards'])), \n            smoothed_rewards, label=f'ε = {epsilon}', linewidth=2)\n\nax1.set_xlabel('Steps')\nax1.set_ylabel('Average Reward')\nax1.set_title('Average Reward vs Time (50-step moving average)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Percentage of optimal actions\nfor epsilon in epsilon_values:\n    # Use moving average for smoother plots\n    smoothed_optimal = np.convolve(results[epsilon]['optimal_actions'], \n                                 np.ones(window)/window, mode='valid')\n    ax2.plot(range(window-1, len(results[epsilon]['optimal_actions'])), \n            smoothed_optimal * 100, label=f'ε = {epsilon}', linewidth=2)\n\nax2.set_xlabel('Steps')\nax2.set_ylabel('% Optimal Action')\nax2.set_title('Optimal Action Selection vs Time (50-step moving average)')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print final performance\nprint(\"\\nFinal Performance (last 100 steps):\")\nprint(\"-\" * 50)\nfor epsilon in epsilon_values:\n    final_reward = np.mean(results[epsilon]['rewards'][-100:])\n    final_optimal = np.mean(results[epsilon]['optimal_actions'][-100:]) * 100\n    print(f\"ε = {epsilon:4.2f}: Avg Reward = {final_reward:.3f}, Optimal Actions = {final_optimal:.1f}%\")\n\nRunning experiments for ε = 0.0...\nRunning experiments for ε = 0.01...\nRunning experiments for ε = 0.1...\nRunning experiments for ε = 0.3...\n\n\n\n\n\nEpsilon-Greedy Performance for Different ε Values\n\n\n\n\n\nFinal Performance (last 100 steps):\n--------------------------------------------------\nε = 0.00: Avg Reward = 1.602, Optimal Actions = 0.0%\nε = 0.01: Avg Reward = 1.549, Optimal Actions = 1.0%\nε = 0.10: Avg Reward = 1.418, Optimal Actions = 1.0%\nε = 0.30: Avg Reward = 1.127, Optimal Actions = 69.0%\n\n\n\n\n\n\nInitial values affect the outcome of a run in both greedy and epsilon-greedy approaches, but in different ways.\nIdea: Setting optimistic initial values (higher than realistic rewards) naturally encourages exploration without adding randomness.\nFor pure greedy method, when Q_t(a) = 5 for instance for all actions (much higher than actual expected rewards/means of the reward distributions):\n\nThe agent starts believing all actions are excellent.\nAfter trying action A, its estimate typically drops below 5\nThis makes untried actions look better by comparison\nEventually, the agent tries all actions at least once\nThen it settles into exploiting what it learned\n\nThink of it like a food critic who starts with unrealistically high expectations for every restaurant. The disappointment after each visit ensures they’ll try every restaurant before settling on favorites.\nWith epsilon-greedy, initial values matter less because:\n\nRandom exploration happens regardless of initial values (\\epsilon portion of the time)\nHigh initial values still affect the exploitation phase (1-\\epsilon portion of the time)\n\nIf Q_t(a) = 5 with epsilon-greedy:\n\nRandom exploration happens anyway (at rate \\epsilon)\nDuring exploitation (1 - \\epsilon), untried actions initially appear better\nThe combination creates more thorough early exploration\n\nThis contradicts the common assumption that epsilon-greedy solves all exploration problems on its own. In practice, optimistic initialization can work synergistically with epsilon-greedy to front-load exploration when it matters most.\nA key limitation worth considering: optimistic initialization only drives temporary exploration. In non-stationary environments where reward distributions change over time, the initial optimism wears off, potentially leading to suboptimal performance if not combined with ongoing exploration mechanisms.\nIn conclusion, initial values matter for determining how much exploration happens early in the learning process.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Bandit Algorithms"
    ]
  },
  {
    "objectID": "lessons/expert/4-bandit-algos.html#gradient-bandit-algorithms",
    "href": "lessons/expert/4-bandit-algos.html#gradient-bandit-algorithms",
    "title": "Bandit Algorithms",
    "section": "Gradient Bandit Algorithms",
    "text": "Gradient Bandit Algorithms\nSo far we estimate action values to guide our policy.\n\nAnalogy\nConcept: Rather than asking “what’s the absolute value of each action?”, gradient bandits ask “how much better is one action compared to others?”\nThink of it like ranking restaurants rather than rating them on a fixed scale. You might not know if a restaurant deserves 3 or 4 stars absolutely, but you can more easily say you prefer it over another one.\n\n\nTheory\nIdea: For action a, learn a preference H_t(a). -&gt; Not interpretable as an expected reward.\nActions with higher preferences are more likely to be taken at a given time.\nTo convert preferences into decisions, we use a softmax transform. In other words, we are mapping states to action probabilities.\nPolicy: The probability distribution over actions P(A_t = a) = \\pi_t(a). This policy changes over time as more action is taken (hence the t).\nBelow is the softmax function.\n\nP(A_t = a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^R e^{H_t(b)}}=\\pi_t(a)\n\nwhere H_t(a) is a numerical preference value of an action a\nDo a kind of gradient ascent step.\n\nStep-size \\alpha &gt; 0, where \\alpha is the step-size hyperparameter\nAfter choosing an action A_t and observing reward, R_t, update our preference as follows:\n\nIncrease the preference for the selected action if the reward is better than expected (that is the average)\nDecrease the preference for the selected action if the reward was worse than expected (the average)\n\nThe update rule is:\n\nUpdate the preference for the action you took\nH_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar R_t)(1-\\pi_t(A_t))\n\nwhere A_t is the action we took. It is the specific action that was selected and taken at time t.\n\n\nUpdate the preference for all other actions you didn’t take\nH_{t+1}(a) = H_t(a) - \\alpha(R_t - \\bar R_t)\\pi_t(a)\n\nNote that, a \\neq A_t and \\bar R_t is the average of rewards not including time t. a is any action in the action space.\n\n\n\n\n\nNote: On baselines (in our case \\bar R_t):\nSay we select our expected rewards from a normal distribution N(u, 1) and not N(0, 1). \\implies Algorithm adapts rapidly.\nThe below are questions assuming we ignore the baseline.\nH_{t}(A_t) = H_t(A_t) + \\alpha(R_t)(1-\\pi_t(A_t))\nH_{t}(a) = H_t(a) - \\alpha(R_t)\\pi_t(a)\n\nwhere A_t \\neq a.\n\nThe baseline helps us to make the correct decision. It can be used to compare the action we took with the average reward of all actions. This helps us to determine if the action we took was better or worse than expected.\nPerforms way worse: No reason to not use the baseline.\n\nclass GradientBandit:\n    def __init__(self, k=10, alpha=0.1, use_baseline=True):\n        self.k = k\n        self.alpha = alpha\n        self.use_baseline = use_baseline\n        self.H = np.zeros(k)  # Action preferences\n        self.reward_sum = 0\n        self.t = 0  # Time step\n        \n    def get_probabilities(self):\n        \"\"\"Convert preferences to action probabilities using softmax\"\"\"\n        exp_H = np.exp(self.H - np.max(self.H))  # Subtract max for numerical stability\n        return exp_H / np.sum(exp_H)\n    \n    def select_action(self):\n        \"\"\"Select action based on current probabilities\"\"\"\n        probabilities = self.get_probabilities()\n        return np.random.choice(self.k, p=probabilities)\n    \n    def update(self, action, reward):\n        \"\"\"Update preferences based on reward\"\"\"\n        self.t += 1\n        self.reward_sum += reward\n        \n        # Calculate baseline (average reward so far)\n        baseline = self.reward_sum / self.t if self.use_baseline else 0\n        \n        # Get current probabilities\n        probabilities = self.get_probabilities()\n        \n        # Update preferences\n        for a in range(self.k):\n            if a == action:\n                # Increase preference for selected action\n                self.H[a] += self.alpha * (reward - baseline) * (1 - probabilities[a])\n            else:\n                # Decrease preference for non-selected actions\n                self.H[a] -= self.alpha * (reward - baseline) * probabilities[a]\n\ndef run_gradient_bandit_experiment(n_steps=1000, n_runs=50):\n    \"\"\"Compare gradient bandit with and without baseline\"\"\"\n    \n    # Test different alpha values\n    alpha_values = [0.1, 0.4]\n    baseline_options = [True, False]\n    \n    results = {}\n    \n    for alpha in alpha_values:\n        for use_baseline in baseline_options:\n            key = f\"α={alpha}, baseline={use_baseline}\"\n            print(f\"Running gradient bandit: {key}\")\n            \n            all_optimal_actions = []\n            \n            for run in range(n_runs):\n                env = MultiArmedBandit(k=10)\n                agent = GradientBandit(k=10, alpha=alpha, use_baseline=use_baseline)\n                \n                optimal_actions = []\n                \n                for step in range(n_steps):\n                    action = agent.select_action()\n                    reward = env.get_reward(action)\n                    agent.update(action, reward)\n                    \n                    optimal_actions.append(1 if action == env.optimal_action else 0)\n                \n                all_optimal_actions.append(optimal_actions)\n            \n            # Average across runs\n            avg_optimal = np.mean(all_optimal_actions, axis=0)\n            results[key] = avg_optimal\n    \n    return results\n\n# Run gradient bandit experiments\ngradient_results = run_gradient_bandit_experiment(n_steps=1000, n_runs=30)\n\n# Visualize results\nplt.figure(figsize=(12, 6))\n\ncolors = ['blue', 'red', 'green', 'orange']\nstyles = ['-', '--', '-', '--']\n\nfor i, (key, optimal_actions) in enumerate(gradient_results.items()):\n    # Use moving average for smoother plots\n    window = 50\n    smoothed = np.convolve(optimal_actions, np.ones(window)/window, mode='valid')\n    plt.plot(range(window-1, len(optimal_actions)), smoothed * 100, \n             label=key, color=colors[i], linestyle=styles[i], linewidth=2)\n\nplt.xlabel('Steps')\nplt.ylabel('% Optimal Action')\nplt.title('Gradient Bandit Algorithm: Effect of Baseline and Learning Rate')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Show how preferences evolve\ndef demonstrate_preference_evolution():\n    \"\"\"Show how action preferences evolve over time\"\"\"\n    np.random.seed(42)\n    env = MultiArmedBandit(k=4)  # Smaller bandit for clearer visualization\n    agent = GradientBandit(k=4, alpha=0.4, use_baseline=True)\n    \n    preference_history = []\n    probability_history = []\n    \n    for step in range(200):\n        preference_history.append(agent.H.copy())\n        probability_history.append(agent.get_probabilities().copy())\n        \n        action = agent.select_action()\n        reward = env.get_reward(action)\n        agent.update(action, reward)\n    \n    # Plot preference evolution\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n    \n    preference_history = np.array(preference_history)\n    probability_history = np.array(probability_history)\n    \n    # Plot preferences\n    for a in range(4):\n        ax1.plot(preference_history[:, a], label=f'Action {a}', linewidth=2)\n    ax1.set_xlabel('Steps')\n    ax1.set_ylabel('Preference H(a)')\n    ax1.set_title('Evolution of Action Preferences')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    \n    # Plot probabilities\n    for a in range(4):\n        ax2.plot(probability_history[:, a] * 100, label=f'Action {a}', linewidth=2)\n    ax2.set_xlabel('Steps')\n    ax2.set_ylabel('Probability (%)')\n    ax2.set_title('Evolution of Action Probabilities')\n    ax2.legend()\n    ax2.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"True reward means: {env.true_values}\")\n    print(f\"Optimal action: {env.optimal_action}\")\n    print(f\"Final preferences: {agent.H}\")\n    print(f\"Final probabilities: {[f'{p:.1f}%' for p in agent.get_probabilities() * 100]}\")\n\ndemonstrate_preference_evolution()\n\nRunning gradient bandit: α=0.1, baseline=True\nRunning gradient bandit: α=0.1, baseline=False\nRunning gradient bandit: α=0.4, baseline=True\nRunning gradient bandit: α=0.4, baseline=False\n\n\n\n\n\nGradient Bandit Algorithm with and without Baseline\n\n\n\n\n\n\n\n\n\n\n\nTrue reward means: [ 0.49671415 -0.1382643   0.64768854  1.52302986]\nOptimal action: 3\nFinal preferences: [-1.29587492 -1.84274748 -1.08631467  4.22493708]\nFinal probabilities: ['0.4%', '0.2%', '0.5%', '98.9%']\n\n\nComment: The gradient bandit algorithm can be viewed as an approximation to an exact algorithm.\nH_{t+1}(a) = H_t(a) + \\frac{dE(R_t)}{dH_ta)}\nE(R_t) = \\sum_{x}\\pi_t(x)q_*(x)\nwhere q_*(x) is the true action value of action x which we don’t know.\nOur Algorithm earlier approximates these updates in expectation.\n\n\nUpper Confidence Bound (UCB) Action Selection\nIdea: If an action is nearly optimal, but has not been selected much, give it a boost.\nRule: A_t = \\arg\\max_a{Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}}}\nwhere Q_t(a) is the action-value estimates as before and c is a tunable constant.\nIf N_t(a)=0, we always select it.\n\n\nWhat about Non-stationarity in Bandit problems?\nQ_n = \\sum_{i=1}^{n-1}R_i - Action-value estimate\nQ_{n+1} = Q_n + \\frac{1}{n}(R_n - Q_n) - Incremental way of computing Q_n.\nThis lead to an interpretation of \\frac{1}{n} as A step size parameter that goes to 0.\nNot good if the environment undergoes a change, as you will be very slow to adapt to this change.\nA single solution is to use a constant step size.\nQ_{n+1} = Q_n + \\alpha(R_n - Q_n)\n\\implies (1+alpha)Q_1 + ... + \\sum_{i=1}^{n}\\alpha(1-alpha)^{n-i}R_i\nwhere \\alpha is a constant step size. \\alpha quantifies the tradeoff between how much you remember and how much you forget. \\alpha = 1 means we only pay attention to the last observed reward.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Bandit Algorithms"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html",
    "href": "lessons/expert/6-monte-carlo-methods.html",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "",
    "text": "Monte Carlo methods are a class of reinforcement learning algorithms that learn directly from episodes of experience without requiring a model of the environment. They are named after the famous casino, as they rely on random sampling and statistical averaging.\nKey insight: Instead of using mathematical models (like Dynamic Programming), we learn from actual experience by running episodes and averaging the results.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#introduction-to-monte-carlo-methods",
    "href": "lessons/expert/6-monte-carlo-methods.html#introduction-to-monte-carlo-methods",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "",
    "text": "Monte Carlo methods are a class of reinforcement learning algorithms that learn directly from episodes of experience without requiring a model of the environment. They are named after the famous casino, as they rely on random sampling and statistical averaging.\nKey insight: Instead of using mathematical models (like Dynamic Programming), we learn from actual experience by running episodes and averaging the results.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#the-monte-carlo-approach",
    "href": "lessons/expert/6-monte-carlo-methods.html#the-monte-carlo-approach",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "The Monte Carlo Approach",
    "text": "The Monte Carlo Approach\nBasic idea: To estimate the value of a state, simply average the returns observed after visiting that state.\nWhy this works: By the Law of Large Numbers, the average of a large number of samples converges to the true expected value.\nV^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s] \\approx \\frac{1}{N} \\sum_{i=1}^N G_t^{(i)}\nWhere G_t^{(i)} is the return from the i-th visit to state s.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#key-assumptions",
    "href": "lessons/expert/6-monte-carlo-methods.html#key-assumptions",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Key Assumptions",
    "text": "Key Assumptions\n\nEpisodic tasks: Episodes must terminate (we need complete returns)\nFinite episodes: Each episode eventually ends\nStationary environment: The environment doesn’t change over time",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#monte-carlo-prediction",
    "href": "lessons/expert/6-monte-carlo-methods.html#monte-carlo-prediction",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Monte Carlo Prediction",
    "text": "Monte Carlo Prediction\n\nFirst-Visit Monte Carlo\nAlgorithm: For each state, average returns only from the first time that state is visited in each episode.\nInitialize:\n    V(s) = 0 for all s ∈ S\n    Returns(s) = empty list for all s ∈ S\n\nFor each episode:\n    Generate episode: S₀, A₀, R₁, S₁, A₁, R₂, ..., Sₜ₋₁, Aₜ₋₁, Rₜ\n    G = 0\n    For t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        If S_t does not appear in S₀, S₁, ..., S_{t-1}:\n            Append G to Returns(S_t)\n            V(S_t) = average(Returns(S_t))\n\n\nEvery-Visit Monte Carlo\nAlgorithm: For each state, average returns from every visit to that state.\nInitialize:\n    V(s) = 0 for all s ∈ S\n    Returns(s) = empty list for all s ∈ S\n\nFor each episode:\n    Generate episode: S₀, A₀, R₁, S₁, A₁, R₂, ..., Sₜ₋₁, Aₜ₋₁, Rₜ\n    G = 0\n    For t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        Append G to Returns(S_t)\n        V(S_t) = average(Returns(S_t))\n\n\nConvergence Properties\nFirst-Visit MC: Converges to V^\\pi(s) as the number of first visits to s approaches infinity.\nEvery-Visit MC: Also converges to V^\\pi(s) under the same conditions.\nPractical difference: Often minimal, but every-visit MC can converge faster for some problems.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#example-blackjack",
    "href": "lessons/expert/6-monte-carlo-methods.html#example-blackjack",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Example: Blackjack",
    "text": "Example: Blackjack\nPerfect example for MC methods: - Natural episodes (hands of blackjack) - Clear terminal states (win/lose/draw) - Reward only at the end of episode\nState representation: (player sum, dealer showing card, usable ace)\nResults after 500,000 episodes: - States visited frequently: accurate value estimates - States visited rarely: less accurate estimates - Natural exploration through random policy",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#monte-carlo-control",
    "href": "lessons/expert/6-monte-carlo-methods.html#monte-carlo-control",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Monte Carlo Control",
    "text": "Monte Carlo Control\nGoal: Find optimal policy using Monte Carlo methods.\nChallenge: We need to ensure adequate exploration of all state-action pairs.\n\nMonte Carlo with Exploring Starts\nKey idea: Every state-action pair has a non-zero probability of being the starting pair of an episode.\nInitialize:\n    Q(s,a) = 0 for all s ∈ S, a ∈ A\n    Returns(s,a) = empty list for all s ∈ S, a ∈ A\n    π(s) = arbitrary policy\n\nFor each episode:\n    Choose random S₀ ∈ S and A₀ ∈ A (exploring starts)\n    Generate episode: S₀, A₀, R₁, S₁, A₁, R₂, ..., Sₜ₋₁, Aₜ₋₁, Rₜ\n    G = 0\n    For t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        If (S_t, A_t) does not appear in (S₀,A₀), (S₁,A₁), ..., (S_{t-1},A_{t-1}):\n            Append G to Returns(S_t, A_t)\n            Q(S_t, A_t) = average(Returns(S_t, A_t))\n    \n    For each s in episode:\n        π(s) = argmax_a Q(s,a)\nProblem: Exploring starts assumption is often unrealistic in practice.\n\n\nOn-Policy Monte Carlo Control\nWithout exploring starts: Use \\epsilon-greedy policies to ensure exploration.\n\\epsilon-greedy policy: \\pi(a|s) = \\begin{cases}\n1 - \\epsilon + \\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{if } a = \\arg\\max_{a'} Q(s,a') \\\\\n\\frac{\\epsilon}{|\\mathcal{A}(s)|} & \\text{otherwise}\n\\end{cases}\nInitialize:\n    Q(s,a) = 0 for all s ∈ S, a ∈ A\n    Returns(s,a) = empty list for all s ∈ S, a ∈ A\n    π = ε-greedy policy derived from Q\n\nFor each episode:\n    Generate episode using π: S₀, A₀, R₁, S₁, A₁, R₂, ..., Sₜ₋₁, Aₜ₋₁, Rₜ\n    G = 0\n    For t = T-1, T-2, ..., 0:\n        G = γG + R_{t+1}\n        If (S_t, A_t) not in (S₀,A₀), ..., (S_{t-1},A_{t-1}):\n            Append G to Returns(S_t, A_t)\n            Q(S_t, A_t) = average(Returns(S_t, A_t))\n            A* = argmax_a Q(S_t, a)\n            Update π to be ε-greedy w.r.t. Q\n\n\nOff-Policy Monte Carlo Control\nKey insight: Use one policy to generate episodes (behavior policy b) and another to evaluate/improve (target policy \\pi).\nImportance sampling: Weight returns by the probability ratio between policies.\n\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}\nWeighted importance sampling: V^\\pi(s) = \\frac{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1} G_t}{\\sum_{t \\in \\mathcal{T}(s)} \\rho_{t:T(t)-1}}\nWhere \\mathcal{T}(s) is the set of all time steps in which state s is visited.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#advantages-of-monte-carlo-methods",
    "href": "lessons/expert/6-monte-carlo-methods.html#advantages-of-monte-carlo-methods",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Advantages of Monte Carlo Methods",
    "text": "Advantages of Monte Carlo Methods\n\nModel-free: No need to know transition probabilities or rewards\nUnbiased: Estimates are unbiased (average of samples equals true value)\nSimple: Easy to understand and implement\nFlexible: Can handle any MDP structure\nStatistically independent: Value estimates for different states are independent",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#disadvantages-of-monte-carlo-methods",
    "href": "lessons/expert/6-monte-carlo-methods.html#disadvantages-of-monte-carlo-methods",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Disadvantages of Monte Carlo Methods",
    "text": "Disadvantages of Monte Carlo Methods\n\nHigh variance: Estimates can vary significantly between episodes\nSlow convergence: Need many episodes for accurate estimates\nEpisodic only: Can’t handle continuing tasks directly\nDelayed learning: Must wait until episode ends to update\nRare states: States visited infrequently have poor estimates",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#reducing-variance",
    "href": "lessons/expert/6-monte-carlo-methods.html#reducing-variance",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Reducing Variance",
    "text": "Reducing Variance\n\nIncremental Updates\nInstead of storing all returns, update estimates incrementally:\nV(S_t) \\leftarrow V(S_t) + \\alpha[G_t - V(S_t)]\nWhere \\alpha is a step size parameter.\n\n\nBaseline Methods\nUse a baseline to reduce variance: G_t - b(S_t)\nWhere b(S_t) is some baseline function.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#comparison-with-dynamic-programming",
    "href": "lessons/expert/6-monte-carlo-methods.html#comparison-with-dynamic-programming",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Comparison with Dynamic Programming",
    "text": "Comparison with Dynamic Programming\n\n\n\nAspect\nMonte Carlo\nDynamic Programming\n\n\n\n\nModel requirement\nModel-free\nRequires model\n\n\nBootstrapping\nNo\nYes\n\n\nBias\nUnbiased\nBiased (initially)\n\n\nVariance\nHigh\nLow\n\n\nConvergence\nSlow\nFast\n\n\nEpisodes\nMust complete\nCan be continuous",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#code-example-monte-carlo-policy-evaluation",
    "href": "lessons/expert/6-monte-carlo-methods.html#code-example-monte-carlo-policy-evaluation",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Code Example: Monte Carlo Policy Evaluation",
    "text": "Code Example: Monte Carlo Policy Evaluation\nimport numpy as np\nfrom collections import defaultdict\n\ndef mc_policy_evaluation(env, policy, num_episodes=100000, gamma=1.0):\n    \"\"\"\n    Monte Carlo policy evaluation using first-visit MC\n    \"\"\"\n    # Initialize\n    V = defaultdict(float)\n    returns = defaultdict(list)\n    \n    for episode in range(num_episodes):\n        # Generate episode\n        states, actions, rewards = generate_episode(env, policy)\n        \n        # Calculate returns\n        G = 0\n        visited_states = set()\n        \n        # Work backwards through episode\n        for t in range(len(states) - 1, -1, -1):\n            G = gamma * G + rewards[t]\n            \n            # First-visit MC\n            if states[t] not in visited_states:\n                visited_states.add(states[t])\n                returns[states[t]].append(G)\n                V[states[t]] = np.mean(returns[states[t]])\n    \n    return dict(V)\n\ndef generate_episode(env, policy):\n    \"\"\"Generate a single episode following given policy\"\"\"\n    states, actions, rewards = [], [], []\n    state = env.reset()\n    \n    while True:\n        action = policy(state)\n        states.append(state)\n        actions.append(action)\n        \n        state, reward, done, _ = env.step(action)\n        rewards.append(reward)\n        \n        if done:\n            break\n    \n    return states, actions, rewards",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#applications",
    "href": "lessons/expert/6-monte-carlo-methods.html#applications",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Applications",
    "text": "Applications\n\nGame playing: Learning to play games like Blackjack, Go\nFinancial modeling: Portfolio optimization, risk assessment\nRobotics: Learning motor skills through trial and error\nA/B testing: Evaluating different strategies\nSimulation optimization: When analytic solutions are intractable",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#historical-context",
    "href": "lessons/expert/6-monte-carlo-methods.html#historical-context",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Historical Context",
    "text": "Historical Context\n\nOrigin: Developed in 1940s for nuclear weapons research\nRL application: Introduced by Sutton and Barto in 1980s\nKey insight: Learning from experience without models\nModern relevance: Foundation for many current RL algorithms",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/6-monte-carlo-methods.html#key-takeaways",
    "href": "lessons/expert/6-monte-carlo-methods.html#key-takeaways",
    "title": "Monte Carlo Methods in Reinforcement Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nExperience-based learning: MC methods learn from actual episodes\nNo model required: Work directly with environment interactions\nUnbiased estimates: Sample averages converge to true values\nHigh variance: Need many samples for accurate estimates\nExploration crucial: Must visit all states/actions sufficiently\n\nMonte Carlo methods provide a crucial bridge between dynamic programming (which requires models) and temporal difference learning (which learns from partial episodes). They demonstrate that we can learn optimal behavior purely from experience!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Monte Carlo Methods in Reinforcement Learning"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html",
    "href": "lessons/expert/1-rl-part-2.html",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "",
    "text": "Reinforcement learning (RL) is a framework for learning optimal behavior through trial-and-error interaction with an environment. Unlike supervised learning (where we have correct answers) or unsupervised learning (where we find patterns), RL learns from rewards and penalties.\nThink of it like learning to drive: You don’t get a manual with every possible scenario - instead, you learn by trying actions (steering, braking) and experiencing consequences (smooth ride, honking horns, accidents). Over time, you develop a driving policy that maximizes good outcomes.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#what-is-reinforcement-learning",
    "href": "lessons/expert/1-rl-part-2.html#what-is-reinforcement-learning",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "",
    "text": "Reinforcement learning (RL) is a framework for learning optimal behavior through trial-and-error interaction with an environment. Unlike supervised learning (where we have correct answers) or unsupervised learning (where we find patterns), RL learns from rewards and penalties.\nThink of it like learning to drive: You don’t get a manual with every possible scenario - instead, you learn by trying actions (steering, braking) and experiencing consequences (smooth ride, honking horns, accidents). Over time, you develop a driving policy that maximizes good outcomes.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#the-rl-framework-agent-environment-interaction",
    "href": "lessons/expert/1-rl-part-2.html#the-rl-framework-agent-environment-interaction",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "The RL Framework: Agent-Environment Interaction",
    "text": "The RL Framework: Agent-Environment Interaction\n\n\n\nRL Agent-Environment Loop\n\n\nFigure 1: The fundamental RL loop where an agent takes actions in an environment and receives observations and rewards\n\nCore Components\nAgent: The learner/decision-maker (you, the driver)\nEnvironment: Everything outside the agent (road, other cars, traffic laws)\nState (s_t): The true state of the environment at time t (positions of all cars, traffic light status)\nObservation (o_t): What the agent actually perceives (what you see through your windshield)\nAction (a_t): Choice made by the agent (turn left, brake, accelerate)\nReward (r_t): Feedback signal from environment (+1 for reaching destination, -100 for accident)\n\n\nThe Markov Property: Why States Matter\nKey assumption: The future depends only on the current state, not the entire history.\nP(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, ...) = P(s_{t+1} | s_t, a_t)\nDriving analogy: To predict what happens next, you only need to know current positions and speeds, not how cars got there. This makes the problem tractable - otherwise we’d need infinite memory.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#policies-and-value-functions",
    "href": "lessons/expert/1-rl-part-2.html#policies-and-value-functions",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "Policies and Value Functions",
    "text": "Policies and Value Functions\n\nPolicy (\\pi)\nA policy maps states to actions: \\pi(a|s) = P(A_t = a | S_t = s)\n\nDeterministic policy: a = \\pi(s) (always take same action in same state)\nStochastic policy: \\pi(a|s) (probability distribution over actions)\n\n\n\nValue Functions: Measuring Long-term Success\nThe state value function measures expected cumulative discounted reward:\nV^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s\\right]\nWhere: - \\gamma \\in [0,1] is the discount factor (future rewards matter less) - R_{t+k+1} is the reward at time t+k+1\nWhy discounting? 1. Uncertainty increases with time 2. Immediate rewards often preferred (bird in hand…) 3. Mathematical convenience (prevents infinite sums)\n\n\nAction-Value Function (Q-function)\nQ^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\mid S_t = s, A_t = a\\right]\nThis tells us: “How good is taking action a in state s, then following policy \\pi?”",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#the-optimal-policy",
    "href": "lessons/expert/1-rl-part-2.html#the-optimal-policy",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "The Optimal Policy",
    "text": "The Optimal Policy\nWe seek the policy that maximizes expected return:\n\\pi^* = \\arg\\max_\\pi \\mathbb{E}_{s_0 \\sim \\rho}[V^\\pi(s_0)]\nWhere \\rho is the distribution over initial states.\nBellman Optimality Equation: V^*(s) = \\max_a \\sum_{s'} P(s'|s,a)[R(s,a,s') + \\gamma V^*(s')]\nThis recursive relationship is the foundation of many RL algorithms.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#exploration-vs.-exploitation-the-central-dilemma",
    "href": "lessons/expert/1-rl-part-2.html#exploration-vs.-exploitation-the-central-dilemma",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "Exploration vs. Exploitation: The Central Dilemma",
    "text": "Exploration vs. Exploitation: The Central Dilemma\n\n\n\nExploration vs Exploitation Trade-off\n\n\nFigure 2: The exploration-exploitation dilemma - balancing trying new actions vs. using known good actions\nThe dilemma: Should you:\n- Exploit: Use current knowledge to maximize immediate reward\n- Explore: Try new actions to potentially find better options\nRestaurant analogy: You know one good restaurant (exploit) but there might be amazing places you haven’t tried (explore). Pure exploitation means you might miss the best restaurant in town. Pure exploration means constantly eating at mediocre new places.\n\nEpsilon-Greedy Strategy\n\nWith probability 1-\\epsilon: choose best known action (exploit)\nWith probability \\epsilon: choose random action (explore)",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#multi-armed-bandits-rl-without-states",
    "href": "lessons/expert/1-rl-part-2.html#multi-armed-bandits-rl-without-states",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "Multi-Armed Bandits: RL Without States",
    "text": "Multi-Armed Bandits: RL Without States\nBandits are a special case where: - No state transitions (each action is independent) - No temporal dependencies - Pure exploration vs. exploitation problem\n\n\n\nMulti-Armed Bandit\n\n\nFigure 3: Multi-armed bandit - multiple slot machines with unknown payout rates\nKey difference from full RL: In bandits, your choice of slot machine doesn’t affect which machines are available next. In full RL, your actions change the state and future options.\n\nUpper Confidence Bound (UCB)\nChoose action that maximizes: \\hat{\\mu}_a + \\sqrt{\\frac{2\\ln t}{n_a}}\nWhere: - \\hat{\\mu}_a = estimated mean reward for action a - n_a = number of times action a was chosen - t = total number of rounds\nThe square root term represents uncertainty - actions tried less often get exploration bonus.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#learning-approaches",
    "href": "lessons/expert/1-rl-part-2.html#learning-approaches",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "Learning Approaches",
    "text": "Learning Approaches\n\nModel-Based vs. Model-Free\nModel-Based: Learn environment dynamics P(s'|s,a) and R(s,a), then plan - Like studying road maps before driving - Can be sample efficient but computationally expensive\nModel-Free: Learn policy/values directly from experience - Like learning to drive just by practicing - Less sample efficient but often more practical\n\n\nTemporal Difference Learning\nKey insight: We don’t need to wait until episode ends to learn!\nTD Error: \\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)\nQ-Learning Update: Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha[r_{t+1} + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]\nThis lets us learn from every single step, not just complete episodes.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#common-pitfalls-and-misconceptions",
    "href": "lessons/expert/1-rl-part-2.html#common-pitfalls-and-misconceptions",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "Common Pitfalls and Misconceptions",
    "text": "Common Pitfalls and Misconceptions\n\nState vs. Observation confusion: The agent rarely sees the full state\nAssuming deterministic environments: Most real environments have randomness\nIgnoring exploration: Greedy policies often get stuck in local optima\nReward hacking: Agents optimize exactly what you specify, not what you intend",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#real-world-applications",
    "href": "lessons/expert/1-rl-part-2.html#real-world-applications",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "Real-World Applications",
    "text": "Real-World Applications\n\nAutonomous driving: States = traffic situations, Actions = steering/speed control\nGame playing: AlphaGo, StarCraft II agents\nRecommendation systems: States = user preferences, Actions = what to recommend\nResource allocation: Cloud computing, power grid management\nRobotics: Learning motor skills, manipulation",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/1-rl-part-2.html#key-takeaways",
    "href": "lessons/expert/1-rl-part-2.html#key-takeaways",
    "title": "Reinforcement Learning Expanded Introduction",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nRL solves sequential decision problems through trial-and-error\nThe Markov property makes problems tractable\nBalancing exploration and exploitation is crucial\nValue functions capture long-term consequences of actions\nWe can learn incrementally without complete episodes\n\nThe power of RL lies in learning optimal behavior without being explicitly told what to do - just by experiencing consequences and optimizing for long-term success.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Expanded Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html",
    "href": "lessons/expert/7-temporal-difference-learning.html",
    "title": "Temporal Difference Learning",
    "section": "",
    "text": "Temporal Difference (TD) learning is one of the most important ideas in reinforcement learning. It combines the best aspects of Monte Carlo methods and Dynamic Programming:\n\nLike Monte Carlo: Model-free, learns from experience\nLike Dynamic Programming: Bootstraps from current estimates, doesn’t wait for episodes to end\n\nKey insight: You can learn from incomplete episodes by using current estimates of future value.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#introduction-to-temporal-difference-learning",
    "href": "lessons/expert/7-temporal-difference-learning.html#introduction-to-temporal-difference-learning",
    "title": "Temporal Difference Learning",
    "section": "",
    "text": "Temporal Difference (TD) learning is one of the most important ideas in reinforcement learning. It combines the best aspects of Monte Carlo methods and Dynamic Programming:\n\nLike Monte Carlo: Model-free, learns from experience\nLike Dynamic Programming: Bootstraps from current estimates, doesn’t wait for episodes to end\n\nKey insight: You can learn from incomplete episodes by using current estimates of future value.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#the-td-learning-idea",
    "href": "lessons/expert/7-temporal-difference-learning.html#the-td-learning-idea",
    "title": "Temporal Difference Learning",
    "section": "The TD Learning Idea",
    "text": "The TD Learning Idea\nTraditional approach (MC): Wait until episode ends, then update: V(S_t) \\leftarrow V(S_t) + \\alpha[G_t - V(S_t)]\nTD approach: Update immediately after each step: V(S_t) \\leftarrow V(S_t) + \\alpha[R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)]\nTD Error: \\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)\nThis is the difference between the estimated value and the “better” estimate using the immediate reward plus the discounted next-state value.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#td0-algorithm",
    "href": "lessons/expert/7-temporal-difference-learning.html#td0-algorithm",
    "title": "Temporal Difference Learning",
    "section": "TD(0) Algorithm",
    "text": "TD(0) Algorithm\nThe simplest TD algorithm updates the value function after every step:\nInitialize V(s) arbitrarily for all s ∈ S\nFor each episode:\n    Initialize S\n    For each step of episode:\n        A = action given by policy for S\n        Take action A, observe R, S'\n        V(S) = V(S) + α[R + γV(S') - V(S)]\n        S = S'\n    Until S is terminal\nKey differences from MC: 1. Online learning: Updates happen during the episode 2. Bootstrapping: Uses current estimate V(S’) rather than actual return 3. Lower variance: Single-step updates reduce variance 4. Biased initially: Estimates are biased until V converges",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#why-td-learning-works",
    "href": "lessons/expert/7-temporal-difference-learning.html#why-td-learning-works",
    "title": "Temporal Difference Learning",
    "section": "Why TD Learning Works",
    "text": "Why TD Learning Works\n\nThe Driving Example\nScenario: You’re driving to work and estimating arrival time.\nStates: Home → Highway → Office Value function: Estimated time to reach office\nMonte Carlo approach: - Only update your estimate after you arrive at work - If you get stuck in traffic, you learn nothing until the end\nTD approach: - Update your estimate when you reach the highway - If highway is clear, immediately revise your time estimate - Learn from partial experience\n\n\nMathematical Intuition\nMC target: G_t (actual return) TD target: R_{t+1} + \\gamma V(S_{t+1}) (bootstrap estimate)\nAs V converges to V^\\pi, the TD target approaches the MC target: R_{t+1} + \\gamma V^\\pi(S_{t+1}) = \\mathbb{E}[G_t | S_t, A_t]",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#td-vs.-monte-carlo-vs.-dynamic-programming",
    "href": "lessons/expert/7-temporal-difference-learning.html#td-vs.-monte-carlo-vs.-dynamic-programming",
    "title": "Temporal Difference Learning",
    "section": "TD vs. Monte Carlo vs. Dynamic Programming",
    "text": "TD vs. Monte Carlo vs. Dynamic Programming\n\n\n\nAspect\nTD\nMonte Carlo\nDynamic Programming\n\n\n\n\nModel required\nNo\nNo\nYes\n\n\nBootstrapping\nYes\nNo\nYes\n\n\nOnline/Offline\nOnline\nOffline\nOffline\n\n\nEpisode completion\nNot required\nRequired\nNot applicable\n\n\nVariance\nLow\nHigh\nLow\n\n\nBias\nInitially biased\nUnbiased\nUnbiased\n\n\nConvergence\nFast\nSlow\nFast",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#sarsa-on-policy-td-control",
    "href": "lessons/expert/7-temporal-difference-learning.html#sarsa-on-policy-td-control",
    "title": "Temporal Difference Learning",
    "section": "SARSA: On-Policy TD Control",
    "text": "SARSA: On-Policy TD Control\nSARSA = State-Action-Reward-State-Action\nKey idea: Update action-value function Q(s,a) using TD learning.\nUpdate rule: Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\nAlgorithm:\nInitialize Q(s,a) arbitrarily for all s ∈ S, a ∈ A\nFor each episode:\n    Initialize S\n    Choose A from S using policy derived from Q (e.g., ε-greedy)\n    For each step of episode:\n        Take action A, observe R, S'\n        Choose A' from S' using policy derived from Q\n        Q(S,A) = Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n        S = S'; A = A'\n    Until S is terminal\nOn-policy: The policy being evaluated and improved is the same as the policy being used to generate behavior.\n\nSARSA Characteristics\n\nConservative: Learns the value of the policy it’s actually following\nSafe: Takes exploration into account when learning\nConverges: Guaranteed to converge to optimal policy under certain conditions",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#q-learning-off-policy-td-control",
    "href": "lessons/expert/7-temporal-difference-learning.html#q-learning-off-policy-td-control",
    "title": "Temporal Difference Learning",
    "section": "Q-Learning: Off-Policy TD Control",
    "text": "Q-Learning: Off-Policy TD Control\nQ-learning learns the optimal action-value function regardless of the policy being followed.\nUpdate rule: Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)]\nAlgorithm:\nInitialize Q(s,a) arbitrarily for all s ∈ S, a ∈ A\nFor each episode:\n    Initialize S\n    For each step of episode:\n        Choose A from S using policy derived from Q (e.g., ε-greedy)\n        Take action A, observe R, S'\n        Q(S,A) = Q(S,A) + α[R + γ max_a Q(S',a) - Q(S,A)]\n        S = S'\n    Until S is terminal\nOff-policy: The policy being learned (greedy w.r.t. Q) is different from the policy generating behavior (ε-greedy).\n\nQ-Learning Characteristics\n\nAggressive: Learns optimal policy even while exploring\nRobust: Can learn from suboptimal behavior\nConverges: Guaranteed to converge to Q* under certain conditions",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#sarsa-vs.-q-learning-the-cliff-walking-problem",
    "href": "lessons/expert/7-temporal-difference-learning.html#sarsa-vs.-q-learning-the-cliff-walking-problem",
    "title": "Temporal Difference Learning",
    "section": "SARSA vs. Q-Learning: The Cliff Walking Problem",
    "text": "SARSA vs. Q-Learning: The Cliff Walking Problem\nSetup: Agent must navigate from start to goal, avoiding a cliff.\nReward structure: - Normal steps: -1 - Cliff: -100 (return to start) - Goal: 0\nSARSA behavior: - Learns a “safe” path away from cliff - Takes exploration into account - Finds suboptimal but safe policy\nQ-learning behavior: - Learns optimal path close to cliff - Ignores exploration in learning - Finds optimal policy\n\n\n\nCliff Walking Comparison",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#expected-sarsa",
    "href": "lessons/expert/7-temporal-difference-learning.html#expected-sarsa",
    "title": "Temporal Difference Learning",
    "section": "Expected SARSA",
    "text": "Expected SARSA\nIdea: Instead of sampling the next action, use the expected value over all possible actions.\nUpdate rule: Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha[R_{t+1} + \\gamma \\mathbb{E}_\\pi[Q(S_{t+1}, A_{t+1})] - Q(S_t, A_t)]\nFor ε-greedy policy: \\mathbb{E}_\\pi[Q(S_{t+1}, A_{t+1})] = \\frac{\\epsilon}{|\\mathcal{A}|} \\sum_a Q(S_{t+1}, a) + (1-\\epsilon) \\max_a Q(S_{t+1}, a)\nAdvantages: - Reduces variance compared to SARSA - More stable learning - Can be used in both on-policy and off-policy settings",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#double-q-learning",
    "href": "lessons/expert/7-temporal-difference-learning.html#double-q-learning",
    "title": "Temporal Difference Learning",
    "section": "Double Q-Learning",
    "text": "Double Q-Learning\nProblem: Q-learning can overestimate action values due to the max operation.\nSolution: Use two Q-functions and update them alternately.\nAlgorithm:\nInitialize Q₁(s,a) and Q₂(s,a) arbitrarily for all s ∈ S, a ∈ A\nFor each episode:\n    Initialize S\n    For each step of episode:\n        Choose A using policy derived from Q₁ + Q₂\n        Take action A, observe R, S'\n        With probability 0.5:\n            Q₁(S,A) = Q₁(S,A) + α[R + γQ₂(S', argmax_a Q₁(S',a)) - Q₁(S,A)]\n        Else:\n            Q₂(S,A) = Q₂(S,A) + α[R + γQ₁(S', argmax_a Q₂(S',a)) - Q₂(S,A)]\n        S = S'\n    Until S is terminal",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#n-step-td-methods",
    "href": "lessons/expert/7-temporal-difference-learning.html#n-step-td-methods",
    "title": "Temporal Difference Learning",
    "section": "n-Step TD Methods",
    "text": "n-Step TD Methods\nIdea: Look ahead n steps instead of just one.\nn-step return: G_t^{(n)} = R_{t+1} + \\gamma R_{t+2} + ... + \\gamma^{n-1} R_{t+n} + \\gamma^n V(S_{t+n})\nn-step TD update: V(S_t) \\leftarrow V(S_t) + \\alpha[G_t^{(n)} - V(S_t)]\nSpecial cases: - n=1: TD(0) - n=∞: Monte Carlo\nTrade-off: Higher n reduces bias but increases variance.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#eligibility-traces-tdλ",
    "href": "lessons/expert/7-temporal-difference-learning.html#eligibility-traces-tdλ",
    "title": "Temporal Difference Learning",
    "section": "Eligibility Traces: TD(λ)",
    "text": "Eligibility Traces: TD(λ)\nProblem: n-step methods require storing n steps of experience.\nSolution: Use eligibility traces to give credit to recently visited states.\nEligibility trace: e_t(s) = \\gamma \\lambda e_{t-1}(s) + \\mathbf{1}(S_t = s)\nTD(λ) update: V(s) \\leftarrow V(s) + \\alpha \\delta_t e_t(s)\nfor all states s, where \\delta_t is the TD error.\nInterpretation: States are updated proportionally to how recently and frequently they were visited.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#convergence-properties",
    "href": "lessons/expert/7-temporal-difference-learning.html#convergence-properties",
    "title": "Temporal Difference Learning",
    "section": "Convergence Properties",
    "text": "Convergence Properties\n\nUnder Tabular Representation\nTD(0) and SARSA: - Converge to true value function under decreasing step sizes - Require that all states continue to be visited\nQ-learning: - Converges to optimal Q-function Q* - Requires that all state-action pairs continue to be visited\n\n\nStep Size Requirements\nFor convergence, step sizes must satisfy: \\sum_{t=1}^{\\infty} \\alpha_t = \\infty \\quad \\text{and} \\quad \\sum_{t=1}^{\\infty} \\alpha_t^2 &lt; \\infty\nExample: \\alpha_t = \\frac{1}{t} satisfies these conditions.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#function-approximation",
    "href": "lessons/expert/7-temporal-difference-learning.html#function-approximation",
    "title": "Temporal Difference Learning",
    "section": "Function Approximation",
    "text": "Function Approximation\nProblem: Tabular methods don’t scale to large state spaces.\nSolution: Approximate value functions using function approximation.\nLinear function approximation: V(s) \\approx \\mathbf{w}^T \\mathbf{x}(s)\nTD update with function approximation: \\mathbf{w}_{t+1} = \\mathbf{w}_t + \\alpha \\delta_t \\nabla_{\\mathbf{w}} V(S_t)\nChallenges: - Convergence no longer guaranteed - Stability issues with nonlinear function approximation - Leads to deep reinforcement learning",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#interactive-example-q-learning-on-gridworld",
    "href": "lessons/expert/7-temporal-difference-learning.html#interactive-example-q-learning-on-gridworld",
    "title": "Temporal Difference Learning",
    "section": "Interactive Example: Q-Learning on GridWorld",
    "text": "Interactive Example: Q-Learning on GridWorld\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport random\n\nclass GridWorld:\n    \"\"\"Simple GridWorld environment\"\"\"\n    def __init__(self, size=5):\n        self.size = size\n        self.start = (0, 0)\n        self.goal = (size-1, size-1)\n        self.obstacles = [(1, 1), (2, 1), (3, 1)]  # Some obstacles\n        self.actions = ['up', 'down', 'left', 'right']\n        self.action_effects = {\n            'up': (-1, 0), 'down': (1, 0), \n            'left': (0, -1), 'right': (0, 1)\n        }\n        self.reset()\n    \n    def reset(self):\n        self.state = self.start\n        return self.state_to_index(self.state)\n    \n    def state_to_index(self, state):\n        \"\"\"Convert (row, col) to single index\"\"\"\n        return state[0] * self.size + state[1]\n    \n    def index_to_state(self, index):\n        \"\"\"Convert single index to (row, col)\"\"\"\n        return (index // self.size, index % self.size)\n    \n    def step(self, action):\n        row, col = self.state\n        dr, dc = self.action_effects[self.actions[action]]\n        new_row, new_col = row + dr, col + dc\n        \n        # Check boundaries\n        new_row = max(0, min(self.size - 1, new_row))\n        new_col = max(0, min(self.size - 1, new_col))\n        \n        # Check obstacles\n        if (new_row, new_col) in self.obstacles:\n            new_row, new_col = row, col  # Stay in place\n        \n        self.state = (new_row, new_col)\n        \n        # Rewards\n        if self.state == self.goal:\n            reward = 10\n            done = True\n        elif self.state in self.obstacles:\n            reward = -10\n            done = False\n        else:\n            reward = -1  # Step penalty\n            done = False\n        \n        return self.state_to_index(self.state), reward, done, {}\n\nclass QLearningAgent:\n    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.95, epsilon=0.1):\n        self.Q = np.zeros((num_states, num_actions))\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.num_actions = num_actions\n    \n    def choose_action(self, state):\n        if random.random() &lt; self.epsilon:\n            return random.randint(0, self.num_actions - 1)\n        else:\n            return np.argmax(self.Q[state])\n    \n    def update(self, state, action, reward, next_state, done):\n        if done:\n            target = reward\n        else:\n            target = reward + self.gamma * np.max(self.Q[next_state])\n        \n        td_error = target - self.Q[state, action]\n        self.Q[state, action] += self.alpha * td_error\n        return abs(td_error)\n\nclass SARSAAgent:\n    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.95, epsilon=0.1):\n        self.Q = np.zeros((num_states, num_actions))\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.num_actions = num_actions\n    \n    def choose_action(self, state):\n        if random.random() &lt; self.epsilon:\n            return random.randint(0, self.num_actions - 1)\n        else:\n            return np.argmax(self.Q[state])\n    \n    def update(self, state, action, reward, next_state, next_action, done):\n        if done:\n            target = reward\n        else:\n            target = reward + self.gamma * self.Q[next_state, next_action]\n        \n        td_error = target - self.Q[state, action]\n        self.Q[state, action] += self.alpha * td_error\n        return abs(td_error)\n\ndef train_agent(agent, env, num_episodes=500, agent_type='Q-Learning'):\n    \"\"\"Train agent and track performance\"\"\"\n    episode_rewards = []\n    episode_lengths = []\n    td_errors = []\n    \n    for episode in range(num_episodes):\n        state = env.reset()\n        episode_reward = 0\n        episode_length = 0\n        episode_td_errors = []\n        \n        if agent_type == 'SARSA':\n            action = agent.choose_action(state)\n        \n        done = False\n        while not done and episode_length &lt; 200:  # Max episode length\n            if agent_type == 'Q-Learning':\n                action = agent.choose_action(state)\n                next_state, reward, done, _ = env.step(action)\n                td_error = agent.update(state, action, reward, next_state, done)\n                state = next_state\n            else:  # SARSA\n                next_state, reward, done, _ = env.step(action)\n                next_action = agent.choose_action(next_state)\n                td_error = agent.update(state, action, reward, next_state, next_action, done)\n                state, action = next_state, next_action\n            \n            episode_reward += reward\n            episode_length += 1\n            episode_td_errors.append(td_error)\n        \n        episode_rewards.append(episode_reward)\n        episode_lengths.append(episode_length)\n        td_errors.append(np.mean(episode_td_errors))\n    \n    return episode_rewards, episode_lengths, td_errors\n\ndef visualize_policy(agent, env):\n    \"\"\"Visualize the learned policy\"\"\"\n    policy_grid = np.zeros((env.size, env.size))\n    value_grid = np.zeros((env.size, env.size))\n    \n    for i in range(env.size):\n        for j in range(env.size):\n            state_idx = env.state_to_index((i, j))\n            if (i, j) == env.goal:\n                policy_grid[i, j] = -1  # Goal\n                value_grid[i, j] = np.max(agent.Q[state_idx])\n            elif (i, j) in env.obstacles:\n                policy_grid[i, j] = -2  # Obstacle\n                value_grid[i, j] = np.min(agent.Q[state_idx])\n            else:\n                policy_grid[i, j] = np.argmax(agent.Q[state_idx])\n                value_grid[i, j] = np.max(agent.Q[state_idx])\n    \n    return policy_grid, value_grid\n\n# Run experiments\nnp.random.seed(42)\nrandom.seed(42)\n\n# Train Q-Learning agent\nenv_q = GridWorld(size=5)\nq_agent = QLearningAgent(num_states=25, num_actions=4, alpha=0.1, gamma=0.95, epsilon=0.1)\nq_rewards, q_lengths, q_td_errors = train_agent(q_agent, env_q, num_episodes=500, agent_type='Q-Learning')\n\n# Train SARSA agent\nenv_s = GridWorld(size=5)\nsarsa_agent = SARSAAgent(num_states=25, num_actions=4, alpha=0.1, gamma=0.95, epsilon=0.1)\nsarsa_rewards, sarsa_lengths, sarsa_td_errors = train_agent(sarsa_agent, env_s, num_episodes=500, agent_type='SARSA')\n\n# Create visualizations\nfig = plt.figure(figsize=(18, 12))\n\n# Learning curves\nax1 = plt.subplot(2, 3, 1)\nwindow = 50\nq_smooth = np.convolve(q_rewards, np.ones(window)/window, mode='valid')\nsarsa_smooth = np.convolve(sarsa_rewards, np.ones(window)/window, mode='valid')\n\nplt.plot(range(window-1, len(q_rewards)), q_smooth, label='Q-Learning', linewidth=2)\nplt.plot(range(window-1, len(sarsa_rewards)), sarsa_smooth, label='SARSA', linewidth=2)\nplt.xlabel('Episodes')\nplt.ylabel('Average Reward')\nplt.title('Learning Progress (50-episode moving average)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Episode lengths\nax2 = plt.subplot(2, 3, 2)\nq_length_smooth = np.convolve(q_lengths, np.ones(window)/window, mode='valid')\nsarsa_length_smooth = np.convolve(sarsa_lengths, np.ones(window)/window, mode='valid')\n\nplt.plot(range(window-1, len(q_lengths)), q_length_smooth, label='Q-Learning', linewidth=2)\nplt.plot(range(window-1, len(sarsa_lengths)), sarsa_length_smooth, label='SARSA', linewidth=2)\nplt.xlabel('Episodes')\nplt.ylabel('Episode Length')\nplt.title('Episode Length vs Training')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\n# Q-Learning policy visualization\nax3 = plt.subplot(2, 3, 4)\nq_policy, q_values = visualize_policy(q_agent, env_q)\n\n# Create arrow visualization\narrows = ['↑', '↓', '←', '→']\nfor i in range(env_q.size):\n    for j in range(env_q.size):\n        if (i, j) == env_q.goal:\n            plt.text(j, i, 'G', ha='center', va='center', fontsize=12, fontweight='bold')\n        elif (i, j) in env_q.obstacles:\n            plt.text(j, i, 'X', ha='center', va='center', fontsize=12, fontweight='bold')\n        else:\n            action = int(q_policy[i, j])\n            plt.text(j, i, arrows[action], ha='center', va='center', fontsize=14)\n\nplt.xlim(-0.5, env_q.size-0.5)\nplt.ylim(-0.5, env_q.size-0.5)\nplt.gca().invert_yaxis()\nplt.title('Q-Learning Policy')\nplt.grid(True)\n\n# SARSA policy visualization\nax4 = plt.subplot(2, 3, 5)\nsarsa_policy, sarsa_values = visualize_policy(sarsa_agent, env_s)\n\nfor i in range(env_s.size):\n    for j in range(env_s.size):\n        if (i, j) == env_s.goal:\n            plt.text(j, i, 'G', ha='center', va='center', fontsize=12, fontweight='bold')\n        elif (i, j) in env_s.obstacles:\n            plt.text(j, i, 'X', ha='center', va='center', fontsize=12, fontweight='bold')\n        else:\n            action = int(sarsa_policy[i, j])\n            plt.text(j, i, arrows[action], ha='center', va='center', fontsize=14)\n\nplt.xlim(-0.5, env_s.size-0.5)\nplt.ylim(-0.5, env_s.size-0.5)\nplt.gca().invert_yaxis()\nplt.title('SARSA Policy')\nplt.grid(True)\n\n# Value function comparison\nax5 = plt.subplot(2, 3, 3)\nim = plt.imshow(q_values, cmap='viridis')\nplt.colorbar(im, shrink=0.8)\nplt.title('Q-Learning Value Function')\n\nax6 = plt.subplot(2, 3, 6)\nim = plt.imshow(sarsa_values, cmap='viridis')\nplt.colorbar(im, shrink=0.8)\nplt.title('SARSA Value Function')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nFinal Performance Comparison:\")\nprint(\"-\" * 40)\nprint(f\"Q-Learning - Last 50 episodes:\")\nprint(f\"  Average reward: {np.mean(q_rewards[-50:]):.2f}\")\nprint(f\"  Average length: {np.mean(q_lengths[-50:]):.1f}\")\n\nprint(f\"\\nSARSA - Last 50 episodes:\")\nprint(f\"  Average reward: {np.mean(sarsa_rewards[-50:]):.2f}\")\nprint(f\"  Average length: {np.mean(sarsa_lengths[-50:]):.1f}\")\n\nprint(f\"\\nKey Insights:\")\nprint(f\"• Q-Learning learns the optimal policy faster\")\nprint(f\"• SARSA is more conservative, considering exploration during learning\")\nprint(f\"• Both converge to good policies, but through different paths\")\n\n\n\n\nQ-Learning vs SARSA on GridWorld Environment\n\n\n\n\n\nFinal Performance Comparison:\n----------------------------------------\nQ-Learning - Last 50 episodes:\n  Average reward: 1.76\n  Average length: 9.2\n\nSARSA - Last 50 episodes:\n  Average reward: 2.34\n  Average length: 8.7\n\nKey Insights:\n• Q-Learning learns the optimal policy faster\n• SARSA is more conservative, considering exploration during learning\n• Both converge to good policies, but through different paths",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#applications",
    "href": "lessons/expert/7-temporal-difference-learning.html#applications",
    "title": "Temporal Difference Learning",
    "section": "Applications",
    "text": "Applications\n\nGame playing: Learning to play Atari games, board games\nRobotics: Learning motor control, navigation\nAutonomous driving: Traffic light control, path planning\nFinance: Algorithmic trading, portfolio management\nHealthcare: Treatment recommendation, drug discovery",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#advantages-of-td-learning",
    "href": "lessons/expert/7-temporal-difference-learning.html#advantages-of-td-learning",
    "title": "Temporal Difference Learning",
    "section": "Advantages of TD Learning",
    "text": "Advantages of TD Learning\n\nOnline learning: Can learn during episodes\nModel-free: No need for environment model\nEfficient: Lower variance than Monte Carlo\nFlexible: Works with continuing tasks\nBootstrapping: Can learn from partial experience",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#disadvantages-of-td-learning",
    "href": "lessons/expert/7-temporal-difference-learning.html#disadvantages-of-td-learning",
    "title": "Temporal Difference Learning",
    "section": "Disadvantages of TD Learning",
    "text": "Disadvantages of TD Learning\n\nInitially biased: Estimates are biased until convergence\nHyperparameter sensitive: Requires tuning of step size\nExploration required: Need adequate exploration for convergence\nFunction approximation challenges: Convergence issues with approximation",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  },
  {
    "objectID": "lessons/expert/7-temporal-difference-learning.html#key-takeaways",
    "href": "lessons/expert/7-temporal-difference-learning.html#key-takeaways",
    "title": "Temporal Difference Learning",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nTD learning is fundamental: Combines best of MC and DP\nBootstrapping is powerful: Can learn from incomplete episodes\nOn-policy vs. off-policy: SARSA vs. Q-learning serve different purposes\nExploration matters: Both algorithms need adequate exploration\nFoundation for modern RL: TD methods underpin most current algorithms\n\nTemporal difference learning represents a breakthrough in reinforcement learning, enabling agents to learn continuously from experience without requiring models or complete episodes. It forms the foundation for many modern RL algorithms!",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Temporal Difference Learning"
    ]
  }
]