[
  {
    "objectID": "lessons/beginner/basic-concepts-and-terms.html",
    "href": "lessons/beginner/basic-concepts-and-terms.html",
    "title": "Basic Concepts and Terms",
    "section": "",
    "text": "Basic Concepts and Terms\nHihi",
    "crumbs": [
      "Data Science Foundations",
      "Basic Concepts and Terms"
    ]
  },
  {
    "objectID": "lessons/deep-learning/intro-to-nns.html",
    "href": "lessons/deep-learning/intro-to-nns.html",
    "title": "Learn with Isaac",
    "section": "",
    "text": "This is my growing collection of lessons, notes, and explorations in data science, machine learning, and deep learning.\n\nüìò These notes are part personal study guide, part public resource ‚Äî feel free to browse, share, or build upon them.\n\n\n\nüìö Navigate the full list of lessons from the sidebar or jump into one of the sections below:\n\n\n\nExploratory Data Analysis\nHypothesis Testing"
  },
  {
    "objectID": "lessons/deep-learning/intro-to-nns.html#start-learning",
    "href": "lessons/deep-learning/intro-to-nns.html#start-learning",
    "title": "Learn with Isaac",
    "section": "",
    "text": "üìö Navigate the full list of lessons from the sidebar or jump into one of the sections below:\n\n\n\nExploratory Data Analysis\nHypothesis Testing"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Not That Deep - The Course",
    "section": "",
    "text": "In a world full of intelligence, humanity is the final exam."
  },
  {
    "objectID": "index.html#what-is-deep-learning",
    "href": "index.html#what-is-deep-learning",
    "title": "Welcome to Not That Deep - The Course",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?\nDeep learning is a subset of machine learning that uses neural networks with multiple layers (hence ‚Äúdeep‚Äù) to analyze various forms of data. Think of it as teaching computers to learn like humans do - through experience and examples rather than explicit programming.\nMuch like how a child learns to recognize cats not by memorizing rules about whiskers and tails, but by seeing thousands of examples, deep learning models learn patterns directly from data. The key difference? These models can process millions of examples at incredible speeds, finding patterns too subtle or complex for humans to detect manually."
  },
  {
    "objectID": "index.html#why-learn-deep-learning",
    "href": "index.html#why-learn-deep-learning",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Why Learn Deep Learning?",
    "text": "Why Learn Deep Learning?\nDeep learning is continuing to revolutionize computing and is behind many technological breakthroughs you encounter daily:\n\nThe voice assistant that understands your requests\nRecommendation systems that suggest your next favorite movie or product\nMedical imaging tools that detect diseases earlier than ever before\nSelf-driving vehicles that can interpret their surroundings\nLanguage translation that gets better every year\n\nBeyond these applications, deep learning is transforming entire industries - from healthcare and finance to transportation and entertainment. As AI continues to advance, understanding deep learning has become an essential skill for:\n\nSoftware engineers who want to build intelligent systems\nData scientists seeking to extract deeper insights from data\nResearchers pushing the boundaries of what‚Äôs possible\nEntrepreneurs identifying new opportunities in the AI revolution\nProfessionals in any field looking to future-proof their careers"
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Course Structure",
    "text": "Course Structure\nThis course takes you from the fundamentals of machine learning to the cutting edge of deep learning research. You‚Äôll build a solid foundation before tackling increasingly sophisticated concepts, with each lesson reinforcing your knowledge through practical examples and hands-on implementation. I developed this course to be accessible to anyone with a basic understanding of programming and mathematics, while also providing depth for those looking to specialize in deep learning. And most importantly, it‚Äôs designed this course for myself, to help me learn and understand deep learning better. I hope you find it as useful as I do!\n\nPrerequisites\nTo get the most from this course, you should have:\n\nCuriosity and persistence - Some concepts may challenge you at first, but with practice, they‚Äôll become clear.\nBasic Python programming skills - You should be comfortable writing functions, using loops, and working with libraries. These are some resources that can help you get started:\n\nPython for Everybody\nAutomate the Boring Stuff with Python\nLearn Python the Hard Way\nCodecademy Python Course\nGoogle‚Äôs Python Class\n\nFundamental mathematics and statistics - Understanding of algebra, calculus (derivatives), and basic statistics will help significantly. These are some resources that can help you get started:\n\n3Blue1Brown‚Äôs Essence of Calculus\nStatQuest with Josh Starmer\nVery Normal YouTube Channel\nKhan Academy Math Courses\n\n\nDon‚Äôt worry if your math is rusty or your programming skills are basic - we‚Äôll review key concepts as needed and build your skills gradually. The most important prerequisite is enthusiasm for learning.\n\n\nWho is This Course For?\nThis course a meant to be an ideal repository of knowledge and better explanation of key concepts with practical examples. This course is ideal for:\n\nSoftware developers transitioning to AI/ML roles\nData analysts looking to expand their technical toolkit\nStudents seeking practical skills beyond academic theory\n\n\n\nWhat You‚Äôll Learn\nYou‚Äôll develop a working knowledge of:\n\nCore machine learning algorithms and when to use them\nDeep neural network architectures and their applications\nData preparation and feature engineering techniques\nModel evaluation, tuning, and deployment strategies\nCurrent best practices in the field"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Getting Started",
    "text": "Getting Started\nReady to begin your deep learning journey? Head to the Lessons page to explore the road to see what lies ahead.\nLet‚Äôs transform the way you think about computing and artificial intelligence!"
  },
  {
    "objectID": "index.html#feedback",
    "href": "index.html#feedback",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Feedback?",
    "text": "Feedback?\nGot ideas, suggestions, or corrections? Reach out on GitHub or open an issue!"
  },
  {
    "objectID": "lessons/index.html",
    "href": "lessons/index.html",
    "title": "Deep Learning - The Gradient‚Äôs Journey",
    "section": "",
    "text": "‚ÄúThe world speaks many languages,‚Äù the old mathematician said to the young programmer. ‚ÄúThere is the language of numbers, the language of patterns, and now, the language of machines that learn. To understand this language is to discover your Personal Legend in the digital age.‚Äù\nThe young programmer nodded, uncertain but eager to begin the journey. ‚ÄúRemember,‚Äù continued the mathematician, ‚Äúthe journey to wisdom is not about complexity, but about seeing the simple truths hidden within the complex, one at a time.‚Äù"
  },
  {
    "objectID": "lessons/index.html#foundations-beginner-level",
    "href": "lessons/index.html#foundations-beginner-level",
    "title": "Deep Learning - The Gradient‚Äôs Journey",
    "section": "Foundations (Beginner Level)",
    "text": "Foundations (Beginner Level)\n\n1: Introduction to Machine Learning\n\nBasic concepts and terminology\nSupervised vs.¬†unsupervised learning\nThe machine learning workflow\n\n\n\n2: Linear Models\n\nLinear regression\nLogistic regression\nGradient descent optimization\n\n\n\n3: Model Evaluation\n\nTraining, validation, and test sets\nOverfitting and underfitting\nCross-validation techniques\n\n\n\n4: Decision Trees and Ensemble Methods\n\nDecision trees\nRandom forests\nGradient boosting"
  },
  {
    "objectID": "lessons/index.html#neural-networks-fundamentals-intermediate-level",
    "href": "lessons/index.html#neural-networks-fundamentals-intermediate-level",
    "title": "Deep Learning - The Gradient‚Äôs Journey",
    "section": "Neural Networks Fundamentals (Intermediate Level)",
    "text": "Neural Networks Fundamentals (Intermediate Level)\n\n5: Neural Network Basics\n\nPerceptrons and activation functions\nFeedforward neural networks\nBackpropagation algorithm\n\n\n\n6: Building Your First Neural Network\n\nTensorFlow and Keras introduction\nImplementing a simple neural network\nTraining and evaluation\n\n\n\n7: Convolutional Neural Networks\n\nImage data and convolution operations\nCNN architectures (LeNet, AlexNet)\nImage classification tasks\n\n\n\n8: Recurrent Neural Networks\n\nSequential data processing\nLSTM and GRU architectures\nTime series prediction"
  },
  {
    "objectID": "lessons/index.html#advanced-deep-learning-advanced-level",
    "href": "lessons/index.html#advanced-deep-learning-advanced-level",
    "title": "Deep Learning - The Gradient‚Äôs Journey",
    "section": "Advanced Deep Learning (Advanced Level)",
    "text": "Advanced Deep Learning (Advanced Level)\n\n9: Advanced CNN Architectures\n\nResNet, Inception, and EfficientNet\nTransfer learning techniques\nComputer vision applications\n\n\n\n10: Natural Language Processing\n\nWord embeddings (Word2Vec, GloVe)\nRNN-based language models\nText classification and generation\n\n\n\n11: Transformers and Attention\n\nAttention mechanisms\nTransformer architecture\nBERT, GPT, and their applications\n\n\n\n12: Generative Models\n\nAutoencoders and variational autoencoders\nGenerative Adversarial Networks (GANs)\nDiffusion models"
  },
  {
    "objectID": "lessons/index.html#cutting-edge-techniques-expert-level",
    "href": "lessons/index.html#cutting-edge-techniques-expert-level",
    "title": "Deep Learning - The Gradient‚Äôs Journey",
    "section": "Cutting-Edge Techniques (Expert Level)",
    "text": "Cutting-Edge Techniques (Expert Level)\n\n13: Reinforcement Learning\n\nRL fundamentals\nDeep Q-Networks\nPolicy gradient methods\n\n\n\n14: Self-Supervised Learning\n\nContrastive learning\nCLIP and SimCLR\nFoundation models\n\n\n\n15: Multimodal Learning\n\nVision-language models\nAudio-visual integration\nCross-modal transformers\n\n\n\n16: Deploying Deep Learning Models\n\nModel optimization and quantization\nEdge deployment\nMonitoring and maintaining production models"
  },
  {
    "objectID": "lessons/expert/rl-fundamentals.html",
    "href": "lessons/expert/rl-fundamentals.html",
    "title": "Reinforcement Learning Fundamentals",
    "section": "",
    "text": "Reinforcement Learning (RL) is like teaching a dog new tricks - but instead of treats, we use numerical rewards, and instead of a dog, we have an algorithm.\n\nSequential decision making tasks\n‚ÄúAgent‚Äù that interacts with an external ‚Äúenvironment‚Äù\nAgent maintains a ‚Äústate‚Äù s_t\nState s_t is passed to a policy \\pi\nBased on \\pi, we choose an action a_t = \\pi(s_t)\n\nEnvironment gives us back an observation o_t\nFinally, the agent updates its state s_{t+1} using a ‚Äústate-update‚Äù function s_{t+1} = u(s_t, a_t, o_{t+1})",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Fundamentals"
    ]
  },
  {
    "objectID": "lessons/expert/rl-fundamentals.html#what-is-reinforcement-learning",
    "href": "lessons/expert/rl-fundamentals.html#what-is-reinforcement-learning",
    "title": "Reinforcement Learning Fundamentals",
    "section": "",
    "text": "Reinforcement Learning (RL) is like teaching a dog new tricks - but instead of treats, we use numerical rewards, and instead of a dog, we have an algorithm.\n\nSequential decision making tasks\n‚ÄúAgent‚Äù that interacts with an external ‚Äúenvironment‚Äù\nAgent maintains a ‚Äústate‚Äù s_t\nState s_t is passed to a policy \\pi\nBased on \\pi, we choose an action a_t = \\pi(s_t)\n\nEnvironment gives us back an observation o_t\nFinally, the agent updates its state s_{t+1} using a ‚Äústate-update‚Äù function s_{t+1} = u(s_t, a_t, o_{t+1})",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Fundamentals"
    ]
  },
  {
    "objectID": "lessons/expert/rl-fundamentals.html#how-do-we-determine-what-a-good-policy-is",
    "href": "lessons/expert/rl-fundamentals.html#how-do-we-determine-what-a-good-policy-is",
    "title": "Reinforcement Learning Fundamentals",
    "section": "How do we determine what a good policy is?",
    "text": "How do we determine what a good policy is?\nWe introduce a ‚Äúreward function‚Äù to guide the choice of a policy.\nR(s_t, a_t) is the reward function -&gt; ‚Äúvalue‚Äù for performing an action in a given state.\nU_{\\pi}(s_0) is value function for policy \\pi evaluated at s_0.\n\nU_{\\pi}(s_0) = {E}_p(a_0, s_1, ..., a_T, s_T|s_0, \\pi) \\left[ \\sum_{t=0}^T  R(s_t, a_t) | s_0 \\right]\n\nInitial state that the agent is in \\pi could correspond to a random choice.\n \\text{Sample 1: } a^{(1)}_0, s^{(1)}_1, ... , a^{(1)}_T, s^{(1)}_T \\implies \\sum R(s_t^{(1)},a_t^{(1)})\n\\text{Sample 2: } a^{(2)}_0, s^{(2)}_1, ... , a^{(2)}_T, s^{(2)}_T \\implies \\sum R(s_t^{(2)},a_t^{(2)})\n:\n\\text{Sample N: }a^{(N)}_0, s^{(N)}_1, ... , a^{(N)}_T, s^{(N)}_T \\implies \\sum R(s_t^{(N)},a_t^{(N)})\nU_{\\pi}(s_0) = \\frac{1}{N} \\sum_{i=1}^N{R^{(i)}}\nAverage reward over different (random) choices of trajectories of interacting with an environment.\nOptimal policy: \\arg\\max_{\\pi}{E_p(s_0)[U_{\\pi}(s_0)]}",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Fundamentals"
    ]
  },
  {
    "objectID": "lessons/expert/rl-fundamentals.html#multi-armed-bandit-problem",
    "href": "lessons/expert/rl-fundamentals.html#multi-armed-bandit-problem",
    "title": "Reinforcement Learning Fundamentals",
    "section": "Multi-Armed Bandit Problem",
    "text": "Multi-Armed Bandit Problem\nImagine you‚Äôre in a Vegas casino with a row of slot machines (the ‚Äúbandits‚Äù - they‚Äôre called one-armed bandits because of the lever).\nEach machine has different odds of paying out, but you don‚Äôt know which ones are better. You have limited money and time. Your challenge: figure out which machines are the best while still making money.\nThis is the multi-armed bandit problem - how do you balance exploring new options to find the best one, versus exploiting what you already know works?\n\nExamine Scenario\nLet‚Äôs say you have 3 coffee shops near your office:\n\nJoe‚Äôs Java: You‚Äôve been there 5 times, it‚Äôs decent (7/10 rating)\nBean Scene: You tried it once, it was amazing (9/10)\nCaf√© Mystery: You‚Äôve never tried it\n\nEach morning, you face the bandit problem:\n\nKeep going to Bean Scene (exploit your best known option)?\nTry Caf√© Mystery (explore to potentially find something better)?\nGive Joe‚Äôs more chances (gather more data)?\n\nIf you only exploit, you might miss an even better coffee shop. If you only explore, you waste money on potentially bad coffee.\n\n\nReal-World Applications\n\nOnline Advertising: Which ad version gets the most clicks? Companies test different versions while still showing profitable ones.\nNetflix Recommendations: Which movies to suggest? Netflix balances showing you movies similar to what you liked (exploit) vs.¬†new genres you might enjoy (explore).\nClinical Trials: Which treatment works best? Doctors must balance giving patients proven treatments vs.¬†testing new ones.\nRestaurant Apps: DoorDash decides which restaurants to show first - popular ones you‚Äôll likely order from, or new ones you might love.\n\nThe key insight is that information has value - sometimes it‚Äôs worth taking a suboptimal choice now to learn something that helps you make better choices later.\nNow, let me challenge an implicit assumption in how this problem is often framed: Is pure optimization always the goal?\nIn real life, variety itself has value. Humans get bored. The ‚Äúbest‚Äù coffee shop might not be best every day. Perhaps the real problem isn‚Äôt just finding the optimal choice, but maintaining a satisfying portfolio of choices over time.\nLet‚Äôs delve into the theory behind the multi-armed bandit problem.\n\nChoice amongst R options at each time step. Step\nreturns a numerical reward sampled from a reward distribution.\n\nGoal: Maximize expected total reward over time.\nLet A_t be action selected at time t.\n\nR_t is the reward received at time t.\n\nValue of an Action q_*(a) = E(R_t|A_t = a) (a = choice of a given machine)\nBut since we do not know q_*(a), the true action-value function, we must estimate it.\nIf we know q_*(a), for every possible choice of a , we always pick the a which maximizes it. Denote the estimate by Q(a).\nIdeally, Q(a) should converge to q_*(a).\n\n\nHow do we choose Q_t(a)?\nOne possibility is to use the average of all rewards received for action a up to time t.:\n\nQ_t(a) = \\frac{\\text{total rewards for }a}{\\text{Number of times }a \\text{ was performed}}\n\n\nQ_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i I(A_i=a)}}{\\sum_{i=1}^{t-1}{I(A_i=a)}}\n\n‚ÄúAction-Value‚Äù methods\nQ_t(a) \\implies q_*(a) by Law of Large Numbers (LLN).\n\n\nHow do we use this for decision-making?\n\n‚ÄúGreedy‚Äù Action Selection\nA_t = \\arg\\max_a{Q_t(a)} ties ave broken at random Q_t(a) is initialized to be 0, for all a. This means that at time t, we choose the action (A) that gives us the maximum expected reward according to our current estimate of the Q-value for each possible action.\nimport jax\n\n# Greedy action selection\n‚Äú\\epsilon-Greedy‚Äù Method\nUse greedy action selection most of the time. ‚ÄúOccasionally‚Äù, with probability \\epsilon &gt; 0, select an action amongst all possible actions uniformly at random. Eventually guaranteed to estimate all q_*(a) (but might not be practical).\nimport jax\n\n# Epsilon-Greedy action selection\n\n# Do some visualizations for different values of epsilon\n# for say 1000 simulations, and 1000 time steps\n\nInitial values affect the outcome of a run.\nIdea: We can choose larger initial values to encourage exploration.\nE.g. Q_t(a) = 5 (large relative to the means of the reward distributions) will encourage more exploration.\nIn conclusion, initial values matter for determining how much exploration happens early in the learning process.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "Reinforcement Learning Fundamentals"
    ]
  }
]