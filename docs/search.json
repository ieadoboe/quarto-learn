[
  {
    "objectID": "lessons/expert/rl-fundamentals.html",
    "href": "lessons/expert/rl-fundamentals.html",
    "title": "1. Reinforcement Learning Introduction",
    "section": "",
    "text": "Reinforcement learning (RL) is a class of methods for solving various kinds of sequential decision making problems.\nRL is like teaching a dog new tricks - but instead of treats, we use numerical rewards, and instead of a dog, we have an algorithm.\nGoal of RL: To design an agent that interacts with an external environment\n\nThe agent maintains an internal state s_t\nThe state s_t is passed to a policy \\pi\nBased on \\pi, we choose an action a_t = \\pi(s_t)\nEnvironment gives us back an observation o_{t+1}\nFinally, the agent updates its state s_{t+1} using a state-update function s_{t+1} = U(s_t, a_t, o_{t+1})\n\n\n\n\nAgent - decision maker or learner. E.g. a robot trying to navigate a maze.\nEnvironment - Everything outside the agent. E.g. The maze.\nState, s_t - The agent’s internal representation of a situation at a particular time, t. It’s a “mental map” of where it is and what the agent knows. E.g. The state might include the coordinates in the maze, what direction it is face and what it has learnt so far.\nPolicy, \\pi - The strategy (or rule) that guides decision making (that is maps states to actions). It is the decision making function. E.g. if the robot is a wall in front, turn right or left.\nAction, a_t = \\pi(s_t) - Choice made by the agent. E.g. Robot turning right or moving forward.\nObservation - The information received from environment by an agent. E.g. Robot observes “I bumped into a wall” or “I found a clear path.”\nState-update function - The function that the agent uses to update what it knows. It is how the agent learns and adapts. E.g. Our robot updates its mental map of the maze after each move, incorporating new information like “there’s a wall here” or “there’s a reward there.”",
    "crumbs": [
      "Cutting-Edge Techniques",
      "1. Reinforcement Learning Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/rl-fundamentals.html#what-is-reinforcement-learning",
    "href": "lessons/expert/rl-fundamentals.html#what-is-reinforcement-learning",
    "title": "1. Reinforcement Learning Introduction",
    "section": "",
    "text": "Reinforcement learning (RL) is a class of methods for solving various kinds of sequential decision making problems.\nRL is like teaching a dog new tricks - but instead of treats, we use numerical rewards, and instead of a dog, we have an algorithm.\nGoal of RL: To design an agent that interacts with an external environment\n\nThe agent maintains an internal state s_t\nThe state s_t is passed to a policy \\pi\nBased on \\pi, we choose an action a_t = \\pi(s_t)\nEnvironment gives us back an observation o_{t+1}\nFinally, the agent updates its state s_{t+1} using a state-update function s_{t+1} = U(s_t, a_t, o_{t+1})\n\n\n\n\nAgent - decision maker or learner. E.g. a robot trying to navigate a maze.\nEnvironment - Everything outside the agent. E.g. The maze.\nState, s_t - The agent’s internal representation of a situation at a particular time, t. It’s a “mental map” of where it is and what the agent knows. E.g. The state might include the coordinates in the maze, what direction it is face and what it has learnt so far.\nPolicy, \\pi - The strategy (or rule) that guides decision making (that is maps states to actions). It is the decision making function. E.g. if the robot is a wall in front, turn right or left.\nAction, a_t = \\pi(s_t) - Choice made by the agent. E.g. Robot turning right or moving forward.\nObservation - The information received from environment by an agent. E.g. Robot observes “I bumped into a wall” or “I found a clear path.”\nState-update function - The function that the agent uses to update what it knows. It is how the agent learns and adapts. E.g. Our robot updates its mental map of the maze after each move, incorporating new information like “there’s a wall here” or “there’s a reward there.”",
    "crumbs": [
      "Cutting-Edge Techniques",
      "1. Reinforcement Learning Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/rl-fundamentals.html#how-do-we-determine-what-a-good-policy-is",
    "href": "lessons/expert/rl-fundamentals.html#how-do-we-determine-what-a-good-policy-is",
    "title": "1. Reinforcement Learning Introduction",
    "section": "How do we determine what a good policy is?",
    "text": "How do we determine what a good policy is?\nGoal of the agent: To choose a policy that maximizes the sum of expected rewards. We introduce a reward function to guide the choice of a policy.\n\nReward function, R(s_t, a_t), is the value for performing an action in a given state.\nValue function, V_{\\pi}(s_0) is for policy \\pi evaluated at the agent’s initial state, s_0.\n\n\nV_{\\pi}(s_0) = {E}_p(a_0, s_1, ..., a_T, s_T|s_0, \\pi) \\left[ \\sum_{t=0}^T  R(s_t, a_t) | s_0 \\right]\n\n\nInitial state that the agent is in \\pi could correspond to a random choice.\n\n \\text{Sample 1: } a^{(1)}_0, s^{(1)}_1, ... , a^{(1)}_T, s^{(1)}_T \\implies \\sum R(s_t^{(1)},a_t^{(1)})\n\\text{Sample 2: } a^{(2)}_0, s^{(2)}_1, ... , a^{(2)}_T, s^{(2)}_T \\implies \\sum R(s_t^{(2)},a_t^{(2)})\n:\n\\text{Sample N: }a^{(N)}_0, s^{(N)}_1, ... , a^{(N)}_T, s^{(N)}_T \\implies \\sum R(s_t^{(N)},a_t^{(N)})\n\nAverage reward over different (random) choices of trajectories of interacting with an environment.\n\nV_{\\pi}(s_0) = \\frac{1}{N} \\sum_{i=1}^N{R^{(i)}}\nWe can define the Optimal policy:\n\n\\pi^* = \\arg\\max_{\\pi}{E_{p(s_0)}[V_{\\pi}(s_0)]}\n\nNote that, this is one way of designing an optimal policy - Maximum expected utility principle. Policy will vary depending on the assumptions we make about the environment and the form of an agent.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "1. Reinforcement Learning Introduction"
    ]
  },
  {
    "objectID": "lessons/expert/rl-fundamentals.html#multi-armed-bandit-problem",
    "href": "lessons/expert/rl-fundamentals.html#multi-armed-bandit-problem",
    "title": "1. Reinforcement Learning Introduction",
    "section": "Multi-Armed Bandit Problem",
    "text": "Multi-Armed Bandit Problem\nWhen there are a finite number of possible actions, this is called a multi-armed bandit.\nImagine you’re in a Vegas casino with a row of slot machines (the “bandits” - they’re called one-armed bandits because of the lever). Each machine has different odds of paying out, but you don’t know which ones are better. You have limited money and time.\nYour challenge: figure out which machines are the best while still making money. This is the multi-armed bandit problem - how do you balance exploring new options to find the best one, versus exploiting what you already know works?\n\nExamine Scenario\nLet’s say you have 3 coffee shops near your office:\n\nJoe’s Java: You’ve been there 5 times, it’s decent (7/10 rating)\nBean Scene: You tried it once, it was amazing (9/10)\nCafé Mystery: You’ve never tried it\n\nEach morning, you face the bandit problem:\n\nKeep going to Bean Scene (exploit your best known option)?\nTry Café Mystery (explore to potentially find something better)?\nGive Joe’s more chances (gather more data)?\n\nIf you only exploit, you might miss an even better coffee shop. If you only explore, you waste money on potentially bad coffee.\n\n\nReal-World Applications\n\nOnline Advertising: Which ad version gets the most clicks? Companies test different versions while still showing profitable ones.\nNetflix Recommendations: Which movies to suggest? Netflix balances showing you movies similar to what you liked (exploit) vs. new genres you might enjoy (explore).\nClinical Trials: Which treatment works best? Doctors must balance giving patients proven treatments vs. testing new ones.\nRestaurant Apps: DoorDash decides which restaurants to show first - popular ones you’ll likely order from, or new ones you might love.\n\nThe key insight is that information has value - sometimes it’s worth taking a suboptimal choice now to learn something that helps you make better choices later.\nNow, let me challenge an implicit assumption in how this problem is often framed: Is pure optimization always the goal?\nIn real life, variety itself has value. Humans get bored. The “best” coffee shop might not be best every day. Perhaps the real problem isn’t just finding the optimal choice, but maintaining a satisfying portfolio of choices over time.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "1. Reinforcement Learning Introduction"
    ]
  },
  {
    "objectID": "lessons/index.html",
    "href": "lessons/index.html",
    "title": "Deep Learning - The Gradient’s Journey",
    "section": "",
    "text": "“The world speaks many languages,” the old mathematician said to the young programmer. “There is the language of numbers, the language of patterns, and now, the language of machines that learn. To understand this language is to discover your Personal Legend in the digital age.”\nThe young programmer nodded, uncertain but eager to begin the journey. “Remember,” continued the mathematician, “the journey to wisdom is not about complexity, but about seeing the simple truths hidden within the complex, one at a time.”"
  },
  {
    "objectID": "lessons/index.html#foundations-beginner-level",
    "href": "lessons/index.html#foundations-beginner-level",
    "title": "Deep Learning - The Gradient’s Journey",
    "section": "Foundations (Beginner Level)",
    "text": "Foundations (Beginner Level)\n\n1: Introduction to Machine Learning\n\nBasic concepts and terminology\nSupervised vs. unsupervised learning\nThe machine learning workflow\n\n\n\n2: Linear Models\n\nLinear regression\nLogistic regression\nGradient descent optimization\n\n\n\n3: Model Evaluation\n\nTraining, validation, and test sets\nOverfitting and underfitting\nCross-validation techniques\n\n\n\n4: Decision Trees and Ensemble Methods\n\nDecision trees\nRandom forests\nGradient boosting"
  },
  {
    "objectID": "lessons/index.html#neural-networks-fundamentals-intermediate-level",
    "href": "lessons/index.html#neural-networks-fundamentals-intermediate-level",
    "title": "Deep Learning - The Gradient’s Journey",
    "section": "Neural Networks Fundamentals (Intermediate Level)",
    "text": "Neural Networks Fundamentals (Intermediate Level)\n\n5: Neural Network Basics\n\nPerceptrons and activation functions\nFeedforward neural networks\nBackpropagation algorithm\n\n\n\n6: Building Your First Neural Network\n\nTensorFlow and Keras introduction\nImplementing a simple neural network\nTraining and evaluation\n\n\n\n7: Convolutional Neural Networks\n\nImage data and convolution operations\nCNN architectures (LeNet, AlexNet)\nImage classification tasks\n\n\n\n8: Recurrent Neural Networks\n\nSequential data processing\nLSTM and GRU architectures\nTime series prediction"
  },
  {
    "objectID": "lessons/index.html#advanced-deep-learning-advanced-level",
    "href": "lessons/index.html#advanced-deep-learning-advanced-level",
    "title": "Deep Learning - The Gradient’s Journey",
    "section": "Advanced Deep Learning (Advanced Level)",
    "text": "Advanced Deep Learning (Advanced Level)\n\n9: Advanced CNN Architectures\n\nResNet, Inception, and EfficientNet\nTransfer learning techniques\nComputer vision applications\n\n\n\n10: Natural Language Processing\n\nWord embeddings (Word2Vec, GloVe)\nRNN-based language models\nText classification and generation\n\n\n\n11: Transformers and Attention\n\nAttention mechanisms\nTransformer architecture\nBERT, GPT, and their applications\n\n\n\n12: Generative Models\n\nAutoencoders and variational autoencoders\nGenerative Adversarial Networks (GANs)\nDiffusion models"
  },
  {
    "objectID": "lessons/index.html#cutting-edge-techniques-expert-level",
    "href": "lessons/index.html#cutting-edge-techniques-expert-level",
    "title": "Deep Learning - The Gradient’s Journey",
    "section": "Cutting-Edge Techniques (Expert Level)",
    "text": "Cutting-Edge Techniques (Expert Level)\n\n13: Reinforcement Learning\n\nRL fundamentals\nDeep Q-Networks\nPolicy gradient methods\n\n\n\n14: Self-Supervised Learning\n\nContrastive learning\nCLIP and SimCLR\nFoundation models\n\n\n\n15: Multimodal Learning\n\nVision-language models\nAudio-visual integration\nCross-modal transformers\n\n\n\n16: Deploying Deep Learning Models\n\nModel optimization and quantization\nEdge deployment\nMonitoring and maintaining production models"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Not That Deep - The Course",
    "section": "",
    "text": "In a world full of intelligence, humanity is the final exam."
  },
  {
    "objectID": "index.html#what-is-deep-learning",
    "href": "index.html#what-is-deep-learning",
    "title": "Welcome to Not That Deep - The Course",
    "section": "What is Deep Learning?",
    "text": "What is Deep Learning?\nDeep learning is a subset of machine learning that uses neural networks with multiple layers (hence “deep”) to analyze various forms of data. Think of it as teaching computers to learn like humans do - through experience and examples rather than explicit programming.\nMuch like how a child learns to recognize cats not by memorizing rules about whiskers and tails, but by seeing thousands of examples, deep learning models learn patterns directly from data. The key difference? These models can process millions of examples at incredible speeds, finding patterns too subtle or complex for humans to detect manually."
  },
  {
    "objectID": "index.html#why-learn-deep-learning",
    "href": "index.html#why-learn-deep-learning",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Why Learn Deep Learning?",
    "text": "Why Learn Deep Learning?\nDeep learning is continuing to revolutionize computing and is behind many technological breakthroughs you encounter daily:\n\nThe voice assistant that understands your requests\nRecommendation systems that suggest your next favorite movie or product\nMedical imaging tools that detect diseases earlier than ever before\nSelf-driving vehicles that can interpret their surroundings\nLanguage translation that gets better every year\n\nBeyond these applications, deep learning is transforming entire industries - from healthcare and finance to transportation and entertainment. As AI continues to advance, understanding deep learning has become an essential skill for:\n\nSoftware engineers who want to build intelligent systems\nData scientists seeking to extract deeper insights from data\nResearchers pushing the boundaries of what’s possible\nEntrepreneurs identifying new opportunities in the AI revolution\nProfessionals in any field looking to future-proof their careers"
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Course Structure",
    "text": "Course Structure\nThis course takes you from the fundamentals of machine learning to the cutting edge of deep learning research. You’ll build a solid foundation before tackling increasingly sophisticated concepts, with each lesson reinforcing your knowledge through practical examples and hands-on implementation. I developed this course to be accessible to anyone with a basic understanding of programming and mathematics, while also providing depth for those looking to specialize in deep learning. And most importantly, it’s designed this course for myself, to help me learn and understand deep learning better. I hope you find it as useful as I do!\n\nPrerequisites\nTo get the most from this course, you should have:\n\nCuriosity and persistence - Some concepts may challenge you at first, but with practice, they’ll become clear.\nBasic Python programming skills - You should be comfortable writing functions, using loops, and working with libraries. These are some resources that can help you get started:\n\nPython for Everybody\nAutomate the Boring Stuff with Python\nLearn Python the Hard Way\nCodecademy Python Course\nGoogle’s Python Class\n\nFundamental mathematics and statistics - Understanding of algebra, calculus (derivatives), and basic statistics will help significantly. These are some resources that can help you get started:\n\n3Blue1Brown’s Essence of Calculus\nStatQuest with Josh Starmer\nVery Normal YouTube Channel\nKhan Academy Math Courses\n\n\nDon’t worry if your math is rusty or your programming skills are basic - we’ll review key concepts as needed and build your skills gradually. The most important prerequisite is enthusiasm for learning.\n\n\nWho is This Course For?\nThis course a meant to be an ideal repository of knowledge and better explanation of key concepts with practical examples. This course is ideal for:\n\nSoftware developers transitioning to AI/ML roles\nData analysts looking to expand their technical toolkit\nStudents seeking practical skills beyond academic theory\n\n\n\nWhat You’ll Learn\nYou’ll develop a working knowledge of:\n\nCore machine learning algorithms and when to use them\nDeep neural network architectures and their applications\nData preparation and feature engineering techniques\nModel evaluation, tuning, and deployment strategies\nCurrent best practices in the field"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Getting Started",
    "text": "Getting Started\nReady to begin your deep learning journey? Head to the Lessons page to explore the road to see what lies ahead.\nLet’s transform the way you think about computing and artificial intelligence!"
  },
  {
    "objectID": "index.html#feedback",
    "href": "index.html#feedback",
    "title": "Welcome to Not That Deep - The Course",
    "section": "Feedback?",
    "text": "Feedback?\nGot ideas, suggestions, or corrections? Reach out on GitHub or open an issue!"
  },
  {
    "objectID": "lessons/deep-learning/intro-to-nns.html",
    "href": "lessons/deep-learning/intro-to-nns.html",
    "title": "Learn with Isaac",
    "section": "",
    "text": "This is my growing collection of lessons, notes, and explorations in data science, machine learning, and deep learning.\n\n📘 These notes are part personal study guide, part public resource — feel free to browse, share, or build upon them.\n\n\n\n📚 Navigate the full list of lessons from the sidebar or jump into one of the sections below:\n\n\n\nExploratory Data Analysis\nHypothesis Testing"
  },
  {
    "objectID": "lessons/deep-learning/intro-to-nns.html#start-learning",
    "href": "lessons/deep-learning/intro-to-nns.html#start-learning",
    "title": "Learn with Isaac",
    "section": "",
    "text": "📚 Navigate the full list of lessons from the sidebar or jump into one of the sections below:\n\n\n\nExploratory Data Analysis\nHypothesis Testing"
  },
  {
    "objectID": "lessons/beginner/basic-concepts-and-terms.html",
    "href": "lessons/beginner/basic-concepts-and-terms.html",
    "title": "Basic Concepts and Terms",
    "section": "",
    "text": "Basic Concepts and Terms\nHihi",
    "crumbs": [
      "Data Science Foundations",
      "Basic Concepts and Terms"
    ]
  },
  {
    "objectID": "lessons/expert/gradient-bandit-algos.html",
    "href": "lessons/expert/gradient-bandit-algos.html",
    "title": "2. Bandit Algorithms",
    "section": "",
    "text": "Let’s delve into the theory behind the multi-armed bandit problem. Choice amongst R options at each time step. Step returns a numerical reward sampled from a reward distribution.\nGoal: Maximize expected total reward over time.\n\nR_t is the reward received at time t.\nA_t be action selected at time t.\na is choice of a given machine/agent.\n\n\n\nq_*(a) = E(R_t|A_t = a)\nBut since we do not know q_*(a), the true action-value function, we must estimate it. If we knew the true value of an agent’s action, q_*(a), for every possible choice of a, we would always pick the a which maximizes it.\n\n\n\nWe denote the estimate by Q(a). Ideally, Q(a) should converge to q_*(a).\nHow do we choose Q_t(a)?\nOne possibility is to use the average (expected value) of all rewards received for action a up to time t:\n\nQ_t(a) = \\frac{\\text{total rewards for }a}{\\text{Number of times }a \\text{ was performed}}\n\n\nQ_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i I(A_i=a)}}{\\sum_{i=1}^{t-1}{I(A_i=a)}}\n\nQ_t(a) \\implies q_*(a) by Law of Large Numbers (LLN).\n\n\n\n\n“Greedy” Action Selection\nA_t = \\arg\\max_a{Q_t(a)} ties ave broken at random Q_t(a) is initialized to be 0, for all a. This means that at time t, we choose the action (A) that gives us the maximum expected reward according to our current estimate of the Q-value for each possible action.\nimport jax\n\n# Greedy action selection\n“\\epsilon-Greedy” Method\nUse greedy action selection most of the time. “Occasionally”, with probability \\epsilon &gt; 0, select an action amongst all possible actions uniformly at random. Eventually guaranteed to estimate all q_*(a) (but might not be practical).\nimport jax\n\n# Epsilon-Greedy action selection\n\n# Do some visualizations for different values of epsilon\n# for say 1000 simulations, and 1000 time steps\n\n\n\n\nInitial values affect the outcome of a run in both greedy and epsilon-greedy approaches, but in different ways.\nIdea: Setting optimistic initial values (higher than realistic rewards) naturally encourages exploration without adding randomness.\nFor pure greedy method, when Q_t(a) = 5 for instance for all actions (much higher than actual expected rewards/means of the reward distributions):\n\nThe agent starts believing all actions are excellent.\nAfter trying action A, its estimate typically drops below 5\nThis makes untried actions look better by comparison\nEventually, the agent tries all actions at least once\nThen it settles into exploiting what it learned\n\nThink of it like a food critic who starts with unrealistically high expectations for every restaurant. The disappointment after each visit ensures they’ll try every restaurant before settling on favorites.\nWith epsilon-greedy, initial values matter less because:\n\nRandom exploration happens regardless of initial values (\\epsilon portion of the time)\nHigh initial values still affect the exploitation phase (1-\\epsilon portion of the time)\n\nIf Q_t(a) = 5 with epsilon-greedy:\n\nRandom exploration happens anyway (at rate \\epsilon)\nDuring exploitation (1 - \\epsilon), untried actions initially appear better\nThe combination creates more thorough early exploration\n\nThis contradicts the common assumption that epsilon-greedy solves all exploration problems on its own. In practice, optimistic initialization can work synergistically with epsilon-greedy to front-load exploration when it matters most.\nA key limitation worth considering: optimistic initialization only drives temporary exploration. In non-stationary environments where reward distributions change over time, the initial optimism wears off, potentially leading to suboptimal performance if not combined with ongoing exploration mechanisms.\nIn conclusion, initial values matter for determining how much exploration happens early in the learning process.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "2. Bandit Algorithms"
    ]
  },
  {
    "objectID": "lessons/expert/gradient-bandit-algos.html#multi-armed-bandit-theory",
    "href": "lessons/expert/gradient-bandit-algos.html#multi-armed-bandit-theory",
    "title": "2. Bandit Algorithms",
    "section": "",
    "text": "Let’s delve into the theory behind the multi-armed bandit problem. Choice amongst R options at each time step. Step returns a numerical reward sampled from a reward distribution.\nGoal: Maximize expected total reward over time.\n\nR_t is the reward received at time t.\nA_t be action selected at time t.\na is choice of a given machine/agent.\n\n\n\nq_*(a) = E(R_t|A_t = a)\nBut since we do not know q_*(a), the true action-value function, we must estimate it. If we knew the true value of an agent’s action, q_*(a), for every possible choice of a, we would always pick the a which maximizes it.\n\n\n\nWe denote the estimate by Q(a). Ideally, Q(a) should converge to q_*(a).\nHow do we choose Q_t(a)?\nOne possibility is to use the average (expected value) of all rewards received for action a up to time t:\n\nQ_t(a) = \\frac{\\text{total rewards for }a}{\\text{Number of times }a \\text{ was performed}}\n\n\nQ_t(a) = \\frac{\\sum_{i=1}^{t-1}{R_i I(A_i=a)}}{\\sum_{i=1}^{t-1}{I(A_i=a)}}\n\nQ_t(a) \\implies q_*(a) by Law of Large Numbers (LLN).\n\n\n\n\n“Greedy” Action Selection\nA_t = \\arg\\max_a{Q_t(a)} ties ave broken at random Q_t(a) is initialized to be 0, for all a. This means that at time t, we choose the action (A) that gives us the maximum expected reward according to our current estimate of the Q-value for each possible action.\nimport jax\n\n# Greedy action selection\n“\\epsilon-Greedy” Method\nUse greedy action selection most of the time. “Occasionally”, with probability \\epsilon &gt; 0, select an action amongst all possible actions uniformly at random. Eventually guaranteed to estimate all q_*(a) (but might not be practical).\nimport jax\n\n# Epsilon-Greedy action selection\n\n# Do some visualizations for different values of epsilon\n# for say 1000 simulations, and 1000 time steps\n\n\n\n\nInitial values affect the outcome of a run in both greedy and epsilon-greedy approaches, but in different ways.\nIdea: Setting optimistic initial values (higher than realistic rewards) naturally encourages exploration without adding randomness.\nFor pure greedy method, when Q_t(a) = 5 for instance for all actions (much higher than actual expected rewards/means of the reward distributions):\n\nThe agent starts believing all actions are excellent.\nAfter trying action A, its estimate typically drops below 5\nThis makes untried actions look better by comparison\nEventually, the agent tries all actions at least once\nThen it settles into exploiting what it learned\n\nThink of it like a food critic who starts with unrealistically high expectations for every restaurant. The disappointment after each visit ensures they’ll try every restaurant before settling on favorites.\nWith epsilon-greedy, initial values matter less because:\n\nRandom exploration happens regardless of initial values (\\epsilon portion of the time)\nHigh initial values still affect the exploitation phase (1-\\epsilon portion of the time)\n\nIf Q_t(a) = 5 with epsilon-greedy:\n\nRandom exploration happens anyway (at rate \\epsilon)\nDuring exploitation (1 - \\epsilon), untried actions initially appear better\nThe combination creates more thorough early exploration\n\nThis contradicts the common assumption that epsilon-greedy solves all exploration problems on its own. In practice, optimistic initialization can work synergistically with epsilon-greedy to front-load exploration when it matters most.\nA key limitation worth considering: optimistic initialization only drives temporary exploration. In non-stationary environments where reward distributions change over time, the initial optimism wears off, potentially leading to suboptimal performance if not combined with ongoing exploration mechanisms.\nIn conclusion, initial values matter for determining how much exploration happens early in the learning process.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "2. Bandit Algorithms"
    ]
  },
  {
    "objectID": "lessons/expert/gradient-bandit-algos.html#gradient-bandit-algorithms",
    "href": "lessons/expert/gradient-bandit-algos.html#gradient-bandit-algorithms",
    "title": "2. Bandit Algorithms",
    "section": "Gradient Bandit Algorithms",
    "text": "Gradient Bandit Algorithms\nSo far we estimate action values to guide our policy.\n\nAnalogy\nConcept: Rather than asking “what’s the absolute value of each action?”, gradient bandits ask “how much better is one action compared to others?”\nThink of it like ranking restaurants rather than rating them on a fixed scale. You might not know if a restaurant deserves 3 or 4 stars absolutely, but you can more easily say you prefer it over another one.\n\n\nTheory\nIdea: For action a, learn a preference H_t(a). -&gt; Not interpretable as an expected reward.\nActions with higher preferences are more likely to be taken at a given time.\nTo convert preferences into decisions, we use a softmax transform. In other words, we are mapping states to action probabilities.\nPolicy: The probability distribution over actions P(A_t = a) = \\pi_t(a). This policy changes over time as more action is taken (hence the t).\nBelow is the softmax function.\n\nP(A_t = a) = \\frac{e^{H_t(a)}}{\\sum_{b=1}^R e^{H_t(b)}}=\\pi_t(a)\n\nwhere H_t(a) is a numerical preference value of an action a\nDo a kind of gradient ascent step.\n\nStep-size \\alpha &gt; 0, where \\alpha is the step-size hyperparameter\nAfter choosing an action A_t and observing reward, R_t, update our preference as follows:\n\nIncrease the preference for the selected action if the reward is better than expected (that is the average)\nDecrease the preference for the selected action if the reward was worse than expected (the average)\n\nThe update rule is:\n\nUpdate the preference for the action you took\nH_{t+1}(A_t) = H_t(A_t) + \\alpha(R_t - \\bar R_t)(1-\\pi_t(A_t))\n\nwhere A_t is the action we took. It is the specific action that was selected and taken at time t.\n\n\nUpdate the preference for all other actions you didn’t take\nH_{t+1}(a) = H_t(a) - \\alpha(R_t - \\bar R_t)\\pi_t(a)\n\nNote that, a \\neq A_t and \\bar R_t is the average of rewards not including time t. a is any action in the action space.",
    "crumbs": [
      "Cutting-Edge Techniques",
      "2. Bandit Algorithms"
    ]
  }
]