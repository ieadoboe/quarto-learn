---
title: "Episodes in Reinforcement Learning"
format: html
---

$V_\pi$ is the unique solution to its own Bellman equation. 

If we know $\pi$, p we can find the value function $V_\pi$ by solving a system of equations in $V_\pi$. 
Number of equations = number of states.

**What is a good policy:** One which is able to extract rewards from the environment.
In general, we say a policy $\pi$ is better than if $V_\pi(s) \geq V_{\pi'}(s)$ for all states $s$.
We say $\pi^*$ is an optimal policy if it is better than or at least as good as any other policy.

"Optimal state-value function"
$$V_*(s) = \max_\pi V_\pi(s)$$

Note that this is a maximization problem.

"Optimal action-value function"
$$q_*(s, a) = \max_\pi q_\pi(s, a)$$
$$ = E[R_{t+1} + r V_*(s_{t+1}) | s_t = s, a_t = a]$$

Recall that $q_\pi(s, a) = E_\pi(G_t | s_t = s, a_t = a)$

$G_t$ becomes the return from following the optimal policy.