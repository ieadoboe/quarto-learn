---
title: "Bandit Algorithms"
format: html
---

## Multi-Armed Bandit Theory

Let's delve into the theory behind the multi-armed bandit problem. Choice amongst R options at each time step. Step returns a numerical reward sampled from a reward distribution.

Goal: Maximize expected total reward over time.

-   $R_t$ is the reward received at time $t$.
-   $A_t$ be action selected at time $t$.
-   $a$ is choice of a given machine/agent.

#### **True Value of an Action**

$$q_*(a) = E(R_t|A_t = a)$$

But since we do not know $q_*(a)$, the true action-value function, we must estimate it. If we knew the true value of an agent's action, $q_*(a)$, for every possible choice of $a$, we would always pick the $a$ which maximizes it.

#### **Estimated value of an Action**

We denote the estimate by $Q(a)$. Ideally, $Q(a)$ should converge to $q_*(a)$.

**How do we choose** $Q_t(a)$?

One possibility is to use the average (expected value) of all rewards received for action $a$ up to time $t$:

$$
Q_t(a) = \frac{\text{total rewards for }a}{\text{Number of times }a \text{ was performed}}
$$

$$
Q_t(a) = \frac{\sum_{i=1}^{t-1}{R_i I(A_i=a)}}{\sum_{i=1}^{t-1}{I(A_i=a)}}
$$

$Q_t(a) \implies q_*(a)$ by Law of Large Numbers (LLN).

### How do we use this for decision-making?

1.  "Greedy" Action Selection\
    $A_t = \arg\max_a{Q_t(a)}$ ties ave broken at random $Q_t(a)$ is initialized to be 0, for all $a$. This means that at time t, we choose the action (A) that gives us the maximum expected reward according to our current estimate of the Q-value for each possible action.

    ``` python
    import jax

    # Greedy action selection
    ```

2.  "$\epsilon$-Greedy" Method\
    Use greedy action selection most of the time. "Occasionally", with probability $\epsilon > 0$, select an action amongst **all** possible actions uniformly at random. Eventually guaranteed to estimate all $q_*(a)$ (but might not be practical).

    ``` python
    import jax

    # Epsilon-Greedy action selection

    # Do some visualizations for different values of epsilon
    # for say 1000 simulations, and 1000 time steps
    ```

### How initial values impact exploration in Bandit Methods

Initial values affect the outcome of a run in both greedy and epsilon-greedy approaches, but in different ways.

**Idea:** Setting optimistic initial values (higher than realistic rewards) naturally encourages exploration without adding randomness.

For **pure greedy method,** when $Q_t(a) = 5$ for instance for all actions (much higher than actual expected rewards/means of the reward distributions):

-   The agent starts believing all actions are excellent.

-   After trying action A, its estimate typically drops below 5

-   This makes untried actions look better by comparison

-   Eventually, the agent tries all actions at least once

-   Then it settles into exploiting what it learned

Think of it like a food critic who starts with unrealistically high expectations for every restaurant. The disappointment after each visit ensures they'll try every restaurant before settling on favorites.

With **epsilon-greedy,** initial values matter less because:

-   Random exploration happens regardless of initial values ($\epsilon$ portion of the time)

-   High initial values still affect the exploitation phase (1-$\epsilon$ portion of the time)

If $Q_t(a) = 5$ with epsilon-greedy:

-   Random exploration happens anyway (at rate $\epsilon$)

-   During exploitation ($1 - \epsilon$), untried actions initially appear better

-   The combination creates more thorough early exploration

This contradicts the common assumption that epsilon-greedy solves all exploration problems on its own. In practice, optimistic initialization can work synergistically with epsilon-greedy to front-load exploration when it matters most.

**A key limitation worth considering:** optimistic initialization only drives temporary exploration. In non-stationary environments where reward distributions change over time, the initial optimism wears off, potentially leading to suboptimal performance if not combined with ongoing exploration mechanisms.

In conclusion, initial values matter for determining how much exploration happens early in the learning process.

## Gradient Bandit Algorithms

So far we estimate action values to guide our policy.

### Analogy

**Concept:** Rather than asking "what's the absolute value of each action?", gradient bandits ask "how much better is one action compared to others?"

Think of it like ranking restaurants rather than rating them on a fixed scale. You might not know if a restaurant deserves 3 or 4 stars absolutely, but you can more easily say you prefer it over another one.

### Theory

**Idea:** For action $a$, learn a **preference** $H_t(a)$. -\> Not interpretable as an expected reward.

Actions with higher preferences are more likely to be taken at a given time.

To convert preferences into decisions, we use a **softmax** transform. In other words, we are mapping states to action probabilities.

**Policy:** The probability distribution over actions $P(A_t = a) = \pi_t(a)$. This policy changes over time as more action is taken (hence the $t$).

Below is the softmax function.

$$
P(A_t = a) = \frac{e^{H_t(a)}}{\sum_{b=1}^R e^{H_t(b)}}=\pi_t(a)
$$

where $H_t(a)$ is a numerical preference value of an action $a$

Do a kind of gradient ascent step.

-   Step-size $\alpha > 0$, where $\alpha$ is the step-size hyperparameter

-   After choosing an action $A_t$ and observing reward, $R_t$, update our preference as follows:

    -   Increase the preference for the selected action if the reward is better than expected (that is the average)

    -   Decrease the preference for the selected action if the reward was worse than expected (the average)

        -   The update rule is:

            -   Update the preference for the action you took

            -   $H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar R_t)(1-\pi_t(A_t))$

                -   where $A_t$ is the action we took. It is the specific action that was selected and taken at time t.

                <!-- -->

            -   Update the preference for all other actions you didn't take

            -   $H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar R_t)\pi_t(a)$

                -   Note that, $a \neq A_t$ and $\bar R_t$ is the average of rewards **not including time** $t$. $a$ is any action in the action space.

**Note:** On baselines (in our case $\bar R_t$):

Say we select our expected rewards from a normal distribution $N(u, 1)$ and not N(0, 1).
$\implies$ Algorithm adapts rapidly.

The below are questions assuming we ignore the baseline.

$H_{t}(A_t) = H_t(A_t) + \alpha(R_t)(1-\pi_t(A_t))$

$H_{t}(a) = H_t(a) - \alpha(R_t)\pi_t(a)$

- where $A_t \neq a$.

The baseline helps us to make the correct decision. It can be used to compare the action we took with the average reward of all actions. This helps us to determine if the action we took was better or worse than expected.

Performs way worse: No reason to not use the baseline.

``` python

# Gradient Bandit Algorithm
import jax

```

**Comment:** The gradient bandit algorithm can be viewed as an approximation to an exact algorithm. 

$H_{t+1}(a) = H_t(a) + \frac{dE(R_t)}{dH_ta)}$

$E(R_t) = \sum_{x}\pi_t(x)q_*(x)$

where $q_*(x)$ is the true action value of action $x$ which we don't know.

Our Algorithm earlier approximates these updates in expectation.






